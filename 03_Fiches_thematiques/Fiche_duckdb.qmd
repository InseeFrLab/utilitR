# Manipuler des données avec `duckdb` {#duckdb}

## Tâches concernées et recommandations

L'utilisateur souhaite manipuler des données structurées sous forme de `data.frame`  par le biais de l'écosystème `duckdb` (sélectionner des variables, sélectionner des observations, créer des variables, joindre des tables).

::: {.callout-important}
Tâches concernées et recommandations
- Pour des tables de données de taille petite et moyenne (inférieure à 1 Go ou moins d'un million d'observations), il est recommandé d'utiliser les *packages* `tibble`, `dplyr` et `tidyr` qui sont présentés dans la fiche [Manipuler des données avec le `tidyverse`](#tidyverse);

- Pour des tables de données de grande taille (plus de 1 Go en CSV, plus de 200 Mo en Parquet, ou plus d'un million d'observations), il est recommandé d'utiliser soit le *package* `data.table` qui fait l'objet de la fiche [Manipuler des données avec `data.table`](#datatable) lorsque les données tiennent en mémoire vive, soit le *package* `arrow` qui fait l'objet de la présente fiche. Nous présentons ici les fonctionnalités du package `duckdb` en complément ou en remplacement de `arrow`.

- Il est essentiel de travailler avec la dernière version d'`arrow`, de `duckdb` et de `R` car les *packages* `arrow` et `duckdb` sont en cours de développement.

- Si les données sont très volumineuses (plus de 5 Go en CSV, plus de 1 Go en Parquet ou plus de 5 millions d'observations), il est essentiel de manipuler des requêtes `duckdb Table`, plutôt que des `tibbles` pour pouvoir réaliser les traitements sur une machine standard. Les éléments présentés dans cette fiche concernent préférentiellement ce cas de figure.

:::

::: {.callout-note}

Apprendre à utiliser `duckdb` n'est pas difficile, car la syntaxe utilisée est quasiment identique à celle du `tidyverse`. Toutefois, une bonne compréhension du fonctionnement de `R` et de `duckdb` est nécessaire pour bien utiliser `duckdb` sur des données volumineuses. Voici quelques conseils pour bien démarrer:

-   Il est indispensable de lire la fiche [Manipuler des données avec le `tidyverse`](#tidyverse) avant de lire la présente fiche.
-   Il est complètement normal de rencontrer des erreurs difficiles à comprendre lorsqu'on commence à utiliser `duckdb`, il ne faut donc pas se décourager.
-   Il ne faut pas hésiter à demander de l'aide à des collègues, ou à poser des questions sur les salons Tchap adaptés (le salon Langage `R` par exemple).
:::

## Présentation du package `duckdb`  et du projet associé

### Qu'est-ce que `duckdb`?  {#sec-presentation}

[`DuckDB`](https://duckdb.org/) est un projet *open-source* (license MIT) qui propose un moteur SQL optimisé pour les requêtes *online analytical processing* (OLAP): 

- un moteur SQL rapide, capable d'utiliser des données au format `parquet` sans les charger en mémoire,
- un dialect SQL enrichi pour l'analyse de données,
- une installation et un déploiement facile,
- un moteur portable, utilisable sous Windows, MacOS, Linux, et interfacé avec de nombreux langages de programmation (R, Python, Javascript, etc.).

`DuckDB` est bien un serveur SQL. Il dispose de sa propre mémoire, et on peut lui envoyer des requêtes SQL sans passer par R. 
Un point important à retenir est donc que **`duckdb` n'est pas un outil spécifique à `R`**, et il faut bien distinguer le projet `DuckDB` du *package* `R` `duckdb`. Ce *package* propose simplement une interface avec R parmi les autres interfaces existantes : Python, Java, Javascript, Julia, etc.

Toutefois, il est très facile à utiliser avec R, ce qui permet de bénéficier des optimisations inhérentes au langage SQL, à la fois en terme d'utilisation de la mémoire et de rapidité de calcul. C'est de plus un bon intermédiaire avant de passer à des infrastructures avancées telles que spark ou oracle.


### A quoi sert le *package* `duckdb`?

Du point de vue d'un statisticien utilisant `R`, le *package* `duckdb` permet de faire trois choses:

-   Importer des données (exemples: fichiers CSV, fichiers Parquet);
-   Manipuler des données avec la syntaxe `dplyr`, ou avec le langage SQL;
-   Écrire des données en format Parquet.


### Quels sont les avantages de `duckdb`?

-   **Performances élevées**: `duckdb` est très rapide pour la manipulation de données tabulaires (nettement plus performant que `dplyr` par exemple);
-   **Ne pas charger les données en mémoire**: `duckdb` permet de travailler directement sur des fichiers du disque dur.
-   **Optimisations automatiques**: `duckdb` sélectionne automatiquement les colonnes nécessaires au calcule, et ne lit que les lignes nécessaires. Cela permet d'accélérer les calculs et de réduire considérablement les besoins en mémoire, même lorsque les données sont volumineuses;
-   **Facilité d'apprentissage** grâce aux approches `dplyr` et SQL: `duckdb` peut être utilisé avec les verbes de `dplyr` (`select`, `mutate`, etc.) et/ou avec le langage SQL. Par conséquent, il n'est pas nécessaire d'apprendre une nouvelle syntaxe pour utiliser `duckdb`, on peut s'appuyer sur la ou les approches que l'on maîtrise déjà. 


### Quelles sont les points d'attention à l'usage ?

-   __Représentation des données en mémoire__ : `duckdb` est un moteur SQL. Les lignes n'ont pas d'ordre pré-défini, et peuvent être retournées dans un ordre aléatoire.

-   __Traitement de données volumineuses__: `duckdb` traite des données qui peuvent être en mémoire vive ou sur le disque dur. En mémoire vive, les échanges sont efficaces avec `arrow` mais il faudra convertir les données pour les utiliser au format `tibble` (fonction `collect()`) ou avec `data.table`. Avec des données sur le disque dur, `duckdb` est capable de faire les traitements sur des données plus volumineuses que la mémoire vive (RAM). C'est un avantage majeur en comparaison aux autres approches possibles en `R` (`data.table` et `dplyr` par exemple).

-   __*Évaluation différée*__: `duckdb` construit des requêtes SQL, qui sont exécutées uniquement quand nécessaire, après optimisation des étapes intermédiaires, et peuvent être exécutées partiellement. La @sec-lazy présente en détail cette notion.

-   __*Traduction en SQL*__: `duckdb` construit des requêtes SQL, et toutes les fonctions de `dplyr` ne sont pas traduites. Il y moins de limitations que `arrow` mais il faut parfois utiliser une fonction SQL directement ou contourner le problème. La @sec-sql donne quelques trucs et astuces dans ce cas.

-   __Interopérabilité__: `duckdb` est conçu pour être interopérable entre plusieurs langages de programmation tels que `R`, Python, Java, C++, etc. Cela signifie que les données peuvent être échangées entre ces langages sans avoir besoin de convertir les données, d'où des gains importants de temps et de performance.



# Utilisation de `duckdb`

## Charger le *package* `duckdb`

Pour utiliser `duckdb`, il faut commencer par charger le *package*. Il est utile de charger le package `dplyr` pour bénéficier de la syntaxe correspondante. 

```{r}
#| output: false
library(duckdb)
library(dplyr)
```

Le moteur `duckdb` fonctionnant "en dehors" de R, il détecte le nombre de processeurs et effectue les opérations en parallèle si possible. Le paramètrage s'effectue au moment de l'ouverture de la connexion (section suivante).


## Connexion à une base de données

**`duckdb` est une base de données distante et s'utilise comme tel: il faut ouvrir une connexion et "charger" les données.** 

Comme beaucoup d'autres bases de données (distantes ou locales), on ouvre une connexion au moteur `duckdb` avec une base de données en mémoire vive de la façon suivante :
```{r}
con <- DBI::dbConnect(duckdb::duckdb()); 
```

À la fin du traitement ou du programme, on ferme la connexion avec le code ci-dessous. L'option `shutdown` est importante : elle permet de fermer le moteur et de récupérer la mémoire. Autrement, il arrive souvent d'avoir d'avoir des connexions à moitié ouvertes, et de devoir relancer la session R.
```{r}
DBI::dbDisconnect(con, shutdown = TRUE)
```

Pour la suite, on supposera que la connexion est ouverte.
```{r}
con <- DBI::dbConnect(duckdb::duckdb()); 
```

**Important** : on peut adosser un fichier `.db` à `duckdb` pour lui permettre de déborder de la mémoire vive ou de stocker dans son format l'ensemble des tables. C'est très lent. Restructurer les calculs ou écrire une base intermédiaire au format `parquet` partitionné est souvent une meilleure solution.



## Chargement des données et écriture au format parquet

### Déclaration de données provenant de R

On charge des données dans `duckdb` au travers de la connexion `con` avec la fonction `duckdb_register()`. Cette methode a l'avantage de ne pas _recopier_ les données. D'autres moyens sont possibles, notamment en passant par `arrow`. L'objet `con` apparaît dans l'onglet `Data` de l'environnement `RStudio`, mais la liste des tables n'y est pas directement accessibles. Pour plus d'informations, se reporter à la documentation du package `DBI`.

```{r}
con %>% duckdb::duckdb_register(name = "bpe_ens_2018_duckdb", df = doremifasolData::bpe_ens_2018 |> as_tibble())
```

Le code ci-dessous permet de vérifier que le chargement des données a bien fonctionné. La fonction `tbl` permet d'accéder à un objet de la base de données par le nom (de la table), ou par du code SQL (utilisation avancée). Par défaut, `duckdb` affiche les 10 premières lignes du résultat, sans effectuer tout le calcul. C'est super pratique !
```{r}
con %>% tbl("bpe_ens_2018_duckdb")
```

### Écriture au format parquet 

**Pour écrire une table (ou n'importe quelle requête) sur le disque au format parquet, on suggère d'utiliser la librairie `arrow`.**
```{r}
con %>% tbl("bpe_ens_2018_duckdb") %>% arrow::to_arrow() %>% arrow::write_dataset("bpe_ens_2018_dataset")
list.files("bpe_ens_2018_dataset")
```

Une autre possibilité est de passer par la commande SQL `COPY ... TO`. Dans mon expérience, passer par `arrow` permet d'exécuter des calculs plus complexes sans faire déborder la mémoire (`arrow` sauvegarde au fur et à mesure), et la syntaxe est plus facile à manipuler, notamment quand on souhaite ajouter des options.


### Déclaration de données provenant du disque dur

**Pour utiliser un fichier parquet dans `duckdb` sans le charger en mémoire, on propose deux méthodes.**

En passant par arrow. Cette méthode est généralisable aux autres formats de données lisibles par `arrow`, notamment CSV, mais utilise des objets intermédiaires : 
```{r}
arrow::open_dataset("bpe_ens_2018_dataset") %>% arrow::to_duckdb(con)
```

En passant directement par duckdb, il faut travailler un peu plus pour construire les noms de fichiers :
```{r messages=FALSE}
con %>% tbl("read_parquet('bpe_ens_2018_dataset/*.parquet')")
```


## Construire des requêtes SQL avec la syntaxe `dplyr`

Le _package_ `R` `duckdb` (en fait `dbplyr`) a été écrit de façon à pouvoir manipuler les données avec la syntaxe de `dplyr` (`select`, `filter`, `mutate`, `left_join`, etc.). `dbplyr` traduit le code R, y compris certaines fonctions de `stringr` et `lubridate` en requête SQL. Cela s'avère très commode en pratique, car lorsqu'on sait utiliser `dplyr` et le `tidyverse`, on peut commencer à utiliser `duckdb` sans avoir à apprendre une nouvelle syntaxe de manipulation de données. Il y a néanmoins des subtilités à connaître, détaillées dans la suite de cette fiche.

Dans l'exemple suivant, on calcule le nombre d'équipements par région, à partir d'un `tibble` et à partir d'un `duckdb table`. La seule différence apparente entre les deux traitement est la présence de la fonction `collect()` à la fin des instructions; cette fonction indique que l'on souhaite que le résultat du traitement soit stocké sous la forme d'un `tibble`. La raison d'être de ce `collect()` est expliquée plus loin, dans le paragraphe sur l'évaluation différée. Les résultats sont identiques, à permutation près. En effet, un moteur SQL ne respecte pas l'ordre par défaut, il faut le demander explicitement avec `arrange`.

:::: {.columns}

::: {.column width="49%"}

__Manipulation d'un `tibble`__

```{r message=FALSE}
doremifasolData::bpe_ens_2018 |>
  group_by(REG) |>
  summarise(
    NB_EQUIP_TOT = sum(NB_EQUIP)
  )
```

:::

::: {.column width="2%"}
<!-- empty column to create gap -->
:::

::: {.column width="49%"}

__Manipulation d'un `duckdb Table`__

```{r message=FALSE}
con %>% tbl("bpe_ens_2018_duckdb") |>
  group_by(REG) |>
  summarise(
    NB_EQUIP_TOT = sum(NB_EQUIP)
  ) |>
  collect()
```

:::

::::



### Le moteur d'exécution SQL de `duckdb`

Il y a une différence fondamentale entre manipuler un `data.frame` ou un `tibble` et manipuler une requête SQL. Pour bien la comprendre, il faut d'abord comprendre la __distinction entre syntaxe de manipulation des données et moteur d'exécution__:

- La syntaxe de manipulation des données sert à décrire les manipulations de données qu'on veut faire (calculer des moyennes, faire des jointures...), indépendamment de la façon dont ces calculs sont effectivement réalisés;
-  le moteur d'exécution fait référence à la façon dont les opérations sur les données sont effectivement réalisées en mémoire, indépendamment de la façon dont elles ont été décrites par l'utilisateur.

__La grande différence entre manipuler un `tibble` et manipuler une requête `duckdb` tient au moteur d'exécution__: si on manipule un `tibble` avec la syntaxe de `dplyr`, alors c'est le moteur d'exécution de `dplyr` qui fait les calculs; si on manipule une requête `duckdb` avec la syntaxe de `dplyr`, alors c'est le moteur d'exécution de `duckdb` qui fait les calculs. C'est justement parce que le moteur d'exécution de `duckdb` est beaucoup plus efficace que celui de `dplyr` que `duckdb` est beaucoup plus rapide.


__Cette différence de moteurs d'exécution a une conséquence technique importante__: une fois que l'utilisateur a défini des instructions avec la syntaxe `dplyr`, il est nécessaire que celles-ci soient converties en SQL pour que `duckdb` puisse les exécuter. De façon générale, `duckdb` fait cette conversion de façon automatique et invisible, car le  _package_ `duckdb` contient la traduction de plusieurs centaines de fonctions du `tidyverse`. Par exemple, le _package_ `duckdb` contient la traduction de la fonction `filter()` de `dplyr`, ce qui fait que les instructions `filter()` écrites en syntaxe `tidyverse` sont converties de façon automatique et invisible en des instructions SQL équivalentes. Il arrive toutefois qu'on veuille utiliser une fonction non supportée. Cette situation est décrite dans le paragraphe "Comment utiliser une fonction non supportée".


### L'évaluation différée avec `duckdb` (_lazy evaluation_) {#sec-lazy}

__Une caractéristique importante de `duckdb` est qu'il pratique l'évaluation différée (_lazy evaluation_): les calculs ne sont effectivement réalisés que lorsqu'ils sont nécessaires__. En pratique, cela signifie que l'on construit successivement des requêtes SQL, sans faire aucun calcul tant que l'utilisateur ne le demande pas explicitement. Il existe deux fonctions pour déclencher l'évaluation d'un traitement : `collect()` et `print()`.

__L'évaluation différée permet d'améliorer les performances en évitant le calcul de résultats intermédiaires inutiles, et en optimisant les requêtes__ pour utiliser le minimum de données et le minimum de ressources. L'exemple suivant illustre l'intérêt de l'évaluation différée dans un cas simple. 


```{r, message=FALSE}
# Étape 1: compter les équipements
eq_dep <- con %>% tbl("bpe_ens_2018_duckdb") |>
  group_by(DEP) |>
  summarise(
    NB_EQUIP_TOT = sum(NB_EQUIP)
  )

# Étape 2: filtrer sur le département
eq_dep |> 
  filter(DEP == "59") |> 
  collect()
```


Dans cet exemple, on procède à un traitement en deux temps: on compte les équipements par département, puis on filtre sur le département. Il est important de souligner que la première étape ne réalise aucun calcul par elle-même, car elle ne comprend ni `collect()` ni `print()`. L'objet `eq_dep` n'est pas une table et ne contient pas de données, il contient simplement une requête (_query_) décrivant les opérations à mener sur la table `bpe_ens_2018_duckdb`.

On pourrait penser que, lorsqu'on exécute l'ensemble de ce traitement, `duckdb` se contente d'exécuter les instructions les unes après les autres: compter les équipements par département, puis conserver uniquement le département 59. Mais en réalité `duckdb` fait beaucoup mieux que cela: __`duckdb` analyse la requête avant de l'exécuter, et optimise le traitement pour minimiser le travail__. Dans le cas présent, `duckdb` repère que la requête ne porte en fait que sur le département 59, et commence donc par filtrer les données sur le département avant de compter les équipements, de façon à ne conserver que le minimum de données nécessaires et à ne réaliser que le minimum de calculs. Ce type d'optimisation s'avère très utile quand les données à traiter sont très volumineuses.


## Comment bien utiliser `duckdb`? (à faire)

Au premier abord, on peut avoir l'impression qu'`duckdb` s'utilise exactement comme `dplyr` (c'est d'ailleurs fait exprès!). Il y a toutefois quelques différences qui peuvent avoir un impact considérable sur les performances des traitements. Cette partie détaille quatre recommandations à suivre pour bien utiliser `duckdb`:

- Utiliser correctement l'évaluation différée;
- Surveiller la consommation de RAM de `R`.




### Savoir bien utiliser l'évaluation différée

La @sec-lazy a présenté la notion d'évaluation différée et son intérêt pour optimiser les performances. Toutefois, l'évaluation différée n'est pas toujours facile à utiliser, et présente des limites qu'il faut bien comprendre. Cette section décrit plus en détail le fonctionnement de l'évaluation différée et ses limites. Pour illustrer ce fonctionnement, on commence par exporter la base permanente des équipements sous la forme d'un dataset duckdb partitionné. La fiche [Importer des fichiers Parquet](#importparquet) décrit en détail ce qu'est un fichier Parquet partitionné et comment le manipuler.

```
{r, message=FALSE}
# Sauvegarder la BPE 2018 sous la forme d'un dataset duckdb partitionné
write_dataset(
  bpe_ens_2018_duckdb,
  "bpe2018/",
  partitioning = "REG",
  hive_style = TRUE
)
```



# Notions avancées

#### Comment fonctionne l'évaluation différée?

Ce paragraphe s'adresse aux lecteurs qui souhaitent comprendre plus en détail le fonctionnement de l'évaluation différée. Les lecteurs pressés peuvent passer directement au paragraphe suivant, sur les limites de l'évaluation différée.

Le traitement suivant est un exemple simple d'utilisation de l'évaluation différée. Ce traitement comprend trois étapes: se connecter aux données avec `open_dataset()`, puis calculer le nombre d'équipements par département, et enfin sélectionner le département 59.

```
{r, message=FALSE}
# Étape 1: se connecter au fichier Paruet Partitionné
ds_bpe2018 <- open_dataset(
  "bpe2018/",
  partitioning = schema("REG" = utf8()),
  hive_style = TRUE
)

# Étape 2: compter les équipements
eq_dep <- ds_bpe2018 |>
  group_by(DEP) |>
  summarise(
    NB_EQUIP_TOT = sum(NB_EQUIP)
  )

# Étape 3: filtrer sur le département
resultats <- eq_dep |> 
  filter(DEP == "59")
```



Voici quelques commentaires pour comprendre ce traitement:

- Le code ci-dessus n'effectue aucun calcul, car il ne comprend ni `collect()` ni `compute()`. Il faut exécuter `resultats |> collect()` ou  `resultats |> compute()` pour que les calculs soient effectivement réalisés.
- Les objets `ds_bpe2018`, `eq_dep` et `resultats` ne sont pas des tables `R` standards contenant des données: ce sont des requêtes (de classe `duckdb_dplyr_query`), qui décrivent des opérations à mener sur des données. C'est justement en utilisant `collect()` ou `compute()` qu'on demande à `duckdb` d'exécuter ces requêtes avec le moteur `acero`. 
-   Il est possible d'afficher le contenu des requêtes avec la fonction `show_exec_plan()`.
    +   La première requête est très courte: elle ne contient que la description des données contenues dans le fichier Parquet partitionné.

        ```
        {r, message=FALSE}
        # Imprimer la première requête
        show_exec_plan(ds_bpe2018)
        ```

    +   La deuxième requête est un peu plus longue, et si on regarde en détail, on constate deux choses. Premièrement, elle contient la première requête, mais elle n'a conservé que les variables utilisées dans le traitement (`NB_EQUIP` et `DEP`). C'est un exemple d'optimisation faite par `duckdb`: le moteur `acero` a compris automatiquement qu'il suffisait de charger seulement deux variables pour réaliser le traitement. Deuxièmement, on retrouve tous les éléments du traitement (notamment le `group_by` et la somme), mais le traitement décrit en syntaxe `tidyverse` a été traduit automatiquement en fonctions internes d'`duckdb` (la fonction `sum` est par exemple remplacée par `hash_sum`).

        ```
        {r, message=FALSE}
        # Imprimer la deuxième requête, qui contient la première
        show_exec_plan(eq_dep)
        ```

    +   Enfin, la troisième requête est encore plus longue, et contient les deux premières. Autrement dit, elle contient l'intégralité du traitement, donc on réalise l'intégralité du traitement lorsqu'on exécute cette requête avec `resultats |> collect()`.

        ```
        {r, message=FALSE}
        # Imprimer la troisième requête, qui contient les deux premières
        show_exec_plan(resultats)
        ```
    
#### Quelles sont les limites de l'évaluation différée?

L'évaluation différée optimise les performances en minimisant la quantité de données chargées en RAM et la quantité de calculs effectivement réalisés. 
Avec cette vision en tête, on pourrait penser que la meilleure façon d'utiliser `duckdb` est d'écrire un traitement entier en mode _lazy_ (autrement dit, sans aucun `compute()` ni aucun `collect()` dans les étapes intermédiaires), et faire un unique `compute()` ou `collect()` tout à la fin du traitement, pour que toutes les opérations soient optimisées en une seule étape. Un traitement idéal ressemblerait alors à ceci:

```
{r, eval=FALSE}
# Se connecter aux données
data1 <- open_dataset("data1.parquet")
data2 <- open_dataset("data2.parquet")

# Une première étape de traitement
table_intermediaire1 <- data1 |>
  select(...) |>
  filter(...) |>
  mutate(...)

# Une deuxième étape de traitement
table_intermediaire2 <- data2 |>
  select(...) |>
  filter(...) |>
  mutate(...)

# Et encore beaucoup d'autres étapes de traitement
# avec beaucoup d'instructions...

# La dernière étape du traitement
resultats <- table_intermediaire8 |>
  left_join(
    table_intermediaire9, 
    by = "identifiant"
  ) |>
  compute()
  
write_parquet(resultats, "resultats.parquet")
```

La réalité n'est malheureusement pas si simple, car __l'évaluation différée a des limites__. En effet, au moment de produire le résultat final de l'exemple précédent, la fonction `compute()` donne l'instruction au moteur `acero` d'analyser puis d'exécuter _l'intégralité du traitement en une seule fois_ (le paragraphe précédent donne un exemple détaillé). Or, le moteur `acero` est certes puissant, mais il a ses limites et ne peut pas exécuter en une seule fois des traitements vraiment trop complexes. Par exemple, `acero` rencontre des difficultés lorsqu'on enchaîne de multiples jointures de tables volumineuses.

__Ces limites de l'évaluation différée peuvent provoquer des bugs violents__. Lorsque le moteur `acero` échoue à exécuter une requête trop complexe, les conséquences sont brutales: `R` n'imprime aucun message d'erreur, la session `R` plante et il faut simplement redémarrer `R` et tout recommencer. Il est donc nécessaire de bien structurer le traitement pour profiter des avantages de l'évaluation différée sans en toucher les limites.

#### Décomposer le traitement en étapes cohérentes, puis le tester

__La solution évidente pour ne pas toucher les limites de l'évaluation différée consiste à décomposer le traitement en étapes__, et à exécuter chaque étape séparément, en mettant un `compute()`. De cette façon, `acero` va réaliser séquentiellement plusieurs traitements un peu complexes, plutôt qu'échouer à réaliser un seul traitement très complexe en une seule fois. 

__La vraie difficulté consiste à savoir quelle est la bonne longueur de ces étapes intermédiaires__: s'il faut éviter de faire de très longues étapes (sinon l'évaluation différée plante), il faut également éviter d'exécuter une à une les étapes du traitement (sinon on perd les avantages de l'évaluation différée). Il n'y a pas de solution miracle, et seule la pratique permet de déterminer ce qui est raisonnable. Voici toutefois quelques conseils de bon sens:

- __Un point de départ raisonnable peut consister à définir des étapes de traitement qui ne dépassent pas 30 ou 40 lignes de code__. Une étape de traitement de 200 lignes aura toutes les chances de poser des problèmes, d'autant qu'elle ne sera probablement pas très lisible.
- __Il est préférable que le séquencement des étapes soit cohérent avec l'objet du traitement.__ Par exemple, si l'ensemble du traitement consiste à retraiter séparément deux tables, puis à les joindre, on peut imaginer trois étapes qui s'achèvent chacune par un `compute()`: le retraitement de la première table, le retraitement de la seconde table, et la jointure.
- __Plus les données sont volumineuses, plus il faut être prudent avant de définir de longues étapes de traitement__.
- __Plus les opérations unitaires sont complexes, plus les étapes doivent être courtes.__ Par exemple, si les opérations sont des `filter()` et des `select()`, il est possible d'en enchaîner un certain nombre en une seule étape de traitement sans aucun problème, car ces opérations sont simples. Inversement, une étape de traitement ne doit pas comprendre plus de trois ou quatre jointures (car les jointures sont des opérations complexes), en particulier si les tables sont volumineuses.


### Utiliser des objets `duckdb Table` plutôt que des `data.frames`

__Lorsqu'on manipule des données volumineuses, il est essentiel de manipuler uniquement des objets `duckdb Table`, plutôt que des `data.frames` (ou des `tibbles`)__. Cela implique deux recommandations:

- __Importer les données directement dans des `duckdb Table`, ou à défaut convertir en `duckdb Table` avec la fonction `as_duckdb_table()`.__ Par exemple, lorsqu'on importe un fichier Parquet avec la fonction `read_parquet()` ou un fichier csv avec la fonction `read_csv_duckdb()`, il est recommandé d'utiliser l'option `as_data_frame = FALSE` pour que les données soient importées sous forme de `duckdb Table`.
- __Utiliser systématiquement `compute()` plutôt que `collect()` dans les étapes de calcul intermédiaires.__ Cette recommandation est particulièrement importante.

    L'exemple suivant explique pourquoi il est préférable d'utiliser `compute()` dans les étapes intermédiaires:

:::: {.columns}

::: {.column width="49%"}

__Situation à éviter__

La première étape de traitement est déclenchée par `collect()`, la table intermédiaire `res_etape1` est donc un `tibble`. C'est le moteur d'exécution de `dplyr` qui est utilisé pour manipuler `res_etape1` lors de la seconde étape, ce qui dégrade fortement les performances sur données volumineuses.

```
{r eval=FALSE,message=FALSE}
# Etape 1
res_etape1 <- bpe_ens_2018_tbl |>
  group_by(DEP) |>
  summarise(
    NB_EQUIP_TOT = sum(NB_EQUIP)
  ) |>
  collect()

# Etape 2
res_final <- res_etape1 |> 
  filter(DEP == "59") |> 
  collect()

# Sauvegarder les résultats
write_parquet(res_final, "resultats.parquet")
```

:::

::: {.column width="2%"}
<!-- empty column to create gap -->
:::

::: {.column width="49%"}

__Usage recommandé__

La première étape de traitement est déclenchée par `compute()`, la table intermédiaire `res_etape1` est donc un `duckdb Table`. C'est le moteur d'exécution `acero` qui est utilisé pour manipuler `res_etape1` lors de la seconde étape, ce qui assure de bonnes performances notamment sur données volumineuses.


```
{r eval=FALSE,message=FALSE}
# Etape 1
res_etape1 <- bpe_ens_2018_tbl |>
  group_by(DEP) |>
  summarise(
    NB_EQUIP_TOT = sum(NB_EQUIP)
  ) |>
  compute()

# Etape 2
res_final <- res_etape1 |> 
  filter(DEP == "59") |> 
  compute()

# Sauvegarder les résultats
write_parquet(res_final, "resultats.parquet")
```

:::

::::

::: {.callout-tip}
Si vous ne savez plus si une table de données est un `duckdb Table` ou un `tibble`, il suffit d'exécuter `class(le_nom_de_ma_table)`. Si la table est un `duckdb Table`, vous obtiendrez ceci: `"Table"        "duckdbTabular" "duckdbObject"  "R6"`. Si elle est un `tibble`, vous obtiendrez `"tbl_df"     "tbl"        "data.frame"`.
:::


### Surveiller la consommation de RAM de `R`


Comme expliqué plus haut, les `duckdb Table` ne sont pas des objets `R` standards, mais des objets C++ qui peuvent être manipulés avec `R` via `duckdb`. En pratique, cela signifie que `R` n'a qu'un contrôle partiel sur la RAM occupé par `duckdb`, et ne parvient pas toujours à libérer la RAM qu'`duckdb` a utilisée temporairement pour réaliser un traitement. En particulier, __la fonction `gc()` ne permet pas de libérer la RAM qu'`duckdb` a utilisée temporairement__. Cette imperfection de la gestion de la RAM implique deux choses:


- Si on travaille sur des données volumineuses, __il est important de surveiller fréquemment sa consommation de RAM pour s'assurer qu'elle n'est pas excessive__; la fiche [Superviser sa session `R`]{#superviser-ressources}.
- Si la consommation de RAM devient très élevée, __la seule solution semble être de redémarrer la session `R`__. En pratique, redémarrer la session `R` ne fait pas perdre plus de quelques minutes, car grâce à `duckdb` et Parquet le chargement des données est très rapide.





## Notions avancées

### Connaître les limites d'`duckdb` {#sec-limites-duckdb}

Le projet `duckdb` est relativement récent et en développement actif. Il n'est donc pas surprenant qu'il y ait parfois des bugs, et que certaines fonctions standards de `R` ne soient pas encore disponibles en `duckdb`. Il est important de connaître les quelques limites d'`duckdb` pour savoir comment les contourner. Voici quatre limites d'`duckdb` à la date de rédaction de cette fiche (janvier 2024):

- les __jointures de tables volumineuses__: `duckdb` ne parvient pas à joindre des tables de données très volumineuses; il est préférable d'utiliser `duckdb` pour ce type d'opération;
- les __réorganisations de données__ (_wide-to-long_ et _long-to-wide_): il n'existe pas à ce jour dans `duckdb` de fonctions pour réorganiser une table de données (comme `pivot_wider` et `pivot_longer` du _package_ `tidyr`).
-   les __fonctions fenêtre__ (_window functions_): `duckdb` ne permet pas d'ajouter directement à une table des informations issues d'une agrégation par groupe de la même table. Par exemple, `duckdb` ne peut pas ajouter directement à la base permanente des équipements une colonne égale au nombre total d'équipements du département:

    ```
{r eval=FALSE, message=FALSE}
    # duckdb ne peut pas exécuter ceci
    data <- bpe_ens_2018_duckdb |>
      group_by(DEP) |>
      mutate(
        NB_EQUIP_TOTAL_DEP  = sum(NB_EQUIP)
      ) |>
      compute()
    ```

-   les __empilements de tables__: il est facile d'empiler plusieurs `tibbles` avec `dplyr` grâce à la fonction `bind_rows()`: `bind_rows(table1, table2, table3, table4)`. En revanche, il n'existe pas à ce jour de fonction similaire dans `duckdb`. Les fonctions `union` et `union_all` permettent d'empiler seulement deux `duckdb Table`, donc pour empiler plusieurs `duckdb Tables` il faut appeler plusieurs fois ces fonctions. Par ailleurs, les deux `duckdb Table` doivent être parfaitement compatibles pour être empilés (il faut le même nombre de colonnes avec le même nom et le même type, ce qui n'est pas toujours le cas en pratique).

    ```
    {r eval=FALSE, message=FALSE}
    # Comment empiler de multiples duckdb Tables
    resultats <- table1 |>
      union(table2) |>
      union(table3) |>
      union(table4) |>
      compute()
    ```



### Surmonter le problème des fonctions non supportées par `acero` {#sec-sql}

__Lorsqu'on manipule des données avec `duckdb`, il arrive fréquemment qu'on écrive un traitement que le moteur d'exécution `acero` n'arrive pas à exécuter.__ En ce cas, `R` renonce à manipuler les données sous forme de `duckdb Table` avec le moteur `acero`, convertit les données en `tibble` et poursuit le traitement avec le moteur d'exécution de `dplyr` (comme un traitement `dplyr` standard). `R` signale systématiquement le recours à cette solution de repli par un message d'erreur qui se termine par `pulling data into R`. 

Le recours à cette solution de repli a pour conséquence de dégrader fortement les performances (car le moteur de `dplyr` est moins efficace qu'`acero`). __Il est donc préférable d'essayer de réécrire la partie du traitement qui pose problème avec des fonctions supportées par  `acero`.__ Cela est particulièrement recommandé si les données manipulées sont volumineuses ou si le traitement concerné doit être exécuté fréquemment. 

::: {.callout-tip}
Il arrive qu'il soit impossible de trouver une solution entièrement supportée par `acero`, ou que la solution soit vraiment trop complexe à écrire. Ce n'est pas une catastrophe: en dernier recours, on peut tout à fait convertir temporairement les données en `tibble` (avec `collect()`) et exécuter le traitement qui pose problème avec `dplyr`. Le traitement sera simplement plus lent (voire beaucoup plus lent). En revanche, il est important de reconvertir ensuite les données en `duckdb Table` le plus vite possible, en utilisant la fonction `as_duckdb_table()`.
:::

#### Une solution simple existe-t-elle?

Dans la plupart des cas, il est possible de trouver une solution simple pour écrire un traitement que le moteur `acero` peut exécuter. Voici quelques pistes:

- Vérifier qu'on utilise la dernière version d'`duckdb` et mettre à jour le _package_ si ce n'est pas le cas;
- Étudier en détail le message d'erreur renvoyé par `R` pour bien comprendre d'où vient le problème;
- Regarder la [liste des fonctions du _tidyverse_ supportées par `acero`](https://duckdb.apache.org/docs/dev/r/reference/acero.html) pour voir s'il est possible d'utiliser une fonction supportée par `acero`;
- Faire des tests pour pour voir si une réécriture mineure du traitement peut régler le problème.

L'exemple qui suit montre que la solution peut être très simple, même lorsque l'erreur semble complexe. Dans cet exemple, on veut calculer le nombre de boulangeries (`TYPEQU == "B203"`) et de poissonneries (`TYPEQU == "B206"`) dans chaque département, en stockant les résultats dans un `duckdb Table` (avec `compute()`). Malheureusement, `acero` ne parvient pas à réaliser ce traitement, et `R` est contraint de convertir les données en `tibble`.

```
{r eval=FALSE,message=FALSE}
resultats <- bpe_ens_2018_duckdb |>
  group_by(DEP) |>
  summarise(
    nb_boulangeries  = sum(NB_EQUIP * (TYPEQU == "B203")),
    nb_poissonneries = sum(NB_EQUIP * (TYPEQU == "B206"))
  ) |>
  compute()
```

Le message d'erreur renvoyé par `R` est la suivante: `! NotImplemented: Function 'multiply_checked' has no kernel matching input types (double, bool); pulling data into R`. En lisant attentivement le message d'erreur et en le rapprochant du traitement, on finit par comprendre que l'erreur vient de l'opération `sum(NB_EQUIP * (TYPEQU == "B203"))`: `duckdb` ne parvient pas à faire la multiplication entre `NB_EQUIP` (un nombre réel) et `(TYPEQU == "B203")` (un booléen). La solution est très simple: il suffit de convertir `(TYPEQU == "B203")` en nombre entier avec la fonction `as.integer()` qui est supportée par `acero`. Le code suivant peut alors être entièrement exécuté par `acero`:

```
{r eval=FALSE,message=FALSE}
resultats <- bpe_ens_2018_duckdb |>
  group_by(DEP) |>
  summarise(
    nb_boulangeries  = sum(NB_EQUIP * as.integer(TYPEQU == "B203")),
    nb_poissonneries = sum(NB_EQUIP * as.integer(TYPEQU == "B206"))
  ) |>
  compute()
```

#### Passer par `duckdb`

Il arrive qu'il ne soit pas possible de résoudre le problème en réécrivant légèrement le traitement. __Une autre solution peut consister à passer par `duckdb`, qui permet de manipuler directement des objets `duckdb Table` de façon simple et transparente.__ Dans l'exemple suivant, on veut ajouter à la base permanente des équipements une colonne égale au nombre total d'équipements du département. Cette opération ne peut pas être exécutée par `duckdb` (voir le paragraphe @sec-limites-duckdb), contrairement à `duckdb`. Voici comment faire avec `duckdb`:

```
{r eval=FALSE, message=FALSE}
library(duckdb)

data <- bpe_ens_2018_duckdb |>
  to_duckdb() |>
  group_by(DEP) |>
  mutate(
    NB_EQUIP_TOTAL_DEP  = sum(NB_EQUIP)
  ) |>
  to_duckdb() |>
  compute()
```

Cet exemple appelle trois commentaires:

- La fonction `to_duckdb()` sert à ce que `duckdb` puisse accéder à l'objet `duckdb Table`;
- Symétriquement, la fonction `to_duckdb()` sert à remettre les données dans un objet `duckdb Table`;
- Les instructions figurant entre ces deux étapes (le `group_by()` puis le `mutate()`) sont exécutées par le moteur d'exécution de `duckdb`, de façon complètement transparente pour l'utilisateur.


#### Définir soi-même des fonctions `duckdb` (utilisation avancée)

Si les pistes mentionnées précédemment ne fournissent pas de solution simple, il est possible d'aller plus loin et d'écrire ses propres fonctions `duckdb`. Cette approche permet de faire beaucoup plus de choses mais elle nécessite de bien comprendre le fonctionnement d'`duckdb` et les fonctions internes de la librairie `libduckdb`. Il s'agit d'une utilisation avancée d'`duckdb` qui dépasse le cadre de la documentation `utilitR`. Les lecteurs intéressés pourront consulter les deux ressources suivantes:

- [un post de blog qui décrit en détail les liens entre `libduckdb` et `R`](https://blog.djnavarro.net/posts/2022-01-18_binding-duckdb-to-r/) (en anglais); 
- la partie du [`Apache duckdb R Cookbook`](https://duckdb.apache.org/cookbook/r/manipulating-data---tables.html#use-duckdb-functions-in-dplyr-verbs-in-duckdb) qui porte sur les `duckdb functions`.



<!-- <div> -->

<!-- <table class='table' style = "width : 100%;"> -->
<!-- <tr> -->
<!-- <th style="width:45%">Code exécuté dans `R`</th> -->
<!-- <th style="width:55%">Signification</th> -->
<!-- </tr> -->
<!-- <td> -->
<!-- ```{r eval=FALSE,message=FALSE} -->
<!-- bpe_ens_2018_duckdb  |> -->
<!--   group_by(REG) |> -->
<!--   summarise(NB_EQUIP_TOT = sum(NB_EQUIP)) -->
<!-- ``` -->
<!-- </td> -->
<!-- <td>Définir une requête calculant le nombre total d'équipements par région (sans l'exécuter)</td>  -->
<!-- </tr> -->
<!-- <tr> -->
<!-- <td> -->
<!-- ```{r eval=FALSE,message=FALSE} -->
<!-- bpe_ens_2018_duckdb  |> -->
<!--   group_by(REG) |> -->
<!--   summarise(NB_EQUIP_TOT = sum(NB_EQUIP)) |> -->
<!--   compute() -->
<!-- ``` -->
<!-- </td> -->
<!-- <td>Calculer le nombre total d'équipements par région et renvoyer les résultats dans un `duckdb Table`</td>  -->

<!-- </tr> -->
<!-- <tr> -->
<!-- <td> -->
<!-- ```{r eval=FALSE,message=FALSE} -->
<!-- bpe_ens_2018_duckdb  |> -->
<!--   group_by(REG) |> -->
<!--   summarise(NB_EQUIP_TOT = sum(NB_EQUIP)) |> -->
<!--   collect() -->
<!-- ``` -->
<!-- </td> -->
<!-- <td>Calculer le nombre total d'équipements par région et renvoyer les résultats dans un `tibble`</td>  -->
<!-- </tr> -->
<!-- </table> -->

<!-- </div> -->

<!-- ```{r} -->
<!-- # Définir une requête calculant le nombre total d'équipements par région (sans l'exécuter) -->
<!-- bpe_ens_2018_duckdb  |> -->
<!--   group_by(REG) |> -->
<!--   summarise(NB_EQUIP_TOT = sum(NB_EQUIP)) -->

<!-- # Calculer le nombre total d'équipements par région et renvoyer un duckdb Table -->
<!-- bpe_ens_2018_duckdb  |> -->
<!--   group_by(REG) |> -->
<!--   summarise(NB_EQUIP_TOT = sum(NB_EQUIP)) |> -->
<!--   compute() -->

<!-- # Calculer le nombre total d'équipements par région et renvoyer un tibble -->
<!-- bpe_ens_2018_duckdb  |> -->
<!--   group_by(REG) |> -->
<!--   summarise(NB_EQUIP_TOT = sum(NB_EQUIP)) |> -->
<!--   collect() -->

<!-- ``` -->

<!-- ::: -->

## Pour en savoir plus {#Ressourcesduckdb}

- la documentation officielle du _package_ [`duckdb`](https://duckdb.apache.org/docs/dev/r/index.html) (en anglais);
- [un post de blog qui décrit en détail les liens entre `libduckdb` et `R`](https://blog.djnavarro.net/posts/2022-01-18_binding-duckdb-to-r/) (en anglais);
- la [liste](https://duckdb.apache.org/docs/dev/r/reference/acero.html) des fonctions du _tidyverse_ supportées par `acero`.

```{r}
DBI::dbDisconnect(con, shutdown = TRUE)
```



