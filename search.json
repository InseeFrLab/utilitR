[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Principales parties\nLes principales parties de la documentation, organisées sous forme thématique.\nteaserList(keys)\nfunction teaserList(docs, css = THEME_TEASERS) {\n  const id = DOM.uid().id;\n  return html`&lt;div id=\"${id}\"&gt;\n    ${getScopedStyle(`#${id}`, css)}\n    &lt;div class=listing-grid&gt;${docs.map(doc =&gt; renderTeaser(doc))}\n  `;\n}\nfunction renderTeaser(doc) {\n  const { image, title, description, website, order } = doc;\n\n  return `\n    &lt;a href=\"${website}\" class=\"grid-item-link\" target=\"_blank\" rel=\"noopener\"&gt;\n      &lt;div class=\"card\"&gt;\n        &lt;p class=\"card-img-top\"&gt;\n          &lt;img class=\"thumbnail-image card-img\" src=\"${image}\" style=\"height: auto;\"&gt;\n        &lt;/p&gt;\n        &lt;div class=\"card-body post-contents\"&gt;\n          &lt;h5 class=\"no-anchor card-title listing-title\" data-anchor-id=\"introduction-aux-méthodes-quantitatives-avec-fa-brands-r-project\"&gt;${title}&lt;/h5&gt;\n          &lt;div class=\"card-text listing-description delink\"&gt;${description}\n          &lt;/div&gt;        \n        &lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/a&gt;\n  `;\n}\nimport { THEME_TEASERS, ICON_LIKE, getScopedStyle } from \"@mootari/notebook-teasers\"\nkeys = [\n  {\n    \"image\": \"https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/cards/version-sensibilisation/cats.png\",\n    \"title\": \"Introduction\",\n    \"description\": \"Des éléments de présentation du projet &lt;code&gt;utilitr&lt;/code&gt;.\",\n    \"website\": \"/01_R_Insee/presentation_utilitr.html\",\n    \"order\": 1\n  },\n  {\n    \"image\": \"https://rgeo.linogaliana.fr/cards/wrangling/kidmatrix.png\",\n    \"title\": \"Mener un projet statistique avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"description\": \"&lt;br&gt;Un ensemble de fiches présentant comment se lancer dans un projet d'analyse de données avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;&lt;br&gt;.\",\n    \"website\": \"03_Fiches_thematiques/Fiche_rprojects.html\",\n    \"order\": 2\n  },\n  {\n    \"image\": \"https://rgeo.linogaliana.fr/cards/introduction/baby.png\",\n    \"title\": \"Import des données avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"description\": \"&lt;br&gt;Un ensemble de fiches présentant comment importer des données depuis &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;.\",\n    \"website\": \"03_Fiches_thematiques/Fiche_import_fichiers_plats.html\",\n    \"order\": 3\n  },\n  {\n    \"image\": \"https://rgeo.linogaliana.fr/cards/wrangling/kidmatrix.png\",\n    \"title\": \"Choisir son paradigme d'analyse de données avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"description\": \"&lt;br&gt;Des fiches générales pour présenter les principaux écosystèmes d'analyse de données.\",\n    \"website\": \"03_Fiches_thematiques/Fiche_tidyverse.html\",\n    \"order\": 4\n  },\n  {\n    \"image\": \"https://rgeo.linogaliana.fr/cards/wrangling/kidmatrix.png\",\n    \"title\": \"Manipuler des données avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"description\": \"&lt;br&gt;Des fiches sur des opérations classiques pour manipuler des bases de données avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"website\": \"03_Fiches_thematiques/Fiche_joindre_donnees.html\",\n    \"order\": 5\n  },\n  {\n    \"image\": \"https://rgeo.linogaliana.fr/cards/quarto/scroll.png\",\n    \"title\": \"Produire des sorties avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"description\": \"&lt;br&gt;Des fiches sur la manière de partager des sorties d'une analyse de données avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"website\": \"03_Fiches_thematiques/Fiche_graphiques.html\",\n    \"order\": 6\n  },\n  {\n    \"image\": \"https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/cards/version-full/eagle.png\",\n    \"title\": \"Bonnes pratiques\",\n    \"description\": \"&lt;br&gt;Des fiches sur la manière de partager des sorties d'une analyse de données avec &lt;i class=\\\"fa-brands fa-r-project\\\"&gt;&lt;/i&gt;\",\n    \"website\": \"02_Bonnes_pratiques/01-qualite-code.html\",\n    \"order\": 7\n  }\n]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[`utilitR` ne couvre pas tous les besoin, loin s'en faut!]{.bigger120}</span>"
    ]
  },
  {
    "objectID": "index.html#ressources-utiles",
    "href": "index.html#ressources-utiles",
    "title": "",
    "section": "Ressources utiles",
    "text": "Ressources utiles\n\nConvertir des codes SAS en R\nLa Dares a constuit un aide-mémoire de SAS vers R pour aider les agents qui doivent convertir des chaînes SAS vers R et Python.\nCet aide-mémoire propose une traduction des codes standards d’une analyse statistique en SAS, R (environnements R base, tidyverse, data.table, arrow/duckdb) et pandas en Python. Ambitieux, il vise à décloisonner les différents langages, en facilitant le passage et l’intercompréhension entre eux.\nLe lecteur y trouvera un utile complément d’utilitR: il met plus l’accent sur le code informatique que sur les explications théoriques, mais s’appuie sur les conseils d’utilitR et s’y réfère si nécessaire.\nN’hésitez pas à le consulter, vous pourriez y trouver des réponses à vos questions !",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>[`utilitR` ne couvre pas tous les besoin, loin s'en faut!]{.bigger120}</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/presentation_utilitr.html",
    "href": "01_R_Insee/presentation_utilitr.html",
    "title": "2  Présentation du projet utilitR",
    "section": "",
    "text": "2.1 Quel est l’objectif d’utilitR ?\nLe projet utilitR vise à produire une documentation collaborative et open source sur R, destinée en premier lieu aux agents de l’Insee. Ce projet est parti du constat qu’il est difficile d’apprendre à utiliser R pour de multiples raisons : multiplicité de packages faisant plus ou moins la même chose, abondance et éclatement de la documentation (souvent en anglais), difficultés supplémentaires pour effectuer des choix éclairés et adaptés à l’environnement informatique de travail…\nLes contributeurs du projet utilitR se sont fixés pour objectif de rassembler dans un seul document tous les éléments utiles pour l’usage de R à l’Insee, en cherchant à couvrir la plupart des cas d’usage courants. Cette documentation a donc été élaborée en tenant compte du contexte de travail propre à l’Institut. Cette documentation peut évidemment contenir des éléments pertinents pour un usage de R en dehors de l’Insee, mais ce n’est pas sa finalité première.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Présentation du projet `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/presentation_utilitr.html#quelle-est-la-spécificité-dutilitr",
    "href": "01_R_Insee/presentation_utilitr.html#quelle-est-la-spécificité-dutilitr",
    "title": "2  Présentation du projet utilitR",
    "section": "2.2 Quelle est la spécificité d’utilitR ?",
    "text": "2.2 Quelle est la spécificité d’utilitR ?\nLe projet utilitR est un projet collaboratif, évolutif, open source et ouvert à tous, porté par des agents de l’Insee et du Service Statistique Public (SSP). Toute personne qui le souhaite peut modifier la documentation ou la compléter en fonction de ses connaissances et de ses expériences (voir Comment contribuer au projet utilitR).\nLe code source de cette documentation est hébergé sous Github et est accessible en cliquant sur ce lien.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Présentation du projet `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/presentation_utilitr.html#contribuer",
    "href": "01_R_Insee/presentation_utilitr.html#contribuer",
    "title": "2  Présentation du projet utilitR",
    "section": "2.3 Comment contribuer à utilitR?",
    "text": "2.3 Comment contribuer à utilitR?\nLe projet utilitR est un projet collaboratif, évolutif, open source et ouvert à tous, auquel tous les agents peuvent contribuer. Le projet est mené par un groupe de contributeurs qui en définissent eux-mêmes le contenu, la structure et le calendrier. Le dépôt de la documentation est situé sur le compte InseeFrLab sur la plateforme Github et peut facilement être retrouvé à cette adresse. Les objectifs et l’approche collaborative du projet utilitR sont détaillés dans le document Manifeste.md.\nToute personne qui le souhaite peut modifier ou compléter la documentation en fonction de ses connaissances et de ses expériences, et toutes les contributions sont les bienvenues : compléments, corrections d’erreur, améliorations, questions… Il n’y a aucun prérequis, et aucun niveau minimal en R n’est demandé.\nTout agent intéressé à contribuer au projet est invité à consulter le guide des contributeurs CONTRIBUTING.md. Ce guide donne également quelques astuces pour les personnes n’étant pas familières avec Github.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Présentation du projet `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/presentation_utilitr.html#contributeurs-du-projet",
    "href": "01_R_Insee/presentation_utilitr.html#contributeurs-du-projet",
    "title": "2  Présentation du projet utilitR",
    "section": "2.4 Contributeurs du projet",
    "text": "2.4 Contributeurs du projet\n\n\n\n\n\n\nContributeurs du projet\n\n\n\n\n\nto be completed\nLe projet utilitR est un projet collaboratif qui a bénéficié des contributions de : {r} liste_contributeurs\nLa coordination est assurée par Pierre-Yves Berrard, Lino Galiana et Olivier Meslin.\nLe logo du projet a été réalisé par Anna Schlaifer.\nLes contributeurs remercient Julien Taquet et Marc Hufschmitt pour leur aide précieuse sur la mise en forme du site et de la brochure.\nLes contributeurs remercient Arnaud Degorre, Benoît Rouppert, Patrick Sillard et Sébastien Roux pour leur soutien.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Présentation du projet `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/presentation_utilitr.html#licence-du-projet",
    "href": "01_R_Insee/presentation_utilitr.html#licence-du-projet",
    "title": "2  Présentation du projet utilitR",
    "section": "2.5 Licence du projet",
    "text": "2.5 Licence du projet\nL’ensemble du contenu de la documentation est mis à disposition sous licence libre (Licence Ouverte 2.0). Il est donc tout à fait possible de réutiliser une partie ou l’ensemble de la documentation pour l’adapter à un contexte différent. Les conditions de réutilisation sont explicitées dans la licence.\nBien que ce ne soit en rien obligatoire, si vous êtes réutilisateurs de la documentation, n’hésitez pas à vous signaler sur notre Github, vos retours pourraient permettre d’améliorer la documentation, au bénéfice de tous.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Présentation du projet `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_utilitR.html",
    "href": "01_R_Insee/Fiche_utiliser_utilitR.html",
    "title": "3  Comment utiliser la documentation utilitR",
    "section": "",
    "text": "3.1 Contenu de la documentation\nCette documentation vise à aider les agents à réaliser des traitements statistiques usuels avec R et à produire des sorties (graphiques, cartes, documents). Cette documentation présente succinctement les outils les plus adaptés à ces tâches, et oriente les agents vers les ressources documentaires pertinentes. En revanche, elle n’aborde pas les outils les plus avancés, notamment ceux utilisés dans un cadre de développement logiciel.\nTrois points importants sont à noter :",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comment utiliser la documentation `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_utilitR.html#contenu-de-la-documentation",
    "href": "01_R_Insee/Fiche_utiliser_utilitR.html#contenu-de-la-documentation",
    "title": "3  Comment utiliser la documentation utilitR",
    "section": "",
    "text": "Cette documentation recommande les outils et les packages les plus adaptés au contexte d’utilisation de R à l’Insee. Ces recommandations ne sont pas nécessairement adaptées à d’autres contextes, et pourront évoluer lorsque ce contexte évoluera.\nCette documentation ne prétend pas être exhaustive ou sans erreurs. Elle doit être vue comme une mise en commun des connaissances sur R que les agents de la statistique publique ont accumulées dans le cadre de leurs activités.\n\n\nCette documentation recommande d’utiliser R avec RStudio, qui apparaît comme la solution la plus simple et la plus complète pour un usage courant de R, et qui est par ailleurs le choix effectué par l’Insee.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comment utiliser la documentation `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_utilitR.html#structure-de-la-documentation",
    "href": "01_R_Insee/Fiche_utiliser_utilitR.html#structure-de-la-documentation",
    "title": "3  Comment utiliser la documentation utilitR",
    "section": "\n3.2 Structure de la documentation",
    "text": "3.2 Structure de la documentation\nLa documentation utilitR est composée de fiches regroupées en deux grandes parties :\n\nLa première partie explique comment utiliser R et RStudio et les outils associés (git, Gitlab) dans les environnements informatiques proposés à l’Insee (AUSv3 et SSP Cloud).\nLa seconde partie est constituée de fiches thématiques expliquant comment réaliser des tâches standards avec R (importation et manipulation de données, exploitation d’enquêtes, réalisation de graphiques, rédaction de documents…).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comment utiliser la documentation `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_utilitR.html#contenu-des-fiches",
    "href": "01_R_Insee/Fiche_utiliser_utilitR.html#contenu-des-fiches",
    "title": "3  Comment utiliser la documentation utilitR",
    "section": "\n3.3 Contenu des fiches",
    "text": "3.3 Contenu des fiches\nChaque fiche porte sur une tâche précise, décrite dans le titre et éventuellement dans les premières lignes. Elle indique quels sont les packages adaptés pour réaliser la tâche en question, et en présente en détail les principales fonctions. Les fiches n’ont toutefois pas la prétention d’être exhaustives ; c’est pourquoi des références figurent à la fin de chaque fiche de façon à orienter le lecteur vers des ressources plus détaillées.\nPar ailleurs, les fiches comportent quatre types de paragraphes mis en évidence par une icône et une couleur, afin de faciliter la lecture et le repérage des informations importantes.\n\n\n\n\nNom\nSymbole\nSignification\n\n\n\nRecommandation\n&lt;svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:30px;width:30px;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:rgba(220, 53, 69, 1);overflow:visible;position:relative;\"&gt;&lt;path d=\"M448 128l-177.6 0c1 5.2 1.6 10.5 1.6 16l0 16 32 0 144 0c8.8 0 16-7.2 16-16s-7.2-16-16-16zM224 144c0-17.7-14.3-32-32-32c0 0 0 0 0 0l-24 0c-66.3 0-120 53.7-120 120l0 48c0 52.5 33.7 97.1 80.7 113.4c-.5-3.1-.7-6.2-.7-9.4c0-20 9.2-37.9 23.6-49.7c-4.9-9-7.6-19.4-7.6-30.3c0-15.1 5.3-29 14-40c-8.8-11-14-24.9-14-40l0-40c0-13.3 10.7-24 24-24s24 10.7 24 24l0 40c0 8.8 7.2 16 16 16s16-7.2 16-16l0-40 0-40zM192 64s0 0 0 0c18 0 34.6 6 48 16l208 0c35.3 0 64 28.7 64 64s-28.7 64-64 64l-82 0c1.3 5.1 2 10.5 2 16c0 25.3-14.7 47.2-36 57.6c2.6 7 4 14.5 4 22.4c0 20-9.2 37.9-23.6 49.7c4.9 9 7.6 19.4 7.6 30.3c0 35.3-28.7 64-64 64l-64 0-24 0C75.2 448 0 372.8 0 280l0-48C0 139.2 75.2 64 168 64l24 0zm64 336c8.8 0 16-7.2 16-16s-7.2-16-16-16l-48 0-16 0c-8.8 0-16 7.2-16 16s7.2 16 16 16l64 0zm16-176c0 5.5-.7 10.9-2 16l2 0 32 0c8.8 0 16-7.2 16-16s-7.2-16-16-16l-32 0 0 16zm-24 64l-40 0c-8.8 0-16 7.2-16 16s7.2 16 16 16l48 0 16 0c8.8 0 16-7.2 16-16s-7.2-16-16-16l-24 0z\"/&gt;&lt;/svg&gt;\nCe paragraphe présente succinctement les outils et les approches les plus adaptés à la tâche concernée. Chaque fiche ne comprend qu'un seul paragraphe de ce type, au début de la fiche.\n\n\nConseil\n&lt;svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:30px;width:22.5px;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:rgba(255, 193, 7, 1);overflow:visible;position:relative;\"&gt;&lt;path d=\"M297.2 248.9C311.6 228.3 320 203.2 320 176c0-70.7-57.3-128-128-128S64 105.3 64 176c0 27.2 8.4 52.3 22.8 72.9c3.7 5.3 8.1 11.3 12.8 17.7l0 0c12.9 17.7 28.3 38.9 39.8 59.8c10.4 19 15.7 38.8 18.3 57.5H109c-2.2-12-5.9-23.7-11.8-34.5c-9.9-18-22.2-34.9-34.5-51.8l0 0 0 0c-5.2-7.1-10.4-14.2-15.4-21.4C27.6 247.9 16 213.3 16 176C16 78.8 94.8 0 192 0s176 78.8 176 176c0 37.3-11.6 71.9-31.4 100.3c-5 7.2-10.2 14.3-15.4 21.4l0 0 0 0c-12.3 16.8-24.6 33.7-34.5 51.8c-5.9 10.8-9.6 22.5-11.8 34.5H226.4c2.6-18.7 7.9-38.6 18.3-57.5c11.5-20.9 26.9-42.1 39.8-59.8l0 0 0 0 0 0c4.7-6.4 9-12.4 12.7-17.7zM192 128c-26.5 0-48 21.5-48 48c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-44.2 35.8-80 80-80c8.8 0 16 7.2 16 16s-7.2 16-16 16zm0 384c-44.2 0-80-35.8-80-80V416H272v16c0 44.2-35.8 80-80 80z\"/&gt;&lt;/svg&gt;\nCe paragraphe détaille les bonnes pratiques à adopter.\n\n\nRemarque\n&lt;svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:30px;width:30px;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:rgba(0, 123, 255, 1);overflow:visible;position:relative;\"&gt;&lt;path d=\"M256 512A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM216 336h24V272H216c-13.3 0-24-10.7-24-24s10.7-24 24-24h48c13.3 0 24 10.7 24 24v88h8c13.3 0 24 10.7 24 24s-10.7 24-24 24H216c-13.3 0-24-10.7-24-24s10.7-24 24-24zm40-208a32 32 0 1 1 0 64 32 32 0 1 1 0-64z\"/&gt;&lt;/svg&gt;\nCe paragraphe donne des informations supplémentaires ou formule une mise en garde.\n\n\nSpécificité Insee\n&lt;svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 576 512\" style=\"height:30px;width:33.75px;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:rgba(81, 81, 81, 1);overflow:visible;position:relative;\"&gt;&lt;path d=\"M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1V472c0 22.1-17.9 40-40 40H456c-1.1 0-2.2 0-3.3-.1c-1.4 .1-2.8 .1-4.2 .1H416 392c-22.1 0-40-17.9-40-40V448 384c0-17.7-14.3-32-32-32H256c-17.7 0-32 14.3-32 32v64 24c0 22.1-17.9 40-40 40H160 128.1c-1.5 0-3-.1-4.5-.2c-1.2 .1-2.4 .2-3.6 .2H104c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9 .1-2.8V287.6H32c-18 0-32-14-32-32.1c0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7L564.8 231.5c8 7 12 15 11 24z\"/&gt;&lt;/svg&gt;\nCe paragraphe porte sur une spécificité de l'Insee qui a un impact sur l'usage de `R`.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comment utiliser la documentation `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_utilitR.html#des-exemples-reproductibles",
    "href": "01_R_Insee/Fiche_utiliser_utilitR.html#des-exemples-reproductibles",
    "title": "3  Comment utiliser la documentation utilitR",
    "section": "\n3.4 Des exemples reproductibles",
    "text": "3.4 Des exemples reproductibles\nMême si certains lecteurs ont uniquement besoin de parcourir une fiche pour s’en imprégner, d’autres éprouveront le besoin d’exécuter des exemples de code pour se les approprier. C’est pourquoi la documentation utilitR propose un grand nombre d’exemples reproductibles. Cela signifie qu’en chargeant les packages indiqués dans chaque fiche, le lecteur pourra exécuter le code des exemples présentés et reproduire le même résultat.\nLes exemples sont facilement repérables par leur mise en page. Voici un exemple :\n\nresultat &lt;- 1 + 1\nresultat\n\nLe résultat de l’exécution d’un exemple est également facile à repérer. Voici le résultat de l’exemple précédent (qui s’affichera dans la console) :\n\n\n[1] 2\n\n\n\n\n\n\n\n\nTip\n\n\n\nPour reproduire les exemples d’une fiche, il est important de les exécuter tous et dans l’ordre dans lequel ils apparaissent dans la fiche. Si vous ne le faites pas, vous rencontrerez très probablement des erreurs.\nVoici un exemple : la fiche sur la manipulation de données textuelles commence par charger le package stringr avec la commande library(stringr). Si vous n’exécutez pas cette instruction, et que vous essayez d’exécuter le premier exemple de la fiche (str_to_lower(\"Hello world\")), R renverra une erreur, car le package contenant la fonction str_to_lower() n’aura pas été chargé.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comment utiliser la documentation `utilitR`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_utilitR.html#le-package-doremifasoldata",
    "href": "01_R_Insee/Fiche_utiliser_utilitR.html#le-package-doremifasoldata",
    "title": "3  Comment utiliser la documentation utilitR",
    "section": "\n3.5 Le package doremifasolData\n",
    "text": "3.5 Le package doremifasolData\n\nAfin de se rapprocher le plus possible des situations de travail rencontrées par les agents de l’Insee, la plupart des exemples de la documentation utilitR reposent sur des données produites par l’Insee. Ces données sont soit directement disponibles sur le site de l’Insee, soit construites à partir de données disponibles sur le site de l’Insee.\nCes jeux de données sont mis à disposition par l’intermédiaire d’un package nommé doremifasolData développé par les contributeurs du projet utilitR. La documentation détaillée de ce package est disponible sur GitHub.\nVoici la liste des tables disponibles dans doremifasolData :\n\n\nTable\nDescription\n\n\n\nbpe_ens_2018\nBase Permanente des Équipements 2018\n\n\ncog_com_2019\nCode Officiel Géographique 2019\n\n\ndata_iris_paris_2017\nDonnées sociales sur les IRIS de Paris 2017\n\n\nfilosofi_com_2016\nDonnées sur les revenus et la pauvreté en 2016, niveau communal\n\n\nfilosofi_epci_2016\nDonnées sur les revenus et la pauvreté en 2016, niveau EPCI\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLe package tire son nom de son “grand frère”, le package doremifasol. Ce package a pour finalité de charger dans R des données disponibles sur le site de l’Insee, sans que l’utilisateur n’ait ni à naviguer sur ce site, ni à effectuer l’import des données. Tous les jeux de données présents dans doremifasolData ont été téléchargés avec doremifasol.\n\n\n\n3.5.1 Comment installer le package doremifasolData\n\nLe package doremifasolData n’est pas disponible sur le répertoire central des packages R (le CRAN). Pour installer installer le package, il est nécessaire d’exécuter les deux commandes suivantes :\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"InseeFrLab/doremifasolData\", ref = \"main\")\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nSi vous utilisez R sur un poste Insee (y compris en télétravail) ou dans l’environnement de travail AUS, il faut exécuter la commande suivante :\n\ninstall.packages(\n  \"doremifasolData\", \n  repos = \"https://nexus.insee.fr/repository/r-public\",\n  type = \"source\"\n)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Comment utiliser la documentation `utilitR`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rprojects.html",
    "href": "03_Fiches_thematiques/Fiche_rprojects.html",
    "title": "4  Utiliser les projets RStudio",
    "section": "",
    "text": "4.1 Pourquoi utiliser des projets RStudio?",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utiliser les projets `RStudio`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rprojects.html#pourquoi-utiliser-des-projets-rstudio",
    "href": "03_Fiches_thematiques/Fiche_rprojects.html#pourquoi-utiliser-des-projets-rstudio",
    "title": "4  Utiliser les projets RStudio",
    "section": "",
    "text": "4.1.1 Principe\nUn programme est reproductible lorsque tout utilisateur est en mesure de faire tourner l’intégralité du programme sans erreur, et de retrouver des résultats identiques. Assurer la reproductibilité d’un programme est une démarche exigeante qui requiert de développer des codes propres, clairs et bien documentés.\nLes projets RStudio sont des outils qui contribuent à assurer la reproductibilité d’un traitement statistique. Les projets RStudio facilitent la collaboration entre plusieurs développeurs, mais aussi l’organisation du travail lorsqu’on travaille seul.\nEn résumé, un projet RStudio permet de faire trois choses :\n\ntenir à jour la liste des programmes qui doivent être exécutés ensemble ;\nassurer un suivi précis des modifications de ces programmes (en association avec Git) ;\nassurer un suivi de l’ensemble des réglages utilisés pour exécuter les programmes (notamment les options de R).\n\nEn pratique, un projet RStudio prend la forme d’un fichier .Rproj, qui est placé habituellement dans le dossier qui contient les programmes. Voici un exemple minimal:\n\n4.1.2 Avantages des projets RStudio\n\nLe principe d’un projet RStudio est de rassembler tous les éléments de contexte propres à ce projet : espace de travail, historique de commandes, variables d’environnement, options de R… Utiliser un projet RStudio présente donc de multiples avantages :\n\nIl centralise l’ensemble des éléments d’un projet : codes, réglages, documentation, et sorties (articles, présentations) ;\nIl facilite la compréhension du traitement pour les utilisateurs extérieurs et rend plus aisées les évolutions postérieures du projet ;\nIl organise l’interaction entre les fichiers (plusieurs codes, rédaction de documents avec R markdown…) et avec les données ;\nIl renforce la portabilité : le répertoire de travail par défaut d’un projet est le dossier où se situe le fichier .Rproj. Cela rend les scripts indépendants de l’arborescence de la machine. Ainsi, si vous avez un code traitement.R situé dans le même dossier que le fichier .Rproj, alors le chemin de ce code est ./traitement.R, où que soit situé le dossier du projet.\n\nUn point important est qu’un projet RStudio peut contenir beaucoup plus de choses que des codes R :\n\ntous types de scripts, quel qu’en soit le langage (SAS, Stata, R, Python, RMarkdown…) ;\nTous les documents utiles pour la compréhension d’un projet (fichiers txt, pdf…) ;\nTous les fichiers nécessaires à l’utilisation d’outils qui sont fréquemment associés aux projets R (git, packrat, renv, R Markdown…).\n\nVoici un exemple de projet RStudio : le package ci-dessous est organisé de manière à ce que les codes (stockés dans le sous-dossier R) soient associés à une documentation propre à chaque fonction (sous-dossier man) et à une documentation plus globale (sous-dossier inst).",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utiliser les projets `RStudio`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rprojects.html#créer-un-projet-rstudio",
    "href": "03_Fiches_thematiques/Fiche_rprojects.html#créer-un-projet-rstudio",
    "title": "4  Utiliser les projets RStudio",
    "section": "\n4.2 Créer un projet RStudio\n",
    "text": "4.2 Créer un projet RStudio\n\n\n4.2.1 Créer le projet\nPour créer un projet, il suffit de faire File &gt; New Project. Le menu suivant s’affiche :\nIl y a trois méthodes pour créer un projet RStudio :\n\nDans un nouveau dossier (New Directory) :\n\nPour un projet tout nouveau.\n\n\nDans un dossier existant (Existing Directory) :\n\nPour organiser des codes existants sous forme de projet.\n\n\nCloner un projet depuis un dépôt distant (Version Control) :\n\nPour récupérer un projet depuis un dépôt Git distant et se connecter à celui-ci. Cette façon de procéder est détaillée dans la fiche Utiliser Git avec RStudio.\n\n\n\n\nIl est fortement conseillé de créer vos projets RStudio en clonant un projet depuis un dépôt distant, car cette méthode initialise automatiquement l’usage du logiciel de contrôle de version Git. Elle met tout en place pour que vous puissiez facilement suivre les évolutions de votre projet et collaborer avec d’autres personnes.\n\n\n4.2.2 Types de projets possibles\nLes projets RStudio sont une forme très souple d’organisation et s’adaptent facilement à des besoins hétérogènes. RStudio facilite l’organisation du travail en proposant différents projets-types correspondant à des besoins standards : projet de traitement de données, développement d’un package R, développement d’une application shiny, écriture d’un livre avec bookdown…\nCes projets-types se distinguent par le fait qu’ils proposent des organisations différentes pour les fichiers du projet. Ainsi, un projet “Ecriture d’un livre avec bookdown” contient d’emblée une organisation par chapitres, et des fichiers pour gérer la bibliographie, tandis qu’un projet “Développement d’un package” contient des fichiers spécifiques au développement de package. Utiliser un projet-type permet donc d’avoir immédiatement une organisation qui soit cohérente avec l’objectif du projet.\n\n4.2.3 Structure d’un projet RStudio\n\nLa création d’un projet RStudio se traduit par quatre actions, quel que soit le type de projet :\n\nun fichier .Rproj est créé dans le dossier principal. Ce fichier sert à deux choses :\n\nIl centralise les options du projet (présentées dans la suite) ;\nIl sert de raccourci pour ouvrir le projet.\n\n\nun sous-dossier caché (nommé .Rproj.user) est créé dans le dossier principal. Les fichiers temporaires du projet sont stockés dans ce dossier (notamment les scripts sauvegardés automatiquement). Par ailleurs, ce dossier est automatiquement ignoré par .Rbuildignore, .gitignore (ne vous inquiétez pas si vous ne comprenez pas ce que cela signifie).\nle projet est chargé dans RStudio ;\nd’autres fichiers peuvent être créés automatiquement, en fonction du type de projet créé (voir section précédente).\n\nUne fois que le projet est créé, le nom du projet apparaît dans la barre de projet (tout en haut à droite, rectangle rouge). Le menu en bas à droite (onglet Files, rectangle jaune) affiche les fichiers contenus dans le projet.\nOn peut accéder aux derniers projets ouverts avec le raccourci en haut à droite :",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utiliser les projets `RStudio`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rprojects.html#travailler-avec-un-projet-rstudio",
    "href": "03_Fiches_thematiques/Fiche_rprojects.html#travailler-avec-un-projet-rstudio",
    "title": "4  Utiliser les projets RStudio",
    "section": "\n4.3 Travailler avec un projet RStudio\n",
    "text": "4.3 Travailler avec un projet RStudio\n\nL’utilisation d’un projet RStudio modifie un certain nombre de réglages de RStudio. Cette section détaille les principaux points.\n\n4.3.1 Que se passe-t-il lors de l’ouverture d’un projet RStudio?\nLorsqu’on ouvre un projet RStudio (en double-cliquant sur le fichier .Rproj par exemple) :\n\nUne nouvelle session R est ouverte ;\nLes fichiers .Rprofile (variables d’environnement) et .RData (sauvegarde temporaire des données) du projet sont chargés s’ils existent ;\nL’historique des commandes .Rhistory du projet est chargé (s’il existe) dans le cadre History ;\nLe working directory est par défaut le dossier-maître du projet (le dossier dans lequel se situe le fichier .Rproj) ;\nLes documents précédemment ouverts sont restaurés dans l’éditeur RStudio ;\nLes autres paramètres RStudio (e.g. position du curseur, etc.) sont restaurés à leur valeur précédent la fermeture du projet.\n\n4.3.2 Que se passe-t-il lors de la fermeture d’un projet RStudio?\nLorsqu’on ferme un projet RStudio :\n\nLes fichiers .RData et/ou .Rhistory sont sauvegardés dans le projet (si les options du projet l’autorisent) ;\nLes documents ouverts non sauvegardés sont sauvegardés temporairement ;\nLes autres paramètres RStudio sont sauvés ;\nLa session R associée au projet est arrêtée.\n\n\nIl est fortement conseillé de désactiver la sauvegarde du fichier .RData à la fermeture du projet. Pour ce faire, il faut aller dans Tools &gt; Global Options et rester dans l’onglet General. Chercher Save workspace to .RData on exit:, et choisir l’option Never.\nVoici pourquoi : sauvegarder et recharger les données du projet avec le fichier .RData est une très mauvaise pratique en termes de reproductibilité. En effet, il se peut que les données sauvegardées dans le .RData aient subi des modifications non documentées (par exemple via la console), ou que les instructions présentes dans les scripts du projet n’aient pas été exécutées dans l’ordre. Dans ce cas, les données contenues dans le .RData peuvent ne pas être cohérentes avec le projet, et en les utilisant vous risquez de ne pas obtenir les résultats que vous voulez.\n\n\n4.3.3 Définir les options d’un projet RStudio\nIl est possible de définir des options dans un projet RStudio via Tools &gt; Project Options.... Le plus souvent, il suffit de les définir une fois pour toutes, à la création du projet. Vous pouvez en apprendre davantage sur la configuration de R en lisant la fiche [Personnaliser la configuration de R].\n\nOptions générales :\n\n\n(Default): utiliser les options globales ;\n\nRestore .RData into workspace at startup — L’option yes n’est pas recommandée si les données sont volumineuses (ralentissement du démarrage de la session) ;\n\nAlways save history (even when not saving .RData) — Si on choisit yes, le projet conserve l’historique des commandes exécutées. Pour éviter que cet historique soit partagé lorsque git est activé (par exemple si vous avez rentré en commande des mots de passe pour accéder à des services type API), assurez vous que la ligne .Rhistory est présente dans le fichier .gitignore (elle y est par défaut mais il vaut mieux vérifier).\n\n\nOptions build (pour les utilisateurs avancés) :\n\n\nUse devtools package functions if available : permet d’utiliser l’outil devtools pour créer le package ;\n\nGenerate documentation with Roxygen : permet d’automatiser la création des fichiers de documentation à partir du package Roxygen.\nOn peut ajouter des options à la compilation. Par exemple, pour gagner du temps, on pourra utiliser l’option --no-build-vignettes qui évite de re-compiler les vignettes (qui peuvent mettre du temps à compiler si elles sont longues).",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utiliser les projets `RStudio`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rprojects.html#utiliser-les-fonctionnalités-de-rstudio",
    "href": "03_Fiches_thematiques/Fiche_rprojects.html#utiliser-les-fonctionnalités-de-rstudio",
    "title": "4  Utiliser les projets RStudio",
    "section": "\n4.4 Utiliser les fonctionnalités de RStudio\n",
    "text": "4.4 Utiliser les fonctionnalités de RStudio\n\n\n4.4.1 Raccourcis utiles pour l’édition de code\nPour gagner du temps, il est important d’utiliser les raccourcis pour les commandes répétitives. En voici une liste non exhaustive :\n\n\nCtrl+Enter : exécuter les lignes sélectionnées;\n\nTab : autocomplétion;\n\nCtrl + Alt + R : exécuter l’ensemble d’un script;\n\nCtrl + I : réindenter code;\n\nCtrl + Shift + A : reformater code;\n\nAlt + Up/Down : déplacer une ligne sans faire copier-coller;\n\nCtrl + Deplacement souris : passage en édition verticale (curseur en colonne);\n\nCtrl + Shift + M : insérer pipe %&gt;%;\n\nAlt+ 6 (Windows) : insérer opérateur assignation &lt;-;\n\nCtrl + Shift + F : rechercher une expression dans plusieurs fichiers;\n\nCtrl + Shift + K : compilation d’un markdown (knitter en bon français);\n\nCtrl + Shift + Alt + M : renommer la variable dans la portée (scope).\n\n\n\n4.4.2 Les addins RStudio\n\n\n4.4.2.1 Addins\nLes addins sont des outils pratiques pour gagner du temps dans l’édition du code. Ils génèrent automatiquement du code ou du texte utile pour les markdown. Par exemple, il est possible d’utiliser un addin pour construire un graphique avec le package ggplot2, puis de récupérer le code qui produit le graphique.\nLes addins sont bien intégrés à RStudio, et donc faciles à utiliser. Pour avoir accès à la liste de tous les addins, il suffit d’exécuter la commande install.packages('addinslist'), puis de cliquer en haut de l’éditeur sur Addins &gt; Browse RStudio addins.\nPour utiliser un addin il faut :\n\nl’installer. Pour cela on peut utiliser le package addinslist, en cliquant en haut de l’éditeur sur Addins &gt; Browse RStudio addins, puis en cliquant sur le nom de l’addin qu’on veut installer. [Avancé] Rappel: Les addins sur CRAN peuvent être installés directement; les addins sur github nécessitent d’avoir configuré le proxy.\nL’exécuter en cliquant sur son nom en haut de l’éditeur Addins &gt; [addin_name].\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nLes addins RStudio sont disponibles sur le SSP Cloud.\nSur un poste local Insee, il est nécessaire de tenir compte du proxy en configurant les variables d’environnement https_proxy et http_proxy dans un fichier .Renviron. Pour éditer ce fichier, taper dans la console R :\n\nusethis::edit_r_environ(scope = \"user\")\n\net assigner aux variables https_proxy et http_proxy l’adresse du proxy (qui peut être obtenue en tapant curl::ie_get_proxy_for_url()). Redémarrer R pour que ces changements soient effectifs (les utilisateurs de RStudio peuvent utiliser le raccourci clavier CTRL+MAJ+F10). Contrairement à ce qui est indiqué sur le site de RStudio, il n’est néanmoins pas nécessaire d’ajouter les variables http_proxy_user et https_proxy_user dans lesquelles seraient stockés d’hypothétiques mots de passe.\nEnfin, dans l’environnement AUS, il est certes possible d’installer des addins présents sur le CRAN mais le package addinslist ne fonctionnera pas (car il va chercher la liste des addins sur internet, auquel on ne peut pas accéder depuis AUS).\n\n\n\n4.4.2.2 Quelques addins utiles\n\n\nesquisse : apprendre à réaliser des graphiques avec ggplot2 ;\n\nsnippetsaddins: convertir automatiquement les / en \\ pour avoir des chemins valides ;\n\n\n“monchemin/toto/fichier.csv” -&gt; “monchemin\\toto\\fichier.csv”\n\n\n\n\nggedit: aide pour générer un code pour graphiques ggplot2 ;\n\nJADD: en sélectionnant une fonction, on assigne les paramètres par défaut dans l’environnement ;\n\nquestionr: recodage facilité des variables factor.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Utiliser les projets `RStudio`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html",
    "title": "5  Utiliser Git avec RStudio",
    "section": "",
    "text": "5.1 Tâches concernées et recommandations\nL’utilisateur souhaite se servir de Git avec RStudio pour suivre les modifications d’un projet RStudio.\nCette fiche est une introduction à l’usage de Git avec RStudio, pas une introduction générale à Git. Il est donc préférable que l’utilisateur ait une connaissance basique de Git avant de lire cette fiche. Si ce n’est pas le cas, il est conseillé de consulter la formation Travail collaboratif avec R. Il est également souhaitable de lire au préalable la fiche Configurer Git sur son poste de travail et de vérifier que les éléments de configuration de Git présentés dans cette fiche sont fonctionnels.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#tâches-concernées-et-recommandations",
    "title": "5  Utiliser Git avec RStudio",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\nRecommandations générales sur l’usage de Git\n\nUtiliser systématiquement Git pour suivre les modifications des codes d’un projet RStudio ;\nNe jamais utiliser Git pour sauvegarder les données ;\nUtiliser l’interface graphique de RStudio pour les usages courants de Git, plutôt que la ligne de commande ;\n\nVous pouvez utiliser Git via la ligne de commande si vous le souhaitez. En revanche, il est très fortement déconseillé d’exécuter une commande qui comprendrait les termes force ou rebase.\nRecommandations sur l’initialisation de Git\n\nIl est recommandé d’utiliser Git dès le lancement de votre projet, même s’il est possible de commencer à suivre un projet déjà existant avec Git.\nIl est recommandé de commencer par créer un dépôt distant sur une forge (Gitlab, Github…), puis de clôner ce dépôt pour travailler sur votre poste local.\nIl est recommandé de renseigner immédiatement le fichier .gitignore afin d’exclure certains fichiers du suivi des modifications (notamment les fichiers de données).\n\nRecommandations sur l’usage des branches\n\nUtiliser RStudio pour créer une branche ou naviguer entre les branches ;\nUtiliser les interfaces Github ou Gitlab pour fusionner deux branches. Sur ce point, vous pouvez consulter la formation Travail collaboratif avec R.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#glossaire-des-termes-techniques",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#glossaire-des-termes-techniques",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.2 Glossaire des termes techniques",
    "text": "5.2 Glossaire des termes techniques",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#pourquoi-utiliser-git",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#pourquoi-utiliser-git",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.3 Pourquoi utiliser Git\n",
    "text": "5.3 Pourquoi utiliser Git\n\nTous les statisticiens se sont déjà demandés (ou ont demandé à leurs collègues) :\n\nquelle était la bonne version d’un programme ;\nqui était l’auteur d’un bout de code en particulier ;\nsi un changement était important ou juste un essai ;\ncomment fusionner deux versions du même programme modifié par deux personnes différentes ;\ncomment travailler à plusieurs sur les mêmes codes…\n\nLa réponse à toutes ces questions est : utiliser Git. Git est un outil qui permet de suivre en détail les évolutions d’un projet impliquant du code informatique, et qui facilite l’archivage des codes et la collaboration entre agents. Les avantages de Git sont multiples :\n\n\nGit contient l’historique de toutes les modifications apportées à un fichier ;\n\nGit permet de revenir facilement à une version antérieure d’un fichier ;\n\nGit permet de travailler à plusieurs en même temps sur les mêmes fichiers, de façon cohérente et sans risque de confusion ;\n\nGit permet de proposer des modifications sur des fichiers et les discuter, sans pour autant modifier la dernière version existante…\n\nGit fonctionne avec tous les langages informatiques (R, Python, SAS, LaTeX, C/C++, Java, etc.) et n’est pas spécifique à R.\n\nEn un mot, Git est le bon outil pour partager des codes et travailler à plusieurs sur un projet statistique (études ou production). Si vous n’êtes pas encore convaincu, une liste plus longue des raisons d’utiliser Git est disponible ici.\n\n\n\n\n\n\nNote\n\n\n\nCette fiche décrit l’utilisation de Git au travers de l’interface graphique RStudio, car celle-ci facilite l’apprentissage de Git pour les débutants. Toutefois, utiliser cette interface n’est nullement obligatoire, et il est possible de réaliser l’ensemble des gestes présentés dans cette fiche avec des lignes de commande. Pour les utilisateurs intéressés par la ligne de commande, le nom de la commande git concerné sera systématiquement mentionné.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#clone",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#clone",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.4 Initialiser l’usage de Git dans un projet RStudio",
    "text": "5.4 Initialiser l’usage de Git dans un projet RStudio\nIl est nettement préférable de commencer à suivre les modifications d’un projet RStudio avec Git dès la création du projet. Il est toutefois possible de commencer à suivre les modifications du projet à n’importe quel moment.\n\n5.4.1 Initialiser l’usage de Git à la création d’un projet\n\n\n5.4.1.1 Etape 1 : Créer le dépôt distant sur la forge\nLa première étape consiste à créer un dépôt vide sur la forge. C’est grâce à ce dépôt que vous allez pouvoir partager des codes avec vos collègues ou stocker vos codes personnels. Deux points sont importants sur la création du dépôt distant :\n\n\nCette étape ne doit être réalisée qu’une seule fois par projet. En particulier, vous n’avez pas à recréer de dépôt distant si vous changez d’ordinateur, ou si les contributeurs du projet changent.\n\nIl se peut que le dépôt distant existe déjà, par exemple si vous rejoignez un projet existant, ou si un de vos collègues a déjà créé le dépôt. En ce cas, vous pouvez passer directement à l’étape 2.\n\nLa marche à suivre pour créer un dépôt distant est très similaire dans Gitlab et dans Github. Les seules différences entre les deux plateformes portent sur la dénomination (un dépôt s’appelle repository sur Github et project sur Gitlab) et sur l’endroit où il faut cliquer pour créer un dépôt :\n\nSur Gitlab, il suffit de cliquer sur New project dans la partie supérieure de la fenêtre :\n\n\nSur Github, il suffit de cliquer sur l’icône + en haut à droite (cadre rouge), puis sur New repository (flèche noire) :\n\nDans les deux cas, vous devez préciser quelques caractéristiques du dépôt :\n\nle nom du dépôt ;\nla visibilité du dépôt (privé ou public) ;\nle contenu de départ du dépôt : le dépôt peut être créé vide, avec un fichier README minimal, ou selon un modèle de dépôt (template). Il est conseillé de créer le dépôt avec un fichier README.\n\n5.4.1.2 Etape 2 : Récupérer l’url du dépôt distant\nLa deuxième étape consiste à récupérer l’adresse du dépôt distant. Cette adresse se termine toujours par .git. Par exemple, l’adresse du dépôt de la documentation utilitR est https://github.com/InseeFrLab/utilitR.git.\nPour récupérer cette adresse, vous devez d’abord vous rendre sur la page du projet sur la forge (comme Gitlab ou Github). Pour afficher l’adresse du dépôt, il faut ensuite cliquer sur un bouton déroulant à droite. Sur Gitlab, il s’agit du bouton Clone (bouton bleu) :\nSur Github, il s’agit du bouton Code (bouton vert) :\nChoisir la méthode d’authentification désirée, SSH ou HTTPS (cf. fiche XXXX). Par la suite, nous allons supposer qu’on utilise une authentification HTTPS.\n\n5.4.1.3 Etape 3 : Créer un projet RStudio en clonant le dépôt distant\nLa troisième étape consiste à cloner le dépôt distant (git clone) c’est-à-dire à créer un projet RStudio sur votre poste local qui est une copie du dépôt distant (voir fiche XXXX pour plus de détails sur les projets RStudio). Cliquer sur File &gt; New Project..., puis choisissez la troisième option (Version Control) :\nIl faut alors renseigner trois champs dans la fenêtre suivante :\n\n\nRepository URL : coller l’adresse du dépôt distant récupérée à l’étape précédente ;\n\nProject directory name : choisir le nom du dossier local qui contiendra le projet RStudio ;\n\nCreate project as subdirectory of : : définir l’emplacement de votre dépôt local dans l’aborescence de votre poste. Il est possible de modifier cet emplacement en cliquant sur le bouton Browse.\n\nAprès avoir validé, RStudio ouvre un projet RStudio. Si RStudio ne mentionne aucune erreur, un nouvel onglet portant le nom de Git doit normalement apparaître en haut à droite :\n\n5.4.2 Initialiser l’usage de Git dans un projet RStudio déjà existant\nBien qu’il soit recommandé d’utiliser Git dès la création d’un projet RStudio, il est également possible de commencer à utiliser Git pour suivre les modifications d’un projet RStudio déjà existant. Vous pouvez utiliser cette méthode lorsque vous avez commencé à travailler seul sur un projet RStudio, et que vous souhaitez le partager avec des collègues ou mieux suivre les modifications du projet.\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nLa méthode décrite ci-dessous n’est applicable que dans AUS, pas sur le SSP-Cloud.\n\n\n\n5.4.2.1 Etape 1 : Initialiser le suivi des modifications du projet RStudio\nLa première étape consiste à initialiser l’utilisation de Git dans le projet RStudio en exécutant la fonction usethis::use_git() du package usethis. Cette fonction met en place l’environnement informatique permettant d’utiliser Git sur votre poste local. En revanche, elle ne crée pas de lien entre votre projet local et un dépôt distant.\n\nusethis::use_git()\n\nCette fonction réalise deux opérations :\n\nelle crée automatiquement tous les fichiers de configuration nécessaires au bon fonctionnement de Git dans le projet sur lequel vous travaillez ;\nelle effectue un premier commit (voir cette section) pour valider la création de ces fichiers de configuration.\n\nLa fonction usethis::use_git() va finalement vous proposer de sauvegarder les fichiers qui figurent déjà dans votre projet (par exemple, les codes R et les fichiers csv présents dans votre projet). Si vous avez des fichiers qui ne doivent pas être sous contrôle de version (notamment des données), il est préférable de refuser cette possibilité (en choisissant le chiffre qui correspond à “Non” parmi la liste de choix), puis de procéder comme ceci :\n\najoutez les lignes nécessaires au .gitignore (voir partie sur le fichier .gitignore pour plus de détails) ;\nvalider l’ensemble des fichiers maintenant que ces fichiers ont été exclus (voir la partie sur la sauvegarde des modifications de fichiers).\n\n5.4.2.2 Etape 2 : Créer le dépôt distant sur la forge\nCette étape est identique à celle qui est décrite dans la section Créer le dépôt distant sur la forge.\n\n5.4.2.3 Etape 3 : Mettre en place le dépôt distant\nLa troisième étape consiste à établir un lien entre votre projet local et un dépôt distant (git add remote). Dans le langage Git, le dépôt distant s’appelle un remote. Pour créer ce lien avec l’interface de RStudio, il faut se rendre dans la partie destinée à la gestion des branches sur laquelle la partie RStudio et les branches revient plus en détails.\nAprès avoir cliqué sur ce bouton, dans la fenêtre s’ouvrant, cliquer sur Add remote :\nDans cette nouvelle fenêtre, deux champs sont à renseigner :\n\n\nRemote name : le nom du dépôt distant. Par convention, on utilise le nom origin (mais vous pouvez lui donner un autre nom) ;\n\nRemote URL : l’url du dépôt distant (cette adresse se termine par .git, voir plus haut).\n\nA ce stade, Git est maintenant capable de contrôler les versions des fichiers locaux et d’interagir avec le dépôt. L’environnement nécessaire pour pouvoir réaliser les opérations usuelles, présentées dans la partie Suivre les modifications avec RStudio est ainsi opérationnel.\n\n5.4.3 Exclure certains fichiers du suivi de modifications\nCertains fichiers peuvent ou doivent être exclus du suivi des modifications. En particulier, les fichiers suivants doivent toujours être exclus du suivi des modifications :\n\nles fichiers de données (csv, SAS, Excel, txt…) ;\nles fichiers de sortie (pdf, html, csv…) ;\nle fichier .Renviron (voir la fiche [Personnaliser la configuration de R]) ;\nles codes contenant des informations confidentielles (comme les mots de passe, les tokens ou les clés d’API).\n\nLes fichiers exclus du suivi des modifications sont répertoriés dans un fichier nommé .gitignore. Si le projet a été créé selon l’une des deux méthodes décrites précédemment, RStudio a créé automatiquement un fichier .gitignore dans le dossier du projet, et ce fichier contient par défaut les lignes suivantes :\n.Rhistory\n.RData\n.Rproj.user\nIl est souvent nécessaire d’ajouter des lignes au fichier .gitignore pour exclure certains fichiers du suivi des modifications. Pour éditer le fichier, il suffit d’exécuter la commande usethis::edit_git_ignore(scope = \"project\"). Par exemple, vous pouvez ajouter les lignes suivantes pour exclure tous les fichiers de format csv, SAS et Excel du suivi des modifications :\n*.csv\n*.sas7bdat\n*.xls\n*.xlsx\nIl est également conseillé d’exclure les fichiers pouvant être produits par des documents R Markdown (notamment les fichiers pdf et html). Par exemple, pour exclure tous les fichiers de formats PDF et HTML, les lignes suivantes suffisent :\n*.html\n*.pdf\nDans l’onglet Git, RStudio propose également un bouton Ignore qui permet d’éditer le fichier .gitignore directement dans une interface graphique.\nCette fenêtre s’affiche ensuite dans RStudio :\n\n\n\n\n\n\nNote\n\n\n\nLe site https://www.toptal.com/developers/gitignore fournit un certain nombre de modèles de fichiers .gitignore selon le type de projet associés à Git. Vous pouvez également utiliser ce modèle pour constituer une base de départ.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#git-local",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#git-local",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.5 Suivre les modifications d’un projet RStudio avec Git\n",
    "text": "5.5 Suivre les modifications d’un projet RStudio avec Git\n\n\n5.5.1 Présentation de l’interface RStudio\nRStudio permet d’utiliser Git via une interface graphique, accessible dans l’onglet Git situé en haut à droite de la fenêtre RStudio.\nC’est grâce à cette interface que vous pourrez effectuer la plupart des opérations Git courantes, qui sont présentées ci-dessous. Les principales commandes de cette interface sont les suivantes :\n\n\nCommit : valider les modifications d’un ou plusieurs fichier(s) ;\n\nPull : récupérer sur le dépôt distant les modifications apportées au projet par d’autres contributeurs ;\n\nPush : partager sur le dépôt distant les modifications que vous avez apportées au projet ;\n\nHistory : consulter l’historique des modifications du projet ;\n\nMore : ajouter des fichiers à ignorer ou revenir en arrière ou utiliser le terminal ;\n\nNew Branch : créer une nouvelle branche dans le dépôt Git ;\nle menu déroulant tout à droite permet de changer de branche.\n\n\n\n\n\n\n\nNote\n\n\n\nSi l’onglet Git n’apparaît pas dans la fenêtre RStudio, c’est que votre projet RStudio n’est pas encore configuré pour utiliser Git. Reportez-vous à la section Initialiser l’usage de Git dans un projet RStudio.\n\n\n\n5.5.2 Repérer les modifications apportées au projet avec RStudio\nL’onglet Git vous indique le statut des différents fichiers avec une petite icône colorée à gauche du nom du fichier (dans le cadre rouge) :\nVoici les statuts les plus courants :\n\n\n\n(modified) : les modifications de ce fichier sont suivies par Git, et le fichier a été modifié depuis la dernière fois que ses modifications ont été sauvegardées dans l’historique de Git ; *\n(deleted) : les modifications de ce fichier sont suivies par Git, et le fichier a été supprimé (ou éventuellement renommé) depuis la dernière fois que ses modifications ont été sauvegardées dans l’historique de Git ; *\n(unknown) : les modifications de ce fichier ne sont pas suivies par Git (autrement dit, ce fichier n’est pas sauvegardé).\n\n5.5.3 Sauvegarder les modifications de vos fichiers\n\n5.5.3.1 Principe\nLorsqu’on souhaite sauvegarder les modifications faites à un ou plusieurs fichiers, il est nécessaire d’effectuer deux opérations :\n\n\ngit add : sélectionner les modifications qui vont être ajoutées à l’historique des modifications de Git (dont le nom technique est index) ;\n\ngit commit : valider les modifications choisies à l’étape précédente et ajouter une entrée à l’index de Git.\n\nLes modifications ne seront enregistrées qu’à l’issue de ces deux opérations, que vous pouvez réaliser soit avec la ligne de commande, soit avec l’interface RStudio. La suite de cette section détaille la marche à suivre avec l’interface RStudio.\nSupposons qu’un fichier source a été modifié et qu’on souhaite valider ces modifications. Ci-dessus, le dépôt comporte plusieurs fichiers modifiés. On souhaite valider les modifications apportées au fichier 03_Fiches_thematiques/Fiche_git_utilisation.Rmd et ajouter le fichier pics/git/onglet_git1.png.\n\n5.5.3.2 Etape 1 : Sélectionner les modifications à sauvegarder\nLa première étape (git add) consiste à sélectionner les fichiers dont on veut sauvegarder les modifications. Pour ce faire, il faut cliquer sur la case vide à gauche du nom du fichier. Cela change le statut des fichiers :\nIl est évidemment possible de sélectionner plusieurs fichiers. Les modifications à sauvegarder ont été sélectionnées, et sont maintenant dans la salle d’attente. Deux statuts supplémentaires peuvent apparaître lorsqu’on sélectionne des modifications à sauvegarder :\n\n\n\n(added) : un nouveau fichier est ajouté à l’historique de Git ; *\n(renamed) : un fichier a été renommé, et l’opération de changement de nom est sélectionnée.\n\n5.5.3.3 Etape 2 : Valider la sauvegarde des modifications\nLa seconde étape (git commit) consiste à valider les modifications choisies à l’étape précédente. Pour ce faire, il faut cliquer sur le bouton commit, situé au-dessus de la liste des fichiers modifiés. En cliquant sur ce bouton, une nouvelle fenêtre s’ouvre :\nLes modifications apportées à chaque fichier sont résumées ligne à ligne dans la partie inférieure (suppressions en rouge, ajouts en vert). Pour valider ces modifications (commit), il faut écrire un message qui décrit la nature des modifications sauvegardées (dans le cadre vert) puis cliquer sur le bouton commit (ovale rouge). Une fenêtre s’ouvre alors et confirme que la modification a été sauvegardée :\n\n5.5.4 Consulter l’historique d’un dépôt ou d’un fichier\nIl est possible, depuis RStudio, de consulter la liste des modifications apportées à un dépôt en cliquant sur le bouton History (cadre rouge). En ligne de commande, il faudrait taper git log pour obtenir une information équivalente.\nCette action ouvre une fenêtre avec la liste des commits réalisée sur la branches (voir plus bas pour la notion de branches), avec les modifications apportées par chaque commit. Cette interface permet de balayer rapidement les modifications apportées aux fichiers, par exemple pour retrouver à quel moment une ligne de code a été modifiée.\n\n5.5.5 Revenir en arrière sur un fichier\nAprès avoir cliqué sur le bouton en forme d’écrou, RStudio propose un second bouton Revert qui permet d’annuler les dernières modifications effectuées sur un fichier. Ce bouton peut être pratique si vous souhaitez reconstituer rapidement l’état d’un fichier tel qu’il était enregistré lors du dernier commit.\nEn revanche, si vous souhaitez revenir à un état plus ancien d’un fichier, il n’existe pas de bouton dans RStudio pour faire cela et il faut dans ce cas utiliser les lignes de commande Git grâce au terminal.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#interagir-avec-le-dépôt-distant",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#interagir-avec-le-dépôt-distant",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.6 Interagir avec le dépôt distant",
    "text": "5.6 Interagir avec le dépôt distant\nAprès avoir effectué des opérations sur son dépôt local, on peut effectuer deux opérations pour interagir avec le dépôt distant. Les boutons dans RStudio illustrent bien les opérations en question :\n\npull\n\n: récupérer les modifications présentes sur le dépôt distant ; * push\n: envoyer les modifications faites en local sur le dépôt distant.\nLorsque vous partagez les modifications que vous avez apportées au projet, vos modifications viennent s’ajouter à la dernière version commune disponible (celle qui figure dans le dépôt commun). C’est pourquoi il est nécessaire que votre dépôt local soit à jour de toutes les modifications partagées par les autres contributeurs avant de partager vos modifications. Il faut donc toujours récupérer la dernière version du projet sur le dépôt distant (en cliquant sur pull) avant d’envoyer des modifications (en cliquant sur push).\n\n\n\n\n\n\nNote\n\n\n\nLes icônes de l’onglet Git sont placées dans l’ordre des actions à réaliser afin de publier un code sur un dépôt distant : (add), commit, pull, push\n\n\nIl peut arriver que d’autres contributeurs aient partagé sur le dépôt distant des modifications du projet en même temps que vous avez apporté des modifications à votre copie locale du projet. Lorsque vous récupérez les modifications présentes sur le dépôt distant avec pull, Git essaie de fusionner automatiquement vos modifications avec celles des autres contributeurs. Il peut néanmoins arriver que ces modifications soient incompatibles, par exemple si deux modifications portent sur la même ligne du même fichier : c’est un conflit de version. En cas de conflit, Git indiquera que la fusion automatique a échoué et vous demandera de choisir par vous-mêmes la bonne version des codes : vous devez alors résoudre un conflit de version. Plus de détails sont disponibles dans la formation Travail collaboratif avec R.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#git-branches",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#git-branches",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.7 RStudio et les branches",
    "text": "5.7 RStudio et les branches\n\n5.7.1 La notion de branche\n\nLes branches sont l’une des fonctionnalités les plus puissantes de la gestion de version. La création de branches dans un projet permet de développer des codes sans modifier la version de référence des codes, et donc de faire cohabiter dans un projet plusieurs versions des mêmes codes sans risque de confusion. Git propose également des fonctions pour fusionner facilement des versions différentes des mêmes codes. Deux points sont particulièrement importants :\n\nDans un projet collaboratif, il existe une branche particulière, appelée le plus souvent main ou master, qui joue le rôle de version de référence. C’est autour d’elle que se développent les branches.\nLes modifications faites sur les codes dans une branche n’altèrent pas la version de référence (celle de la branche master) tant qu’il n’y a pas de fusion entre branches (merge).\n\n5.7.2 Créer et utiliser des branches avec RStudio\nLa partie droite de l’interface git de RStudio permet de travailler avec des branches.\nCette interface offre principalement deux fonctions :\n\n\nCréer une nouvelle branche (cadre orange) : si vous cliquez sur ce bouton, une boîte de dialogue s’ouvre, dans laquelle vous devrez définir le nom de la nouvelle branche L’instruction correspondante en ligne de commande est git checkout -b ma-branche, où ma-branche est le nom de la nouvelle branche Deux points sont à noter:\n\nLa branche source, à partir de laquelle la nouvelle branche diverge, sera celle sur laquelle vous vous situez au moment de la création.\nIl est recommandé de se placer sur master avant chaque création de nouvelle branche.\n\n\n\nNaviguer parmi les branches (cadre bleu) : le menu déroulant affiche d’abord la liste des branches disponibles dans votre dépôt local (LOCAL BRANCHES) puis la liste des branches existantes sur le dépôt distant (REMOTE: ORIGIN). Pour afficher le contenu d’une branche, il suffit de cliquer sur le nom de cette branche. L’instruction correspondante en ligne de commande est git checkout ma-branche, où ma-branche est le nom de la branche dont vous souhaitez afficher le contenu.\n\n\n\n\n\n\n\nNote\n\n\n\nL’interface graphique de RStudio ne permet pas de réaliser toutes les opérations possibles sur les branches. En particulier, il n’est pas possible de fusionner des branches avec cette interface. Les interfaces des forges (telles que Gitlab, Github…) sont beaucoup plus adaptées pour mener ce type d’opération. Vous pouvez en apprendre davantage en consultant la formation Travail collaboratif avec R.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#terminal-git",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#terminal-git",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.8 Utiliser le terminal",
    "text": "5.8 Utiliser le terminal\nUtiliser Git avec la ligne de commande permet de faire davantage de choses qu’avec l’interface RStudio. Grâce à elle, on peut par exemple corriger une maladresse, ou fusionner des branches sans utiliser l’interface d’une forge. L’utilisation de la ligne de commande est toutefois moins facile pour les débutants.\nL’utilisation de la ligne de commande n’est pas plus risquée que l’interface RStudio, à condition de respecter une règle simple : il ne faut jamais exécuter une commande qui comprend les termes force ou rebase. En effet, ces options peuvent créer des conflits insolubles, voire rendre le projet inutilisable par vos collaborateurs.\nPour ouvrir un terminal Git, cliquer sur l’écrou dans l’onglet Git:\nCette action ouvre un terminal Git bash permettant de taper :\n\ndes commandes Linux, par exemple ls (lister les fichiers) ;\ndes commandes Git, par exemple git status (lister l’état des fichiers dans Git).\n\nVous pouvez en apprendre davantage sur l’utilisation de la ligne de commande dans la formation Travail collaboratif avec R ).",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_git_utilisation.html#RessourcesUsageGit",
    "href": "03_Fiches_thematiques/Fiche_git_utilisation.html#RessourcesUsageGit",
    "title": "5  Utiliser Git avec RStudio",
    "section": "\n5.9 Pour en savoir plus",
    "text": "5.9 Pour en savoir plus\n\nTravailler avec Git via RStudio et versionner son code (thinkr.fr)\n\nHappy Git with R\n\n\nUne présentation vidéo d’une utilisation Git pour débutants faite à l’Insee\n\nFormation à l’utilisation de Git avec RStudio (SSM Agriculture)",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Utiliser `Git` avec RStudio</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_installer_packages.html",
    "href": "01_R_Insee/Fiche_installer_packages.html",
    "title": "6  Utiliser des packages R",
    "section": "",
    "text": "6.1 Introduction aux packages R",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Utiliser des _packages_ `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_installer_packages.html#introduction-aux-packages-r",
    "href": "01_R_Insee/Fiche_installer_packages.html#introduction-aux-packages-r",
    "title": "6  Utiliser des packages R",
    "section": "",
    "text": "6.1.1 Qu’est-ce qu’un package R ?\nUn package est un ensemble de fonctions développées par des utilisateurs de R, qui améliorent ou étendent les fonctionnalités de base de R.\nPar exemple, le package ggplot2 propose des fonctions pour réaliser toutes sortes de graphiques avec R. Les packages sont au coeur de la logique collaborative de R, car ils permettent de bénéficier des contributions d’autres utilisateurs de R. Début 2021, il existe plus de 14 000 packages disponibles pour R, qui couvrent une multitude de tâches. Il est donc primordial de savoir où chercher des packages et comment les utiliser.\nD’un point de vue technique, un package est constitué d’objets R (la plupart du temps des fonctions, parfois des données) et d’une documentation empaquetés en un seul fichier, dans le but d’être partagé facilement. En outre, les packages R ont trois caractéristiques importantes :\n\nUn package R peut exiger une version minimale de R pour fonctionner. Par exemple, le package ggplot2 (qui sert à faire des graphiques) ne peut fonctionner qu’avec R 3.2 au minimum. Vous pouvez donc être contraints de modifier la version de R que vous utilisez pour pouvoir installer un package. En pratique cela arrive rarement.\nLes packages R sont spécifiques à la version de R avec laquelle ils ont été installés. Ainsi, un package installé avec la version 3.6 de R peut ne pas fonctionner avec les versions 3.5 ou 4.0. La principale conséquence pratique de cette caractéristique est que vous devrez réinstaller les packages si vous modifiez la version de R que vous utilisez. Cette réinstallation ne présente généralement aucune difficulté technique, mais elle peut prendre du temps.\nLes packages R font fréquemment appel à d’autres packages appelés dépendances : la fonction fonction1() du package package1 fait appel à la fonction fonction2() du package package2, et ainsi de suite. Un package R ne pourra être utilisé que si ses dépendances sont également installées. Fort heureusement, la plupart du temps vous n’avez pas à vous préoccuper des dépendances de packages car R installe par défaut toutes les dépendances nécessaires.\n\n6.1.2 Où peut-on trouver des packages R ?\nLes packages R sont disponibles sur des sites spécialisés appelés dépôts (repositories ou repo en anglais). Le principal dépôt est le dépôt officiel du projet R, le CRAN (Comprehensive R Archive Network).\nLes packages peuvent également être mis à disposition par leurs auteurs sur les forges logicielles (telles que Github et Gitlab), mais il s’agit alors la plupart du temps de versions expérimentales dont l’usage est principalement destiné aux utilisateurs avancés de R.\n\n\n\n\n\n\nNote\n\n\n\nLe CRAN dispose de plusieurs copies, appelées miroirs, dont la plupart sont hébergés sur des serveurs d’universités ou d’établissements de recherche. Télécharger un package sur un miroir ou un autre est strictement équivalent. L’intérêt de choisir un miroir proche de sa localisation permet simplement de gagner en vitesse et de réduire les flux sur le réseau.\n\n\n\n6.1.3 Comment utiliser un package R ?\nPour utiliser un package R, il est nécessaire de réaliser deux actions, qui sont détaillées dans la suite de cette fiche :\n\nl’installation consiste à télécharger le package sur internet, puis à l’installer sur l’ordinateur, dans un dossier connu de R. La fonction principale pour installer un package est install.packages ;\nle chargement consiste à indiquer à R que l’on souhaite utiliser le package dans la session courante. La fonction principale pour charger un package est library.\n\nIl est important de comprendre que ces deux actions sont complètement distinctes. L’installation est une opération qu’on ne réalise qu’une seule fois : une fois qu’un package a été installé sur un ordinateur, il y est présent de façon permanente. A noter qu’il existe toutefois des raisons de réinstaller un package, par exemple pour le mettre à jour. Inversement, le chargement d’un package est une opération qu’il faut réaliser à chaque fois que vous ouvrez une session R.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Utiliser des _packages_ `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_installer_packages.html#installer-un-package-r",
    "href": "01_R_Insee/Fiche_installer_packages.html#installer-un-package-r",
    "title": "6  Utiliser des packages R",
    "section": "\n6.2 Installer un package R\n",
    "text": "6.2 Installer un package R\n\n\n6.2.1 La méthode standard : la fonction install.packages\n\nLa façon la plus commune (et commode) d’installer un package est d’utiliser la fonction install.packages. Cette fonction réalise successivement deux actions :\n\nElle télécharge la dernière version du package sur un dépôt distant de packages R (typiquement le dépôt officiel) ;\nElle installe le package sur la machine de l’utilisateur, dans un dossier appelé librairie.\n\nLa fonction install.packages propose de multiples options, dont les principales sont décrites ci-dessous. Il est toutefois recommandé de ne pas modifier les réglages par défaut et d’utiliser cette fonction sous sa forme la plus simple :\n\ninstall.packages(\"monBeauPackage\")\n\nTrois points sont à noter sur l’utilisation de la fonction install.packages :\n\nCette fonction requiert que vous disposiez d’un accès à internet (pour télécharger les packages) ;\nLes noms des packages sont sensibles à la casse : il est donc important de bien respecter majuscules et minuscules. Par exemple, la commande install.packages(\"Ggplot2\") ne fonctionnera pas, car le package s’appelle en réalité ggplot2 (en minuscules) ;\n\nCette fonction accepte en paramètre un vecteur de noms de packages, afin d’installer plusieurs packages en une seule fois :\n\ninstall.packages(c(\"package1\", \"package2\", \"package3\"))\n\n\n\n\nUne fois qu’un package a été installé sur un ordinateur, il y est présent de façon permanente. Il est donc conseillé de ne pas inclure les instructions d’installation des packages dans les programmes R, pour éviter des réinstallations superflues.\n\n\n6.2.2 Utilisation de la fonction install.packages\n\n\n6.2.2.1 Choix du dépôt\nPar défaut, la fonction install.packages télécharge le package demandé sur le dépôt officiel du projet R, le CRAN. Il est possible de modifier le dépôt utilisé grâce à l’argument repos de install.packages, mais vous n’avez normalement pas besoin de le faire dans le cadre d’un usage standard de R.\n\nL’installation de packages dans AUS fonctionne de façon légèrement différente. En effet, une session R ne peut pas se connecter au site du CRAN depuis AUS car les serveurs AUS n’ont pas accès à internet pour des raisons de sécurité. Pour contourner cette difficulté, l’Insee dispose de son propre dépôt de packages R, accessible uniquement depuis le réseau interne. Ce dépôt est un miroir du CRAN, et est mis à jour quotidiennement. La configuration de R dans AUS est définie de sorte que R utilise par défaut le dépôt interne. Par conséquent, vous n’avez donc aucun réglage à faire par vous-même pour utiliser ce dépôt.\n\n\n6.2.2.2 Installation des dépendances\nPar défaut, la fonction install.packages installe également toutes les dépendances nécessaires. Il est possible de modifier ce réglage grâce à l’argument dependencies, mais vous n’avez normalement pas besoin de le faire dans le cadre d’un usage standard de R.\n\n6.2.2.3 Emplacement des packages installés\nUne fois qu’un package a été téléchargé, la fonction install.packages l’installe, c’est-à-dire qu’elle le copie dans un dossier local appelé librairie (ou parfois bibliothèque). Par défaut, les librairies dans lesquelles les packages sont installés sont situées dans le sous-dossier library de l’emplacement où est installé R.\n\nSur AUS, les packages sont installés dans le dossier U:/R/win-library/x.y, où x.y désigne le numéro de version de R (3.6 ou 4.0 par exemple).\n\nIl est possible de modifier la librairie dans laquelle un package est installé grâce à l’argument lib, mais vous n’avez normalement pas besoin de le faire dans le cadre d’un usage standard de R.\n\n6.2.2.4 Type de packages\n\nSous Windows, les packages sont installés par défaut à partir de fichiers spécialement compilés pour ce système d’exploitation. Ces fichiers sont désignés sous le nom de fichiers binaires (binary en anglais).\nL’argument type de install.packages permet de choisir une méthode alternative à la valeur par défaut, \"win.binary\". Utiliser type = \"source\" permet par exemple de récupérer une version plus récente d’un package (par exemple sur une forge logicielle), mais peut nécessiter l’installation d’outils de développement supplémentaires sous Windows. Ce type d’installation est plutôt destiné à des utilisateurs avancés ; il est donc conseillé de conserver la valeur par défaut de l’argument type.\n\n6.2.3 Installation avancée\nIl existe d’autres méthodes pour installer des packages R, qui sont décrites ci-dessous. De façon générale, ces méthodes sont destinées aux utilisateurs avancés de R.\n\n6.2.3.1 Installer depuis une forge logicielle (GitHub, GitLab…)\nCertains packages sont mis à disposition sur une forge logicielle, par exemple GitHub ou GitLab, et pas sur le CRAN. C’est notamment le cas des packages dont le développement est encore expérimental, ou dont l’usage est très spécifique.\nLe package remotes a pour finalité principale d’installer des packages depuis des forges logicielles. Pour installer un package depuis une forge, il faut utiliser la fonction dédiée : install_github pour GitHub, install_gitlab pour GitLab, install_svn pour SVN, etc… Voici un exemple :\n\n# installe depuis `https://github.com/InseeFrLab/doremifasolData`\nremotes::install_github(repo = \"InseeFrLab/doremifasolData\")\n\nSeul le premier argument (repo) est obligatoire. Il est formé du nom du compte sur la forge logicielle, suivi du nom du dépôt. Le package installé contiendra les modifications les plus récentes. Installer depuis une forge permet ainsi de récupérer la version de développement d’un package (seules les versions stables sont en général disponibles sur le CRAN). Pour installer un package tel qu’il était à un moment donné, il est également possible de faire suivre le nom du dépôt par une référence à un commit, à un tag (une version) ou à une branche. Voici un exemple avec un tag : remotes::install_gitlab(\"py_b/funprog@v-0.3.0\"). Voici un exemple avec un commit : remotes::install_github(\"InseeFrLab/doremifasolData@a9df2d3d0e372\").\n\n\n\n\n\n\nNote\n\n\n\nLorsque vous installez un package depuis une forge logicielle, R crée automatiquement une archive temporaire (un fichier .tar.gz), puis installe le package à partir de celle-ci. Si vous travaillez dans un environnement Windows, il est nécessaire que Rtools soit installé sur votre poste pour que R puisse construire l’archive.\nRtools est un logiciel (pas un package) qui contient un certain nombre d’outils pour construire des packages R sous Windows. Rtools n’est en revanche pas nécessaire sur les systèmes Mac ou Linux. Vous pouvez vérifier si Rtools est installé en exécutant la fonction pkgbuild::has_rtools(). Celle-ci renvoie TRUE si Rtools est installé, FALSE sinon. Si Rtools n’est pas installé, vous pouvez l’installer facilement en le téléchargeant sur internet.\n\n\n\n6.2.4 Installer depuis un fichier\nIl est aussi possible d’installer un package depuis une archive compressée (fichiers “.zip” pour les binaires Windows ou “.tar.gz” pour les packages source). Pour indiquer que la source est un fichier local, il est obligatoire de spécifier repos = NULL.\n\ninstall.packages(\"chemin/en/local/package1_x.y.z.zip\", repos = NULL)\n\nSi le fichier est de type “source” (extension .tar.gz), Rtools peut s’avérer nécessaire, notamment si certaines fonctions sont écrites en C ou en C++.\nInstaller un package depuis un fichier peut être utilisé dans les cas suivants :\n\nle package n’est pas sur un dépôt distant (CRAN ou une forge logicielle) ;\non souhaite installer une version plus ancienne que celle actuellement sur le dépôt distant ;\naucune connexion internet n’est possible (par exemple, transmission du fichier compressé via une clé usb).\n\nIl est à noter que cette méthode atteint rapidement ses limites dès lors que des dépendances non installées sont requises. En effet, il faudra installer manuellement ces dépendances (et éventuellement les dépendances des dépendances…).\n\n6.2.5 Mettre à jour les packages\n\nIl peut arriver qu’un de vos programmes génère une erreur parce que vous utilisez un package dans une version trop ancienne. Pour mettre à jour un package, il suffit de le réinstaller avec install.packages. Cette méthode de mise à jour est la plus simple, et résout la très grande majorité des problèmes de version de packages.\nIl existe également la fonction update.packages. Cette fonction ne sert pas à mettre un package particulier, mais à mettre à jour tous les packages d’une librairie. Vous pouvez utiliser cette fonction si vous le souhaitez, mais son usage est plus complexe que celui d’install.packages. Vous pouvez en apprendre davantage sur cette fonction en consultant l’aide associée (?update.packages).\n\n6.2.6 Gestion des packages avec RStudio\nRStudio dispose d’un onglet “Packages” (par défaut dans le cadran en bas à droite) où l’on peut gérer les packages sans saisir des instructions dans la console. Les fonctionnalités principales sont les suivantes :\n\naffichage des packages installés sur le système (correspond au résultat de la fonction installed_packages). Cocher un package dans cette liste permet de le charger en mémoire (library).\nle bouton  Install  permet de chercher un package sur le CRAN (ou un autre dépôt) ou de sélectionner un package sous forme de fichier. L’auto-complétion sur le nom du package est proposée lors de la recherche, ce qui la rend plus aisée et prévient les problèmes de casse (majuscules / minuscules).\nle bouton  Update  permet de lister les packages pour lesquels il existe une version plus récente et d’éventuellement les mettre à jour.\n\nChacune de ces manipulations effectuées par l’intermédiaire de l’interface graphique génère (et exécute) la commande correspondante dans la console.\nLors de l’ouverture d’un script, RStudio tente aussi de détecter automatiquement si les packages utilisés dans celui-ci sont installés. Dans le cas contraire, il affiche un bandeau en haut du script proposant d’installer les packages manquants : ▲ _package_ --- required but is not installed. Install Don't show again\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nIl ne faut pas essayer d’installer des packages en cliquant sur le bouton  Install  du bandeau si vous travaillez dans AUS. En effet, si vous cliquez sur ce bouton, RStudio essaie de télécharger le package depuis le CRAN (qui n’est pas accessible depuis AUS). Il faut utiliser la fonction install.packages à la place.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Utiliser des _packages_ `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_installer_packages.html#utiliser-un-package-r",
    "href": "01_R_Insee/Fiche_installer_packages.html#utiliser-un-package-r",
    "title": "6  Utiliser des packages R",
    "section": "\n6.3 Utiliser un package R\n",
    "text": "6.3 Utiliser un package R\n\nUne fois qu’un package est installé sur votre machine, il est pratique de procéder au chargement du package avec la fonction library() pour pouvoir l’utiliser. Charger un package consiste à indiquer à R qu’on souhaite l’utiliser dans la session courante. Ceci rend possible l’utilisation des fonctions implémentées par le package. Par exemple, pour charger le package ggplot2, il est nécessaire d’exécuter la commande suivante :\n\nlibrary(ggplot2)\n\nPar défaut, la fonction library cherche le package dans la librairie d’installation des packages (voir le paragraphe @ref(libraries)). R renvoie une erreur s’il ne trouve pas le package considéré dans la librairie. Il est possible de modifier la librairie dans laquelle la fonction library cherche les packages grâce à l’argument lib.loc, mais vous n’avez normalement pas besoin de le faire dans le cadre d’un usage standard de R.\n\n\n\n\n\n\nTip\n\n\n\nIl est indispensable de rassembler les instructions de chargement de package au début des programmes R, car cela permet à un utilisateur qui ne connaît pas les programmes de repérer facilement les packages utilisés.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMême si un package n’a pas été chargé à l’aide de la fonction library(), il est possible d’appeler ses fonctions avec l’opérateur ::. Par exemple, même sans avoir chargé le package ggplot2, il est possible d’initialiser un plot avec la commande suivante :\n\nggplot2::ggplot(data = df)\n\nL’opérateur :: est particulièrement utile en cas de conflits entre packages, comme détaillé dans la fiche sur les bonnes pratiques pour améliorer la reproductibilité.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Utiliser des _packages_ `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_installer_packages.html#pour-en-savoir-plus",
    "href": "01_R_Insee/Fiche_installer_packages.html#pour-en-savoir-plus",
    "title": "6  Utiliser des packages R",
    "section": "\n6.4 Pour en savoir plus",
    "text": "6.4 Pour en savoir plus\n\nLe livre R Packages par Hadley Wickham et Jenny Brian (en anglais). Il porte principalement sur le développement de packages, mais l’introduction du livre détaille les concepts abordés dans cette fiche.\nLa fiche sur la gestion des dépendances.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Utiliser des _packages_ `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html",
    "href": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html",
    "title": "7  Comment choisir un package ?",
    "section": "",
    "text": "7.1 Tâches concernées et recommandations\nVous souhaitez réaliser une tâche qui n’est pas décrite dans la documentation utilitR. Vous souhaitez donc déterminer si un package déjà existant répond à votre besoin et si vous pouvez l’utiliser.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comment choisir un _package_ ?</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#tâches-concernées-et-recommandations",
    "title": "7  Comment choisir un package ?",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\nChoisir les packages qu’on utilise dans un programme est une tâche très importante et délicate. Il est normal d’y consacrer du temps. Pour rechercher un package adapté à une tâche, privilégier :\n\nles CRAN Tasks Views notamment la page consacrée à la statistique publique ;\nla liste des logiciels libres utiles pour la statistique publique.\n\nVous pouvez utiliser un package dans un programme si les deux critères suivants sont remplis :\n\nle package apporte réellement quelque chose à votre programme (rapidité d’exécution, simplicité et concision du code, simplification de tâches complexes…) ;\nle package présente un niveau de risque suffisamment faible.\n\nLe niveau de risque doit être d’autant plus faible que le package sera utilisé souvent et pendant une longue période. Il faut donc être plus exigeant pour les packages utilisés durablement en production que pour les packages utilisés pour une étude ponctuelle.\nVous pouvez évaluer le risque que présente l’utilisation d’un package grâce à l’analyse de différentes dimensions :\n\nl’opportunité ;\nla qualité de la documentation ;\nla facilité d’utilisation ;\nla popularité et la communauté d’utilisateurs ;\nla réactivité de la maintenance ;\nla qualité des développements ;\nla réputation des auteurs.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comment choisir un _package_ ?</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#utiliser-un-package-nest-pas-un-choix-anodin",
    "href": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#utiliser-un-package-nest-pas-un-choix-anodin",
    "title": "7  Comment choisir un package ?",
    "section": "\n7.2 Utiliser un package n’est pas un choix anodin",
    "text": "7.2 Utiliser un package n’est pas un choix anodin\nUtiliser des packages vous permet de gagner en productivité et de réaliser simplement des opérations complexes. Une des forces de l’écosystème R est le grand nombre de packages développés par des utilisateurs, qui enrichissent le langage de base et proposent des fonctions supplémentaires qui couvrent un éventail extrêmement large de tâches. En pratique, il est aujourd’hui inenvisageable (et inefficace) de mener un projet avec R sans faire appel à des packages. La preuve en est qu’une large part de la présente documentation porte sur les packages qui répondent aux besoins courants des statisticiens.\nToutefois, l’utilisation d’un package présente aussi des inconvénients. En effet, utiliser un package ajoute une dépendance au programme qu’on est en train d’écrire, ce qui est une source de fragilité. Les packages suivent un cycle de vie et peuvent se situer dans une phase où ils sont encore susceptibles d’évoluer : par exemple, une nouvelle version d’un package peut déprécier une fonction, ou en modifier le fonctionnement interne, et briser ainsi votre programme ; un package peut également être retiré du CRAN si son auteur ne souhaite plus le maintenir. Un code peut donc être fonctionnel au moment où on l’écrit, et ne plus fonctionner quelques mois plus tard si un des packages qu’il utilise a évolué 1.\nÉtant donné qu’un package peut lui-même dépendre d’autres packages, l’utilisation d’un seul package peut engendrer un nombre important de dépendances en cascade, et donc autant de sources de fragilité pour vos programmes. Pour trouver l’ensemble des dépendances d’un package, on peut utiliser la fonction tools::package_dependencies(). Voici comment trouver l’ensemble des dépendances du package rmarkdown :\n\ntools::package_dependencies(\"rmarkdown\", recursive = TRUE)\n\n$rmarkdown\n [1] \"bslib\"       \"evaluate\"    \"fontawesome\" \"htmltools\"   \"jquerylib\"  \n [6] \"jsonlite\"    \"knitr\"       \"methods\"     \"tinytex\"     \"tools\"      \n[11] \"utils\"       \"xfun\"        \"yaml\"        \"base64enc\"   \"cachem\"     \n[16] \"fastmap\"     \"grDevices\"   \"lifecycle\"   \"memoise\"     \"mime\"       \n[21] \"rlang\"       \"sass\"        \"digest\"      \"highr\"       \"stats\"      \n[26] \"cli\"         \"glue\"        \"fs\"          \"R6\"          \"rappdirs\"   \n\n\nLe point essentiel est donc le suivant : le simple fait qu’un package fasse (ou semble faire) ce que vous voulez n’est pas une raison suffisante de l’utiliser, surtout si votre programme doit rester fonctionnel pendant une longue période. Déterminer si on peut utiliser un package revient à faire un arbitrage entre avantages et inconvénients, et à évaluer le risque d’instabilité d’un package. De multiples facteurs permettent d’apprécier qualitativement ce risque.\n\n\n\n\n\n\nTip\n\n\n\nChaque nouvelle dépendance doit être ajoutée avec précaution. Il faut donc analyser chacune des dimensions présentées et déterminer si certains facteurs rendent le package inutilisable.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comment choisir un _package_ ?</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#comment-trouver-un-package-adapté-à-une-tâche",
    "href": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#comment-trouver-un-package-adapté-à-une-tâche",
    "title": "7  Comment choisir un package ?",
    "section": "\n7.3 Comment trouver un package adapté à une tâche",
    "text": "7.3 Comment trouver un package adapté à une tâche\nPour rechercher un package adapté à une tâche, privilégier :\n\nles CRAN Tasks Views notamment la page consacrée à la statistique publique ;\nla liste des logiciels libres utiles pour la statistique publique.\nla page collaborative frrrenchies contient une liste de packages utiles pour manipuler les données françaises.\n\nVous pouvez également rechercher des articles de blog ou poser la question sur des sites d’entraide (RStudio Community, par exemple).\nPour chaque package candidat, il est recommandé d’analyser le contenu du fichier DESCRIPTION qui comprend les méta-données du package. Ce fichier est disponible sur le site du CRAN à l’adresse https://cran.r-project.org/package=nomdupackage.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comment choisir un _package_ ?</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#comment-savoir-si-vous-pouvez-utiliser-un-package",
    "href": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#comment-savoir-si-vous-pouvez-utiliser-un-package",
    "title": "7  Comment choisir un package ?",
    "section": "\n7.4 Comment savoir si vous pouvez utiliser un package\n",
    "text": "7.4 Comment savoir si vous pouvez utiliser un package\n\nDéterminer si on peut utiliser un package revient à comparer les avantages et les risques que présente son utilisation. Les paragraphes qui suivent vous donnent des conseils simples pour mener cette analyse de risque.\n\n7.4.1 L’opportunité\nUtiliser un package comporte une part de risque. Cependant, ne pas utiliser de packages peut parfois être plus risqué. En effet, il serait inefficient voire dangereux de “réinventer la roue” s’il existe un package stable et éprouvé qui permet de réaliser une tâche complexe. Coder soi-même une tâche complexe peut conduire à un code difficilement maintenable, moins rapide, présentant des bugs et éventuellement des failles de sécurité. Le premier conseil est donc : vous pouvez utiliser un package à chaque fois que vous voulez réaliser une opération complexe à coder par vous-même.\nOn peut illustrer ces extrêmes au travers de deux exemples de tâches :\n\n\non veut tester si un vecteur numérique comprend des nombres impairs ; il serait inopportun d’utiliser la fonction is.odd() du package FSA alors qu’elle peut simplement s’écrire :\nis_odd &lt;- function(x) {\n  x %% 2 != 0\n}\n\non veut manipuler des données au format JSON ou XML ; il pourrait être tentant de se passer de package et d’employer des expressions régulières. Cependant, cette technique est reconnue comme dangereuse et dans ce cas, il est beaucoup plus prudent d’utiliser les packages jsonlite ou xml2, par exemple.\n\n\n\n\n\n\n\nTip\n\n\n\nIl est envisageable d’utiliser un package s’il apporte réellement quelque chose à votre programme (rapidité d’exécution, simplicité et concision du code, simplification de tâches complexes…)\n\n\n\n7.4.2 La qualité de la documentation\nLa présence d’une documentation riche, détaillée et présentant des exemples d’utilisation est un signe que le package a été développé avec soin. Les packages R offrent la possibilité de créer des pages de documentation appelées vignettes à cette fin. On peut lister les vignettes d’un package grâce à la fonction vignette().\nIl est également devenu très simple pour les auteurs de packages de créer un site web compagnon de chacun de leur package. Lorsqu’il existe, ce site web est référencé dans la rubrique URL du fichier DESCRIPTION.\n\n\n\n\n\n\nTip\n\n\n\nNe pas utiliser de package ne comprenant ni vignette ni site web associé.\n\n\n\n7.4.3 La facilité d’utilisation\nSuivant le niveau de soin qui a été apporté à un package, l’utilisation peut être plus ou moins simple. Des packages mal conçus vont être complexes à utiliser :\n\nnom des fonctions et des paramètres peu compréhensibles ;\nbesoin d’opérer de nombreuses étapes ou transformations de données afin d’utiliser les fonctions ;\nmessages d’erreurs peu explicites…\n\n\n\n\n\n\n\nTip\n\n\n\nNe pas utiliser de package qui apparaissent comme trop complexes.\n\n\n\n7.4.4 Popularité et communauté d’utilisateurs\nUtiliser un package confidentiel fait peser plusieurs risques potentiels :\n\ndifficulté à obtenir de l’aide ;\nprésence de bugs non détectés ;\nadéquation du package à la tâche.\n\nIl est difficile d’évaluer précisément la popularité d’un package. Il faut donc collecter des indices :\n\nprésence d’articles de blog par des rédacteurs autres que les auteurs du package (voir le site r-bloggers.com, par exemple) ;\nprésence de réponses à des questions sur ce package sur des sites d’entraide (stackoverflow.com, RStudio Community…) ;\nnombre d’étoiles (stars) sur GitHub ou GitLab, les packages ayant moins de 30 étoiles pouvant être considérés comme confidentiels ;\nnombre d’issues (ouvertes ou fermées) et de pull requests sur GitHub ou GitLab ;\n\nle nombre de téléchargements du package ; étant donné que le CRAN dispose de nombreux miroirs, il est impossible de connaître le nombre total de téléchargements d’un package. Ceci dit, le package cranlogs permet d’obtenir le nombre de téléchargements depuis le miroir RStudio du CRAN (qui est très utilisé). Pour connaître le nombre de téléchargements du package ggplot2 le mois dernier :\nwith(\n  cranlogs::cran_downloads(packages = \"ggplot2\", when = \"last-month\"), \n  sum(count)\n)\nL’interprétation absolue de ces chiffres doit être effectuée avec prudence. En effet, il existe des robots qui effectuent des copies intégrales de l’ensemble des packages. Pour autant, on peut considérer qu’un nombre de téléchargements mensuels inférieur à 1 000 signale un package confidentiel.\n\n\n\n\n\n\n\n\nTip\n\n\n\nNe pas utiliser de package qui paraisse trop confidentiel.\n\n\n\n7.4.5 Réactivité de la maintenance\nLe risque principal est d’utiliser un package qui ne serait plus maintenu ou bien qui présenterait ce risque de façon accrue. Une façon relativement simple d’y parvenir est d’estimer le facteur d’autobus, défini comme le nombre minimal de contributeurs qu’il faut retirer à un projet pour le faire échouer ou s’arrêter. Plus précisément, il faut se méfier d’un package qui a un facteur d’autobus égal à 1, ce qui signifie que sa maintenance n’est assurée que par une seule personne.\nSi le code source du package est hébergé sur GitHub (ce qui est le cas pour la majorité des packages), on peut facilement trouver le nombre de personnes ayant contribué à un package en allant dans l’onglet Insights et le volet Contributors. Par exemple, pour le package ggplot2, la liste des contributeurs se situe à cette adresse : https://github.com/tidyverse/ggplot2/graphs/contributors.\nIl est important d’analyser ces contributions sur une période relativement récente (les deux dernières années, par exemple).\nL’analyse de l’ensemble de l’activité sur GitHub ou GitLab est également un excellent moyen d’évaluer la réactivité de la maintenance : des issues et/ou des pull requests ouvertes et restées sans réponse depuis plusieurs mois sont le signe d’un projet peu réactif. Dans le doute, il est possible d’ouvrir une issue dans le projet afin de demander s’il est toujours activement maintenu (cette pratique est courante).\n\n\n\n\n\n\nTip\n\n\n\nNe pas utiliser de package qui présente de nombreuses issues restées sans réponse.\nSe méfier très fortement des packages n’ayant qu’un seul contributeur.\n\n\n\n7.4.6 Qualité des développements\nLes auteurs de packages peuvent très facilement mettre en place des outils de qualimétrie permettant de repérer l’état dans lequel se situe le projet. En pratique, ces informations sont présentées au travers de badges colorés présents sur la page d’accueil du projet (sur GitHub ou GitLab).\n\nOn peut par exemple trouver :\n\n\nle badge du cycle de vie ; au travers de ce badge, les auteurs signalent explicitement la phase dans laquelle se situe le projet.2. Il est recommandé de n’utiliser que des packages déclarés comme actifs ou stables. \nEn cas d’absence de ce badge, on peut se référer au numéro de version du package ; en effet, il est d’usage que les versions considérées comme stables par leurs auteurs bénéficient d’un numéro de version majeur (le premier chiffre) strictement positif, tel que 1.0.0, 1.2.1, etc. Un package ayant un numéro de version majeur égal à 0 signale que le package n’est pas encore considéré comme stable par ses développeurs.\n\nle taux de couverture de code par les tests ; ce badge s’appelle codecov ou coverage et la valeur qui est comprise indique le pourcentage de lignes de code faisant l’objet d’au moins un test. Il est recommandé de ne pas utiliser un package dont le taux de couverture par les tests est inférieur à 80 %. L’absence de ce badge est rédhibitoire car elle indique que les développeurs ne respectent pas les bonnes pratiques de développement.\nl’utilisation de l’intégration continue ; ce badge s’appelle build, R-CMD-check ou encore pipeline. Il indique que les tests sont automatiquement effectués à chaque modification du code source du package. Il s’agit d’une bonne pratique que tout auteur de package se doit désormais d’employer. L’absence d’utilisation de l’intégration continue doit conduire à écarter le package.\n\nEn dehors des badges, on peut également utiliser le package goodpractice afin d’analyser la qualité du code d’un package.\n\n\n\n\n\n\nTip\n\n\n\n\nNe pas utiliser de package dont le code source n’est pas hébergé sur une forge telle que GitHub ou GitLab.\nNe pas utiliser de package qui n’utilise pas l’intégration continue.\nNe pas utiliser de package qui n’affiche pas le taux de couverture de code par les tests.\n\n\n\nUne analyse plus approfondie du code source peut permettre de repérer l’utilisation d’autres bonnes pratiques telles que le linting.\n\n7.4.7 Réputation des auteurs\nLes auteurs de package ont des profils extrêmement divers : étudiants, statisticiens exerçant dans les secteurs publics ou privés, chercheurs, enseignants… des événements personnels ou professionnels peuvent les amener à maintenir moins activement un package voire à cesser toute activité dans le domaine. De façon générale, les packages développés par une organisation privée ou publique présentent un risque moindre d’être abandonnés.\nDe plus, développer des packages de qualité nécessite un peu d’expérience. Il est donc utile d’établir le profil des auteurs.\nOn peut facilement trouver les différents packages auxquels une personne a contribué en utilisant l’adresse suivante : https://www.rdocumentation.org/collaborators/name/Prenom%20Nom. Il est également possible de trouver la liste des packages dont la maintenance est assurée par une personne donnée, voir https://stackoverflow.com/a/10082179/.\nLes packages de rOpenSci ou de RStudio sont toujours développés en appliquant des standards élevés, leur qualité est donc assurée, le seul risque résiduel étant celui lié à la maturité du projet qu’il faut vérifier.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comment choisir un _package_ ?</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#footnotes",
    "href": "03_Fiches_thematiques/Fiche_comment_choisir_un_package.html#footnotes",
    "title": "7  Comment choisir un package ?",
    "section": "",
    "text": "l’utilisation d’un gestionnaire de dépendances tel que [renv] (https://cran.r-project.org/package=renv) permet toutefois de figer la version des packages utilisés par un programme↩︎\npour une description des différentes phases, voir https://www.repostatus.org/ ou https://www.tidyverse.org/lifecycle/↩︎",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Comment choisir un _package_ ?</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_gerer_dependances.html",
    "href": "03_Fiches_thematiques/Fiche_gerer_dependances.html",
    "title": "8  Gérer les dépendances",
    "section": "",
    "text": "8.1 Tâches concernées et recommandations\nUn programme R utilise le plus souvent des packages qui sont autant de dépendances. Cette fiche présente les méthodes pour déclarer et gérer les dépendances avec R, tâche à effectuer avec soin lorsqu’on partage ses programmes. Il est recommandé d’avoir lu les fiches Utiliser des packages R et Utiliser les projets RStudio au préalable.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gérer les dépendances</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_gerer_dependances.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_gerer_dependances.html#tâches-concernées-et-recommandations",
    "title": "8  Gérer les dépendances",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\nIl est recommandé :\n\nde ne pas écrire install.packages() au début de ses programmes ;\nde toujours utiliser l’organisation par projet. La fiche Utiliser les projets RStudio présente avec plus de détails ces éléments\n\nIl existe deux méthodes pour gérer les dépendances :\n\nutiliser le package renv, qui permet à un autre utilisateur d’installer les dépendances à l’aide de la fonction renv::restore() ;\nles déclarer dans un fichier DESCRIPTION situé à la racine de son projet, ce qui permet à un autre utilisateur d’installer les dépendances à l’aide la fonction remotes::install_deps() .\n\nIl est recommandé d’adopter la méthode reposant sur renv.\nToutefois, ce package nécessite un accès internet pour fonctionner. Si vous travaillez dans AUS, vous êtes contraints d’utiliser la méthode utilisant le fichier DESCRIPTION.\nPour bien gérer les dépendances, il faut adopter une approche déclarative et non pas impérative. Cela revient à décrire de façon formelle les pré-requis à la bonne exécution des programmes. Il est préférable de procéder ainsi car de cette manière on sépare la construction de l’environnement d’exécution du code de son exécution proprement dite, en en laissant la maîtrise à l’utilisateur. Inversement, utiliser la commande install.packages() directement dans les codes forcerait la modification de la configuration de l’utilisateur du projet sans avoir recueilli son accord (et risquerait de lui créer des problèmes sur ses autres projets).\n\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nLe package renv nécessite que votre environnement de travail ait accès à internet, ce qui n’est pas le cas d’AUS.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gérer les dépendances</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_gerer_dependances.html#pourquoi-fonctionner-en-mode-projet",
    "href": "03_Fiches_thematiques/Fiche_gerer_dependances.html#pourquoi-fonctionner-en-mode-projet",
    "title": "8  Gérer les dépendances",
    "section": "\n8.2 Pourquoi fonctionner en mode projet ?",
    "text": "8.2 Pourquoi fonctionner en mode projet ?\nIl est très rare d’organiser son code au sein d’un même programme monolithique ; il s’agit même d’une mauvaise pratique car elle rend le code moins lisible. Afin que le code soit plus facile à gérer, celui-ci est plus souvent réparti entre plusieurs fichiers à l’extension .R au sein d’un même dossier. Ce dossier est la matérialisation du mode projet.\nPar exemple, vous pouvez avoir cette organisation au sein de votre dossier :\n├── import.R\n├── traitement.R\n├── graphique.R\nSi vous travaillez avec les projets RStudio, vous trouverez un fichier supplémentaire à l’extension .Rproj :\n├── import.R\n├── traitement.R\n├── graphique.R\n├── monprojet.Rproj\nLe fichier .Rproj comprend un ensemble de méta-données vous permettant de contrôler le comportement de RStudio afin que l’ensemble des personnes travaillant sur ce projet aient un comportement similaire de RStudio. Vous pouvez en apprendre davantage sur les projets RStudio en lisant la fiche Utiliser les projets RStudio.\n\n\n\n\n\n\nNote\n\n\n\nAu sein de votre projet, il est recommandé de ranger l’ensemble des programmes R au sein d’un dossier nommé R :\n├── R/\n├──── import.R\n├──── traitement.R\n├──── graphique.R\n├── monprojet.Rproj\n\n\nVous connaissez très certainement la fonction install.packages() qui vous permet d’installer un package. Afin de documenter le fait que vous ayez installé certains packages nécessaires à l’exécution de vos programmes, il vous est certainement apparu naturel de conserver ces commandes dans votre code.\nCependant, vous avez également certainement remarqué que ces commandes ne sont à exécuter qu’une seule fois et donc qu’il convient de les mettre en commentaire afin de ne pas perturber l’exécution de vos programmes.\nToutefois, il existe une autre approche beaucoup plus robuste, ici recommandée, qui considère que les dépendances d’un projet sont des méta-données nécessaires à sa bonne exécution et qu’elles doivent être déclarées de façon explicite.\n\n\n\n\n\n\nTip\n\n\n\nNe conservez pas les commandes install.packages() dans vos programmes. Utilisez une des deux méthodes déclaratives présentées dans cette fiche.\n\n\nChacune de ces méthodes va déclarer les dépendances de votre projet dans un fichier spécifique.\nAvec la première méthode, celle utilisant le fichier DESCRIPTION, vous obtiendrez la structure de dossier suivante :\n├── DESCRIPTION\n├── R/\n├──── import.R\n├──── traitement.R\n├──── graphique.R\n├── monprojet.Rproj\nLe fichier DESCRIPTION sera le fichier dans lequel vous renseignerez les dépendances de votre projet.\nAvec la seconde méthode, celle utilisant le package renv, vous obtiendrez automatiquement la structure de dossier suivante :\n├── .Rprofile\n├── R/\n├──── import.R\n├──── traitement.R\n├──── graphique.R\n├── monprojet.Rproj\n├── renv.lock\n├── renv/\nLes dépendances seront décrites automatiquement dans le fichier renv.lock, le fichier .Rprofile et le dossier renv étant là pour assurer le bon fonctionnement du package renv.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gérer les dépendances</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_gerer_dependances.html#approche-description",
    "href": "03_Fiches_thematiques/Fiche_gerer_dependances.html#approche-description",
    "title": "8  Gérer les dépendances",
    "section": "\n8.3 Utilisation du fichier DESCRIPTION\n",
    "text": "8.3 Utilisation du fichier DESCRIPTION\n\nLe fichier DESCRIPTION est un fichier conçu à l’origine pour renseigner les méta-données d’un package. Cependant, il est courant de l’utiliser même dans le cas où le projet n’est pas un package. Il s’agit donc d’un usage un peu détourné de ce fichier mais communément admis au sein de la communauté R.\n\n\n\n\n\n\nNote\n\n\n\nLa méthode utilisant le fichier DESCRIPTION n’est pas la plus fiable pour gérer les dépendances. Si votre session R a accès à internet, il est préférable d’adopter la méthode qui repose sur le package renv.\n\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nLa méthode utilisant le fichier DESCRIPTION est la seule méthode qui fonctionne dans l’espace informatique AUS.\n\n\n\n8.3.1 La déclaration des dépendances dans le fichier DESCRIPTION\n\nUtiliser un fichier DESCRIPTION permet de déclarer les dépendances de son projet mais aussi d’autres méta-données telles que les auteurs des programmes, la date de réalisation, une description textuelle du projet, etc.\nVoici un exemple de fichier DESCRIPTION que nous analyserons par la suite :\nPackage: stage\nType: Book\nTitle: Stage de Fin d'Études\nVersion: 0.0.0.9000\nDate: 2021-06-08\nAuthors@R: c(person(\"Ella\", \"Lapêche\", role = c(\"aut\", \"cre\"),\n                     email = \"ella.lapeche@insee.fr\"),\n              person(\"Jacques\", \"Ouzi\", role = \"aut\",\n                     email = \"jacques.ouzi@insee.fr\"))\nDescription: Ce projet comprend les programmes\n  réalisés durant notre stage de fin d'études.\nEncoding: UTF-8\nLanguage: fr\nImports:\n    bookdown,\n    data.table,\n    doremifasol,\n    dplyr (&gt;=1.0.0)\nRemotes: \n    rstudio/bookdown@v0.20,\n    InseeFrLab/DoReMiFaSol\nLes rubriques servant à déclarer des dépendances à notre projet sont nommées Imports et Remotes et c’est à elles que nous allons nous intéresser par la suite.\n\n\n\n\n\n\nNote\n\n\n\nNous ne commenterons pas ici les nombreux champs possibles pour un fichier DESCRIPTION. Les lecteurs intéressés peuvent se référer à la section idoine dans la documentation officielle de R, Writing R Extensions.\n\n\n\n8.3.2 Le champ Imports\n\nToutes les dépendances d’un projet, c’est-à-dire l’ensemble des packages nécessaires au bon déroulement des programmes, doivent être déclarées dans la rubrique Imports.\nSi les programmes utilisent des packages qui ne sont pas disponibles sur le CRAN (vous utilisez, par exemple, un package mis à disposition par des collègues), vous devez aussi les déclarer dans la rubrique Imports.\n\n\n\n\n\n\nTip\n\n\n\nAu lieu d’écrire install.packages(\"nomdunpackage\") dans vos programmes, vous déclarerez ainsi nomdunpackage dans la rubrique Imports.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPrenez garde que les noms de package soient bien séparés par des virgules (,).\n\n\nLorsque les programmes ont besoin d’une version récente d’un package, vous pouvez l’indiquer de la façon suivante :\nImports:\n    dplyr (&gt;=1.0.0)\nCela signifie que les programmes nécessitent au moins la version 1.0.0 de dplyr pour fonctionner. Cela peut être utile lorsque vous souhaitez indiquer que vous utilisez des fonctionnalités récentes offertes par ce package.\n\n\n\n\n\n\nNote\n\n\n\nIl n’est pas possible de mentionner une version exacte d’un package dans la rubrique Imports, on ne peut indiquer qu’une version minimale. Autrement dit, vous pouvez écrire dplyr (&gt;=1.0.0) mais vous ne pouvez pas écrire dplyr (==1.0.0).\nCe mode de fonctionnement, qui est un choix de conception de R, est particulièrement ennuyeux et c’est précisément l’utilisation du champ Remotes qui peut nous permettre de fixer précisément une version donnée d’un package.\n\n\nDans l’exemple de fichier DESCRIPTION précédent, nous pouvons constater que :\n\nle projet a quatre dépendances : bookdown, data.table, {doremifasol} et dplyr car ces packages sont mentionnés dans le champ Imports.\nle projet requiert une version de dplyr supérieure ou égale à 1.0.0.\n\n8.3.3 Le champ Remotes\n\nDeux types de dépendances peuvent être déclarées à l’aide du fichier DESCRIPTION :\n\ndes packages dans leur dernière version disponible sur le CRAN\ndes packages dans une version qui n’est pas disponible sur le CRAN\n\n\n\n\n\n\n\nNote\n\n\n\nLe CRAN ne met à disposition que la dernière version de chaque package. Il s’agit d’un choix important de conception effectué par le CRAN qui peut créer des difficultés.\n\n\nSavoir différencier les deux situations précédentes demande d’effectuer une petite gymnastique :\n\nlorsqu’un package est uniquement déclaré dans le champ Imports sans être déclaré dans le champ Remotes, cela signifie que le projet utilise la version de ce package disponible sur le CRAN. Il s’agit du fonctionnement par défaut du champ Imports.\nlorsqu’un package est déclaré dans le champ Imports ainsi que dans le champ Remotes, cela signifie que le projet utilise une version qui n’est pas disponible sur le CRAN mais “ailleurs” (pour comprendre ce que signifie cet “ailleurs”, lisez-la suite).\n\nLe fonctionnement du champ Remotes est le suivant :\n\nlorsque la valeur du champ Remotes est de la forme org/project, cela signifie que la dernière version disponible sur GitHub est utilisée, soit le code source disponible à l’adresse https://github.com/org/project. Attention, les codes disponibles sur GitHub peuvent évoluer quotidiennement. Il est donc plus prudent d’utiliser la méthode suivante.\nlorsque la valeur du champ Remotes est de la forme org/project@version, cela signifie que la version utilisée sera celle référencée sur GitHub, c’est-à-dire disponible à l’adresse https://github.com/org/project/tree/version.\nlorsque la valeur du champ Remotes est de la forme git::git@git.lab.sspcloud.fr:nom/projet.git, cela signifie que la version utilisée sera celle disponible à l’adresse https://git.lab.sspcloud.fr/nom/projet.\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nL’environnement de travail AUS n’ayant pas accès à internet, les utilisateurs de R à l’Insee ne pourront pas utiliser une version disponible sur internet comme GitHub, par exemple.\nEn revanche, il est possible d’utiliser une version disponible sur le GitLab interne de l’Insee grâce à la dernière méthode.\n\n\nDans l’exemple de fichier DESCRIPTION précédent, nous pouvons constater que :\n\nle projet a quatre dépendances : bookdown, data.table, {doremifasol} et dplyr car ces packages sont mentionnés dans le champ Imports.\nles versions disponibles sur le CRAN sont utilisées pour les packages data.table et dplyr car ces deux packages ne sont pas déclarés dans le champ Remotes.\nles versions des packages bookdown et {doremifasol} nécessaires ne sont pas celles mises à disposition par le CRAN car ces deux packages apparaissent dans le champ Remotes.\nla version du package bookdown utilisée sera la version v0.20 disponible à l’adresse https://github.com/rstudio/bookdown/tree/v0.20.\nla version du package {doremifasol} utilisée sera la version de développement sur GitHub, soit la toute dernière version disponible à l’adresse https://github.com/InseeFrLab/DoReMiFaSol.\n\n8.3.4 Manipulation au quotidien du fichier DESCRIPTION\n\nIl est tout à fait possible de créer et de modifier le fichier DESCRIPTION manuellement. Cependant, ce fichier doit obéir à des règles très strictes, notamment au niveau de l’indentation. Afin de modifier ce fichier en toute sécurité, le package usethis offre des fonctions permettant de réaliser facilement les modifications usuelles.\nAprès avoir créé votre dossier qui représente votre projet, la première étape consiste à créer le fichier DESCRIPTION. Vous pouvez vous aider de la fonction usethis::use_description() afin de le créer. Une fois créé, vous pouvez le modifier en respectant son format.\n\n\n\n\n\n\nTip\n\n\n\nIl est recommandé de créer le fichier DESCRIPTION avant même d’écrire des programmes R et de référencer les dépendances au fur et à mesure, sans quoi cette tâche peut s’avérer fastidieuse.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi vous obtenez une erreur lors de l’utilisation de la fonction usethis::use_description(), il est fort probable que cela provienne du nom de votre dossier. Dans ce cas, n’hésitez pas à utiliser usethis::use_description(check_name = FALSE).\n\n\nAfin de déclarer une dépendance à votre projet, vous pouvez ajouter le nom du package dans la rubrique Imports du fichier DESCRIPTION ou bien utiliser la fonction usethis::use_package(). Cela vous évitera notamment d’oublier une virgule.\nAinsi, en exécutant\n\nusethis::use_package(\"dplyr\")\n\nCela ajoutera le package dplyr dans la rubrique Imports s’il ne l’est déjà. Vous pouvez également déclarer une version minimum :\n\nusethis::use_package(\"dplyr\", min_version = \"1.0.0\")\n\nCe qui ajoutera la ligne\ndplyr (&gt;=1.0.0)\ndans le champ Imports.\nPour déclarer une dépendance dont la version n’est pas disponible sur le CRAN, on pourra utiliser la fonction usethis::use_dev_package() qui modifiera simultanément les champs Imports et Remotes du fichier DESCRIPTION. Par exemple, l’exécution de cette commande\n\nusethis::use_dev_package(\n  \"monpackage\",\n  remote = \"git::git@git.lab.sspcloud.fr:nom/monpackage.git\"\n)\n\nmodifiera les champs Imports et Remotes de la façon suivante :\nImports: \n    monpackage\nRemotes: \n    git::git@git.lab.sspcloud.fr:nom/monpackage.git\n\n8.3.5 Installer les dépendances du projet\nEn gérant ainsi les dépendances d’un projet, une tierce personne pourra installer l’ensemble des dépendances en une seule commande :\n\nremotes::install_deps()\n\n\n\n\n\n\n\nNote\n\n\n\nPar défaut, la fonction remotes::install_deps() installe les packages utilisés par un projet dans les librairies générales de R. Par conséquent, vos autres projets peuvent être indirectement affectés lorsque vous utilisez cette fonction pour installer les dépendances d’un projet. C’est pour cette raison qu’il est préférable d’utiliser renv, qui est conçu pour éviter ce problème.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gérer les dépendances</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_gerer_dependances.html#renv",
    "href": "03_Fiches_thematiques/Fiche_gerer_dependances.html#renv",
    "title": "8  Gérer les dépendances",
    "section": "\n8.4 Utilisation du package {renv}\n",
    "text": "8.4 Utilisation du package {renv}\n\nLe package renv offre une solution plus moderne et beaucoup plus simple que la précédente pour déclarer ses dépendances. Elle ne fonctionne que dans un projet. On peut choisir d’utiliser cette méthode à n’importe quel moment de la réalisation du projet.\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nDans l’espace informatique AUS, les commandes courantes issues du package renv fonctionnent. Cependant quelques soucis ont été constatés lors d’utilisations plus spécifiques et avancées du package.\n\n\nPour commencer à utiliser renv, il suffit d’exécuter :\n\nrenv::init()\n\nCette commande crée automatiquement un ensemble de fichiers et de dossiers dont vous n’aurez pas à vous préoccuper (ne les supprimez pas !) car c’est notamment dans le dossier renv que le package renv va sauvegarder les packages dont le projet a besoin.\nVotre projet aura ainsi une structure ressemblant à la suivante :\n├── .Rprofile\n├── R/\n├──── import.R\n├──── traitement.R\n├──── graphique.R\n├── monprojet.Rproj\n├── renv.lock\n├── renv/\nLa particularité du package renv est qu’il construit un environnement isolé. Cela surprend au premier abord car les packages que vous aviez installés ne sont plus tous disponibles. Certains doivent être réinstallés grâce à la fonction install.packages(). En effet, le package renv va construire un environnement isolé de votre installation de R.\nLes versions des packages utilisés par votre projet sont toutes décrites dans le fichier renv.lock de façon automatique. Il reste possible d’installer ou de mettre à jour un package comme d’habitude.\nPour prendre une photo des numéros de version des packages installés, il faut utiliser la fonction renv::snapshot(). Cette fonction va mettre à jour les numéros de version des packages que vous utilisez.\nSi vous partagez votre projet avec d’autres personnes, ces dernières pourront installer les mêmes packages dans les mêmes versions grâce à la fonction renv::restore().",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gérer les dépendances</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_gerer_dependances.html#laquelle-de-ces-deux-méthodes-choisir",
    "href": "03_Fiches_thematiques/Fiche_gerer_dependances.html#laquelle-de-ces-deux-méthodes-choisir",
    "title": "8  Gérer les dépendances",
    "section": "\n8.5 Laquelle de ces deux méthodes choisir ?",
    "text": "8.5 Laquelle de ces deux méthodes choisir ?\nSi votre environnement de travail R est connecté à internet, il est beaucoup plus simple d’utiliser la méthode avec renv.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gérer les dépendances</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_gerer_dependances.html#RessourcesDependances",
    "href": "03_Fiches_thematiques/Fiche_gerer_dependances.html#RessourcesDependances",
    "title": "8  Gérer les dépendances",
    "section": "\n8.6 Pour en savoir plus",
    "text": "8.6 Pour en savoir plus\n\n\nIntroduction à {renv} (en anglais).",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Gérer les dépendances</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_se_documenter.html",
    "href": "03_Fiches_thematiques/Fiche_se_documenter.html",
    "title": "9  Se documenter sur R",
    "section": "",
    "text": "9.1 Tâches concernées et recommandations\nVous souhaitez vous y retrouver dans les différents niveaux de documentation que l’on trouve sur R, afin de répondre à un besoin précis ou d’enrichir vos compétences. La documentation concernant R est luxuriante, à son image. L’objet de cette fiche est de vous permettre de vous y retrouver. Trois cas sont à distinguer :",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Se documenter sur `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_se_documenter.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_se_documenter.html#tâches-concernées-et-recommandations",
    "title": "9  Se documenter sur R",
    "section": "",
    "text": "vous savez de quelle fonction vous avez besoin mais ne savez pas comment elle fonctionne ;\nvous savez ce que vous voulez faire mais ne savez pas quelle fonction ou package utiliser ;\nvous n’avez pas en tête un projet particulier mais voulez découvrir ou vous tenir à jour sur les possibilités de R.\n\n\n\n\n\n\n\nTâche concernée et recommandation\n\n\n\n\nsi vous souhaitez en savoir davantage sur l’utilisation d’une fonction dont vous connaissez l’existence, il est souvent suffisant de consulter la documentation de la fonction en exécutant ?nomFonction dans RStudio ;\nsi vous cherchez de l’information sur l’utilisation d’un package particulier, regardez si une vignette est disponible ;\nsi vous souhaitez explorer des pans de l’écosystème R qui sont complètement nouveaux pour vous, vous tenir informé des nouveautés ou évoluer dans vos pratiques, vous pouvez :\n\nconsulter la ou les fiches correspondante(s) de la documentation utilitR ;\nrejoindre les communautés d’utilisateurs qui vous correspondent.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Se documenter sur `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_se_documenter.html#vous-savez-quelle-fonction-utiliser",
    "href": "03_Fiches_thematiques/Fiche_se_documenter.html#vous-savez-quelle-fonction-utiliser",
    "title": "9  Se documenter sur R",
    "section": "9.2 Vous savez quelle fonction utiliser",
    "text": "9.2 Vous savez quelle fonction utiliser\nSi vous savez quelle fonction utiliser mais ne savez pas comment cette dernière fonctionne, il est souvent suffisant de regarder la documentation de la fonction. Il ne faut pas hésiter à se référer très fréquemment à cette documentation : c’est la solution la plus rapide pour retrouver le nom ou la signification d’un argument. Pour cela, il suffit d’exécuter ?NomFonction dans la console de RStudio (sans espace entre le ? et le nom de votre fonction). Par exemple, si vous souhaitez réaliser un export en csv avec la fonction write.csv, exécutez l’instruction :\n?write.csv\nLa documentation de la fonction apparaîtra dans le panneau en bas à droite de RStudio. Pour chaque fonction, vous allez retrouver :\n\nune description de son fonctionnement ;\nles différents arguments, ainsi que les valeurs par défaut ;\ndes exemples.\n\nIl est important de noter que la fonction ? ne fonctionnera que si le package qui contient la fonction a été chargé avec l’instruction library. Si vous n’êtes pas sûr que le package ait été chargé (par exemple parce que vous avez oublié son nom), vous pouvez utiliser le double point d’interrogation (??) afin de chercher un mot dans tous les fichiers d’aide de votre poste. Ainsi ??select vous renverra la liste de tous les fichiers d’aide contenant le mot select. Toutefois, cette méthode n’est pas la plus recommandée, car elle peut aboutir à un nombre de résultats très importants.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Se documenter sur `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_se_documenter.html#vous-savez-ce-que-vous-voulez-faire-mais-cest-tout",
    "href": "03_Fiches_thematiques/Fiche_se_documenter.html#vous-savez-ce-que-vous-voulez-faire-mais-cest-tout",
    "title": "9  Se documenter sur R",
    "section": "9.3 Vous savez ce que vous voulez faire, mais c’est tout",
    "text": "9.3 Vous savez ce que vous voulez faire, mais c’est tout\nSi vous savez ce que vous voulez faire, mais vous ne savez pas comment le faire, la première étape consiste à vérifier si la documentation utilitR contient une fiche qui porte sur votre besoin. Cette documentation est conçue pour couvrir la plupart des usages courants de R, il est donc probable que vous puissiez y trouver de premiers éléments.\nSi ce n’est pas le cas, vous pouvez vous référer à la fiche Comment choisir un package ?, afin de déterminer s’il est opportun de choisir un package, et si oui lequel. Une fois le package sélectionné, pour comprendre comment il fonctionne, commencez par rechercher si des vignettes sont disponibles. Une vignette est une page d’aide intégrée à un package qui permet d’aller au-delà de la documentation fonction par fonction que nous avons vue ci-dessus.\nPour voir quelles vignettes sont disponibles, vous pouvez utiliser la fonction vignette. En utilisant l’argument package, vous pouvez lister toutes les vignettes d’un package donné, puis appeler celle que vous souhaitez directement par son nom. Ainsi, vignette(package = \"dplyr\") va lister les cinq vignettes du package dplyr, puis vignette(\"two-table\") afficher la vignette consacrée aux verbes à deux tables.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Se documenter sur `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_se_documenter.html#vous-voulez-découvrir-des-choses-sur-r",
    "href": "03_Fiches_thematiques/Fiche_se_documenter.html#vous-voulez-découvrir-des-choses-sur-r",
    "title": "9  Se documenter sur R",
    "section": "9.4 Vous voulez découvrir des choses sur R",
    "text": "9.4 Vous voulez découvrir des choses sur R\nL’écosystème R est extrêmement vaste et évolue rapidement, si bien que la difficulté peut parfois être liée au fait de savoir qu’une fonctionnalité existe plutôt qu’à son utilisation proprement dit. Par conséquent, il est utile de consacrer du temps à se tenir informé de l’actualité et à échanger avec d’autres utilisateurs de R.\nLes utilisateurs de R forment de nombreuses communautés qui se superposent. Selon votre niveau en R, votre niveau en anglais et les sujets qui vous intéressent, vous pouvez rejoindre certaines de ces communautés. En voilà quelques unes :\n\nau sein du service statistique public, des communautés tchap sont actives sur les canaux #langage R et #spyrales (orienté formation) ;\nau niveau francophone, la communauté est très présente sur le slack r-grrr (inscription nécessaire mais libre). On peut également poser des questions sur le forum francophone ;\nau niveau mondial, de nombreux échanges ont lieu sur twitter (hashtag #rstats) ;\nle site Rbloggers agrège de nombreux posts de blogs relatifs à R. Il est possible de s’abonner à un flux pour recevoir, chaque jour, les derniers posts disponibles sur ce site.\n\n\nÀ l’Insee, en plus des groupes Tchap qui peuvent s’être formés localement, on peut trouver des échanges dans le canal #Insee - Outils stats V2.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Se documenter sur `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_se_documenter.html#RessourcesDocumentation",
    "href": "03_Fiches_thematiques/Fiche_se_documenter.html#RessourcesDocumentation",
    "title": "9  Se documenter sur R",
    "section": "9.5 Pour en savoir plus",
    "text": "9.5 Pour en savoir plus\n\nsite détaillant les fonctionnalités d’aide incluses dans R.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Se documenter sur `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html",
    "href": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html",
    "title": "10  Résoudre un problème avec R",
    "section": "",
    "text": "10.1 Tâches concernées et recommandations\nVotre code ne fait pas ce que vous voudriez qu’il fasse et vous ne comprenez pas ce qui ne fonctionne pas. Il faut alors chercher l’origine du problème, et demander de l’aide si vous ne parvenez pas à trouver une solution. Cette fiche explique comment isoler le problème et comment demander de l’aide.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Résoudre un problème avec `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#tâches-concernées-et-recommandations",
    "title": "10  Résoudre un problème avec R",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\nIl est recommandé de suivre la méthode suivante pour résoudre un problème avec R :\n\nCommencer par isoler le problème en le reproduisant sur un jeu de données publiques (iris, cars,… ), et en réduisant le code à un exemple minimal ;\nSi vous avez un message d’erreur, copier le dans un moteur de recherche, et vérifier si la question a déjà été traitée ;\nPosez une question sur Stackoverflow, un forum ou un service de messagerie instantanée en incluant toujours le code permettant de reproduire l’erreur.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Résoudre un problème avec `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#commencer-par-isoler-le-problème",
    "href": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#commencer-par-isoler-le-problème",
    "title": "10  Résoudre un problème avec R",
    "section": "10.2 Commencer par isoler le problème",
    "text": "10.2 Commencer par isoler le problème\nQuand on rencontre un problème en travaillant sur un programme, les sources d’erreurs potentielles sont multiples et pas toujours évidentes à identifier si on s’en tient aux messages d’erreur. De plus, il se peut que l’origine de l’erreur se situe à une étape intermédiaire de votre programme, et non à l’étape qui produit une erreur ou qui vous semble problématique.\nUn problème peut être dû à deux types d’erreur : soit une erreur de votre part (cas le plus fréquent), soit d’un bug dans une fonction de R (cas plus rare). Une erreur de votre part peut par exemple provenir d’un jeu de données mal préparé (présence de valeurs manquantes, valeurs hors normes, problème de format), d’un mauvais usage d’une fonction et de ses options, d’une version erronée d’un package… Un bug dans une fonction de R peut être dû au fait que les développeurs n’ont pas prévu le cas dans lequel vous vous trouvez, ou que le code de la fonction contient une erreur. Ces deux types d’erreur se résolvent de façon très différente. S’il s’agit d’une erreur de votre part, il faut corriger votre programme. S’il s’agit d’un bug dans une fonction de R, il faut le signaler aux développeurs de la fonction concernée. Il est donc essentiel d’isoler rapidement le problème pour comprendre de quel type d’erreur il s’agit.\nVoici deux conseils pour isoler le problème. Dans la grande majorité des cas, ces deux conseils sont suffisants pour trouver l’origine du problème et le résoudre.\n\nVous pouvez commencer par exécuter votre programme pas-à-pas en vérifiant à chaque étape si le résultat intermédiaire correspond à ce que vous attendez, jusqu’à trouver l’instruction qui pose problème.\nUne fois que vous avez identifié l’étape problématique, vous pouvez consulter la documentation des fonctions que vous utilisez en exécutant la commande ?fonction (exemple: ?mutate), et lire la fiche correspondante de la documentation utilitR s’il en existe une.\n\nSi la lecture de la documentation ne vous a pas permis de résoudre le problème, voici comment procéder pour aller plus loin. vous pouvez créez un nouveau script et essayer de reproduire ce que vous souhaitez faire, mais avec un jeu de données public (iris, mtcars, ggplot2::diamonds…). Cette approche est très fréquente sur internet et s’appelle MWE (minimum working example ou exemple minimal reproductible). L’intérêt d’écrire un exemple minimal est double :\n\nsi l’exemple minimal aboutit au résultat que vous souhaitez, vous savez que le problème provient d’une erreur de votre part, mais pas des fonctions que vous souhaitez utiliser. Il faut donc vérifier à nouveau les étapes précédentes du programme ;\nsi l’exemple minimal n’aboutit pas au résultat que vous souhaitez, vous disposez maintenant d’un exemple que vous allez pouvoir utiliser pour demander de l’aide.\n\n\nSi vous ne trouvez pas jeu de données adapté à votre problème, vous pouvez en constituer un nouveau à l’aide des fonctions data.frame(), tibble::tibble() ou data.table::data.table().",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Résoudre un problème avec `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#chercher-de-laide",
    "href": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#chercher-de-laide",
    "title": "10  Résoudre un problème avec R",
    "section": "10.3 Chercher de l’aide",
    "text": "10.3 Chercher de l’aide\n\n10.3.1 La question a-t-elle déjà été posée ?\nLa première chose à faire est de vérifier que la question n’a pas déjà été posée et résolue : il est rare de rencontrer un problème dont la solution ne figure pas déjà en ligne, en particulier quand on débute avec R. Si vous avez un message d’erreur, vous pouvez le copier/coller dans un moteur de recherche. Si vous n’en avez pas, le plus efficace est d’essayer de formuler votre problème et de rechercher ça dans un moteur de recherche générique, ou directement sur le moteur de recherche de Stack Overflow.\n\nLes ressources disponibles en ligne sur R en anglais sont beaucoup plus nombreuses et riches que celles existant en français. Il est donc préférable de faire des recherches en anglais, même si cela vous demande un effort supplémentaire.\n\n\n\n10.3.2 Où chercher de l’aide ?\nSelon le type de problème, votre niveau en anglais et le temps que vous êtes prêts à attendre, vous pouvez chercher de l’aide aux endroits suivants :\n\nStackoverflow (SO) est un site de questions/réponses sur de nombreuses thématiques liées à l’informatique. C’est la façon la plus efficace d’obtenir une réponse rapide, y compris sur des sujets particulièrement complexes, pour peu qu’on accepte de se plier au formalisme demandé (voir plus bas) ;\nle groupe slack grrr (francophone) dispose d’un canal #questions qui est assez réactif. Toujours en français, le forum du Cirad héberge une importante communauté R. S’il n’est plus la référence qu’il a longtemps été, il est toujours actif ;\nAu sein du service public français, il est possible de trouver de l’aide sur la messagerie instantanée Tchap, sur le canal #langage R ;\nA l’Insee, il y a également un canal dédié sur Tchap (#Insee - Outils stats v2).\n\nCes différentes plateformes sont classées par nombre d’utilisateurs décroissant, ce qui a un impact mécanique sur la rapidité des réponses et la chance de tomber sur un utilisateur capable de vous aider. Cependant, certaines problématiques sont propres à l’Insee ou au SSP et seront plus facilement traitées à ce niveau (problèmes d’environnement, de proxy ou de pare-feu par exemple). Par ailleurs, il peut être plus rassurant au début de poser sa question dans un environnement qu’on connaît.\n\n\n10.3.3 Comment poser une question ?\nLa formulation de votre question dépend de son contenu.\n\n10.3.3.1 Résoudre une erreur dans un programme\nSi vous essayez de résoudre une erreur dans un programme, votre question doit contenir les éléments suivants :\n\nUne description en une phrase du problème que vous rencontrez ;\nLe code de l’exemple minimal reproductible ;\nce que vous obtenez (données, messages d’erreur éventuels) ;\nce que vous aimeriez obtenir.\n\nIl est essentiel que l’exemple se suffise à lui-même. N’oubliez pas les appels éventuels de packages et travaillez avec des jeux de données publiques ou que vous reconstruisez dans votre code. Un bon test peut être de vider l’environnement, de relancer la session et d’exécuter votre exemple pour vérifier qu’il s’exécute correctement.\n\n\n\n\n\n\nTip\n\n\n\nIl arrive fréquemment que des erreurs soient difficiles à reproduire du fait d’un environnement différent. Il peut être utile, à la fin d’une question, de faire figurer le résultat de la commande sessionInfo() pour aider les autres personnes à comprendre dans quel environnement un problème advient\n\n\n\n\n10.3.3.2 Savoir comment réaliser une opération\nSi votre problème est que vous ne savez pas comment réaliser une certaine opération, votre question doit contenir les éléments suivants :\n\nUne description rapide des données que vous utilisez ;\nUne description de la tâche que vous souhaitez réaliser ;\nUne description de ce que vous souhaitez obtenir.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Résoudre un problème avec `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#comment-rédiger-sa-question",
    "href": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#comment-rédiger-sa-question",
    "title": "10  Résoudre un problème avec R",
    "section": "10.4 Comment rédiger sa question ?",
    "text": "10.4 Comment rédiger sa question ?\nUne question doit être lisible et intelligible, il faut donc accorder de l’attention à la mise en page. La plupart des sites d’entraide ou des messageries instantanées reposent sur la syntaxe Markdown ce qui permet d’isoler les blocs de code afin de les rendre lisibles la fiche R Markdown développe avec plus de détails ce format d’édition.\nLes morceaux de code doivent être introduits dans des blocs de la manière suivante:\n```r\nprint(\"toto\")\n```\nCeci permet de séparer de manière claire les éléments à exécuter dans R d’autres éléments (explications notamment). Le package reprex propose des fonctionnalités pour faciliter l’édition d’un post à partir d’un script (exemple ici)",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Résoudre un problème avec `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#stack-overflow",
    "href": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#stack-overflow",
    "title": "10  Résoudre un problème avec R",
    "section": "10.5 Zoom sur Stack Overflow",
    "text": "10.5 Zoom sur Stack Overflow\n\n10.5.1 Fonctionnement général\nStack Overflow est un site de questions/réponses qui fait référence sur de nombreux sujets en lien avec l’informatique. Son fonctionnement s’apparente à un forum : des utilisateurs posent des questions, d’autres y répondent. L’objectif de chaque page de discussion de Stack Overflow est de proposer une question aussi claire que possible, immédiatement suivie de la réponse la plus pertinente (et éventuellement avec les autres réponses ensuite). Une particularité essentielle de Stack Overflow est donc que les réponses ne sont pas ordonnées chronologiquement mais par pertinence. Par ailleurs, si la question a besoin d’être précisée ou mise à jour, l’auteur est invité à la modifier.\nLa pertinence des réponses est établi en fonction du choix de la personne qui a posé la question (elle décide de la réponse qu’elle accepte), et des autres contributeurs : tout le monde peut voter pour les réponses, positivement ou négativement.\nLes contributeurs de Stack Overflow sont hiérarchisés par un complexe système de réputation : on gagne des points de réputation quand on pose des questions jugées pertinentes, et quand on apporte des réponses qui sont jugées pertinentes. Plus la réputation d’un contributeur est élevée, plus il peut contribuer au site (nombre de votes disponibles, possibilité d’éditer les questions d’autres contributeurs voire d’en supprimer).\n\n\n10.5.2 Poser une question sur Stack Overflow\nEn plus des conseils donnés plus haut, voici quelques règles spécifiques à Stack Overflow :\n\nil est nécessaire de créer un compte pour poser une question (mais pas pour consulter les réponses) ;\nvotre question doit impérativement être rédigée en anglais, et comprendre un exemple minimal reproductible ;\nil faut accorder une importance particulière au titre de votre question, votre titre doit résumer le problème autant que possible ;\npensez à insérer des étiquettes (tags). Les tags permettent de catégoriser les questions. Ainsi, le tag #r rassemble toutes les questions ayant trait à R, le tag #dplyr celles ayant trait à dplyr, ainsi de suite ;\nrépondez aux éventuelles questions de clarification qu’on vous posera en commentaires en éditant votre question ;\nvotez pour les réponses qui vous semblent pertinentes. Cela aide la communauté, et donc vous également, à déterminer les meilleures solutions à un problème ;\nprenez soin de votre réputation en évitant les doublons et en prenant le temps de rédiger votre question.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Résoudre un problème avec `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#RessourcesResoudre",
    "href": "03_Fiches_thematiques/Fiche_resoudre_un_probleme.html#RessourcesResoudre",
    "title": "10  Résoudre un problème avec R",
    "section": "10.6 Pour en savoir plus",
    "text": "10.6 Pour en savoir plus\n\nComment poser une question sur le site de Stackoverflow ;\nComment faire un bon exemple reproductible avec R ;\nReprex, ou comment demander de l’aide efficacement.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Résoudre un problème avec `R`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html",
    "href": "03_Fiches_thematiques/Fiche_targets.html",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "",
    "text": "11.1 Tâches concernées et recommandations\nL’utilisateur souhaite automatiser une chaîne de traitement complexe afin de la rendre reproductible et rapide à exécuter en cas de modification.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_targets.html#tâches-concernées-et-recommandations",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\nLe package targets permet de construire simplement une chaîne de traitement reproductible.\nLes deux éléments suivants sont à prendre en considération :\n\nce package ne sera approprié que si la chaîne de traitement est exclusivement écrite en R ;\nil est fortement recommandé de savoir créer des fonctions, afin de modulariser le code.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html#pourquoi-utiliser-targets",
    "href": "03_Fiches_thematiques/Fiche_targets.html#pourquoi-utiliser-targets",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "\n11.2 Pourquoi utiliser targets ?",
    "text": "11.2 Pourquoi utiliser targets ?\nLe package targets peut être particulièrement intéressant :\n\ndans le cadre du développement d’un prototype ayant vocation à devenir une chaîne de production pérenne écrite avec R ;\ndans le cas d’un projet d’étude qui vise à une forte reproductibilité.\n\nPlus précisément, utiliser targets pour un projet permet de :\n\nViser la reproductibilité de l’ensemble des étapes de traitement, tout en réduisant au strict nécessaire la répétition de ces étapes, parfois longues ;\nAdopter des bonnes pratiques de développement en R par l’usage (modulariser le code, décomposer ses traitements par étapes, assurer la lisibilité des étapes successives du traitement…)\nReprésenter sous forme de pipeline les étapes de sa chaîne de traitement et leurs dépendances à partir d’une technique de graphiques directionnels asynchrones (appelés DAG pour l’acronyme anglais dans la sphère informatique)\nFaciliter la prise en main par une autre personne grâce à une organisation standardisée des codes et à une description complète de l’enchaînement des étapes intégrée dans le code lui-même.\n\n\n\n\n\n\n\nNote\n\n\n\nLe guide des bonnes pratiques utilitR devrait prochainement s’enrichir d’éléments concernant la gestion de pipelines de données en R et en Python.\nLes premiers éléments du débat sont disponibles sur l’issue #388 dans le dépôt Github d’utilitR.",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html#quelles-sont-les-tâches-automatisées-par-targets",
    "href": "03_Fiches_thematiques/Fiche_targets.html#quelles-sont-les-tâches-automatisées-par-targets",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "\n11.3 Quelles sont les tâches automatisées par targets ?",
    "text": "11.3 Quelles sont les tâches automatisées par targets ?\ntargets permet de définir et d’exécuter une chaîne de traitement avec :\n\nSauvegarde automatique de résultats intermédiaires, ce qu’on appelle les “targets” (cibles)\nTraçabilité de ces résultats intermédiaires par targets: lors de la répétition d’une exécution de la chaîne de traitement, ils ne sont mobilisés que si ils sont reproductibles.\nSi une fonction ou un input nécessaire au calcul d’une “target” est modifié, targets repère automatiquement les étapes à reconduire, et seulement celles-ci.\n\nAinsi, le lancement du traitement et la vérification de la reproductibilité sont effectués ensemble au cours du développement du projet par l’appel de tar_make().\nVérifier la reproductibilité revient ainsi à ne pas ‘tout relancer’ de 0 ! Ceci représenterait un coût trop élevé. targets automatise le travail d’aller-retour dans les étapes d’une étude ou de prototypage (j’ai modifié l’étape 1, il faut donc que je relance l’étape 2 qui en dépend…), en construisant un graphe des dépendances des différentes étapes du traitement.\nPour la suite de la fiche, prenons l’exemple d’une étude qui se structurerait suivant les étapes suivantes :\n\nCharger les données\nTraiter les données\nProduire des résultats\nReprésenter des résultats",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html#un-projet-minimal-pour-comprendre-lessentiel",
    "href": "03_Fiches_thematiques/Fiche_targets.html#un-projet-minimal-pour-comprendre-lessentiel",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "\n11.4 Un projet minimal pour comprendre l’essentiel",
    "text": "11.4 Un projet minimal pour comprendre l’essentiel\n\n11.4.1 Structure du projet\nUn projet targets est un projet R en règle générale structuré de la sorte :\n\nun fichier _targets.R décrivant les éléments de configuration (par exemple packages utilisés) et l’enchaînement des traitements\nun dossier R comprenant les scripts définissant les fonctions utilisées par le projet\nun dossier data pour les données externes (non générées au cours du projet)\n\nL’architecture des dossiers du projet ressemble par conséquent à ceci :\n├── _targets.R\n├── R/\n├───── mesfonctions_pour_faire_ceci.R\n├───── mesfonctions_pour_faire_cela.R\n├──── ...\n├── data/\n├───── donnees_entrees.csv\n└───── ...\n\n\n\n\n\n\nTip\n\n\n\nOrganiser ses fichiers de cette façon est très commun, mais pas indispensable pour l’utilisation de targets. La seule obligation est que le fichier _targets.R soit positionné dans le répertoire de travail.\nUne manière commode pour un utilisateur souhaitant utiliser targets est donc de créer un projet RStudio à la racine duquel il place ce fichier. En prévision des futures fonctions qu’il va écrire, il crée un dossier R/. Le fichier _targets.R détaille l’enchaînement des traitements. Il doit toujours contenir une instruction chargeant le package targets.\n\n\n\n11.4.2 Premier exemple\nPartons d’un exemple simple :\n\non lit les données de population depuis un fichier CSV ;\non a créé une fonction pour ne garder que les communes de plus de 200 000 habitants ;\nsur ces communes, on désire connaître la proportion dont le revenu médian est supérieur à 25 000 euros.\n\nLa chaîne de traitement est donc ici linéaire. Chaque étape dépend de la précédente et uniquement de celle-ci. Le fichier d’instruction _targets prendra alors la forme suivante:\n\n# fichier _targets.R\n\nlibrary(targets)\n\ntar_option_set(packages = c(\"dplyr\", \"readr\"))\n\nsource(\"mesfonctions_pour_faire_ceci.R\", encoding = \"utf-8\")\n\n# on crée un fichier à partir d'un des jeux d'exemples\nraw_file_path &lt;- \"data/donnes_entrees.csv\"\ndir.create(\"data\")\nreadr::write_csv(doremifasolData::filosofi_com_2016, raw_file_path)\n\nlist(\n  \n  tar_target(csv_file, raw_file_path, format = \"file\"),\n\n  tar_target(\n    raw_filosofi_epci, readr::read_csv(csv_file),\n  ),\n  tar_target(\n    grandes_villes, garde_grandes_villes(raw_filosofi_epci)\n  ),\n  tar_target(\n    prop_sup_25k, grandes_villes %&gt;% dplyr::summarise(mean(MED16 &gt; 25000)*100)\n  )\n)\n\nLes fonctions écrites par l’analyste et utilisées dans la chaîne de traitement (en l’occurrence garde_grandes_villes) sont contenues dans les fichiers que l’on “source” au départ, ici depuis un script \"mesfonctions_pour_faire_ceci.R.\nLes packages utilisés dans les traitements sont définis via la fonction tar_option_set du package targets. Ici, on a besoin des packages dplyr et readr dans notre chaîne de traitement.\nLa chaîne de traitement est représentée par une liste de tar_target, soit les objets R qui sont les cibles intermédiaires de l’analyse. Ils sont le résultat de l’application à une cible précédente d’une fonction pour obtenir la cible suivante :\n\nIci la première cible est particulière (format = file) : on spécifie où sont les données d’entrée afin de surveiller si elles changent.\nLa seconde prend en entrée la première cible data_file et la transforme en appliquant la fonction readr::read_csv en un nouvel objet R, raw_filosofi_epci. Il s’agit ainsi des données brutes après l’import dans R, avant toute modification\nLa troisième applique cette fois une fonction écrite par l’utilisateur à raw_filosofi_epci pour obtenir grandes_villes, et ainsi de suite…\n\nAinsi, le fichier _targets.R contient la description de l’ensemble des étapes du traitement. La complexité des traitements est résumée de façon concise par un ensemble minimal de fonctions résumant les grandes étapes. Afin de faire tourner l’analyse, l’utilisateur fait appel au sein du projet à la fonction tar_make(). Il s’agit de la fonction qu’un utilisateur du package targets utilisera le plus fréquemment. L’utilisateur est informé de l’évolution des calculs.\n\ntar_make()\n\n- The project is out-of-sync -- use `renv::status()` for details.\n▶ dispatched target csv_file\n● completed target csv_file [0.064 seconds, 4.193 megabytes]\n▶ dispatched target raw_filosofi_epci\nRows: 34932 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): CODGEO, LIBGEO\ndbl (27): NBMENFISC16, NBPERSMENFISC16, MED16, PIMP16, TP6016, TP60AGE116, T...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n● completed target raw_filosofi_epci [0.148 seconds, 961.603 kilobytes]\n▶ dispatched target grandes_villes\n● completed target grandes_villes [0.011 seconds, 1.479 kilobytes]\n▶ dispatched target prop_sup_25k\n● completed target prop_sup_25k [0.003 seconds, 158 bytes]\n▶ ended pipeline [0.484 seconds]\n\n\nLorsque la chaîne de traitement est de taille relativement modeste (comme ici), on peut la visualiser avec la fonction tar_visnetwork:\n\ntar_visnetwork()\n\nOn obtient bien un diagramme linéaire comme on en avait l’intuition.\n\n\n\n\n\n\nNote\n\n\n\nIl est tout à fait possible de stocker l’ensemble des cibles intermédiaires dans un emplacement différent du projet. Il s’agit même d’une bonne pratique de séparer le lieu de stockage du code de celui des données.\nIl sera nécessaire d’éditer les options de la chaîne dans le fichier _targets.R. Par exemple avec cette ligne de commande, au début du fichier _targets.R (mais après l’appel à library(targets):\n\ntar_config_set(store = \"mon_dossier_donnees/projet-toto\")\n\n\n\n\n11.4.3 Modification d’une étape intermédiaire\nL’utilisateur décide ensuite de modifier la définition des grandes villes considérées. Supposons qu’il ajoute un argument à la fonction garde_grandes_villes pour ne garder que celles dont la population est supérieure à seuil. Dans le fichier _targets.R, il est nécessaire de changer la définition de l’étape de définition de grandes_villes. Cela amènera à une chaîne ayant la structure suivante\n\n# fichier _targets.R\n\nlibrary(targets)\n\ntar_option_set(packages = c(\"dplyr\", \"readr\"))\n\nsource(\"mesfonctions_pour_faire_ceci.R\", encoding = \"utf-8\")\n\n# on crée un fichier à partir d'un des jeux d'exemples\nraw_file_path &lt;- \"data/donnes_entrees.csv\"\ndir.create(\"data\")\nreadr::write_csv(doremifasolData::filosofi_com_2016, raw_file_path)\n\nlist(\n  \n  tar_target(csv_file, raw_file_path, format = \"file\"),\n  \n  tar_target(\n    raw_filosofi_epci, readr::read_csv(csv_file),\n  ),\n  tar_target(\n    grandes_villes, garde_grandes_villes(raw_filosofi_epci, seuil = 10000)\n  ),\n  tar_target(\n    prop_sup_25k, grandes_villes %&gt;% dplyr::summarise(mean(MED16 &gt; 25000)*100)\n  )\n)\n\nIci, le pipeline est de taille relativement modeste et il est facile d’identifier la source de modification. Néanmoins, la représentation sous forme de diagramme peut aider à mieux s’en rendre compte\n\ntar_visnetwork()\n\nLa modification de la fonction garde_grandes_villes entraîne la nécessaire mise à jour de grandes_villes et toutes les cibles qui en dépendent, mais pas du début de la chaîne de traitement !\ntargets va ainsi intelligemment utiliser ceci pour minimiser le temps nécessaire pour mettre à jour l’ensemble de la chaîne de traitement\n\ntar_make()\n\n- The project is out-of-sync -- use `renv::status()` for details.\n✔ skipped target csv_file\n✔ skipped target raw_filosofi_epci\n▶ dispatched target grandes_villes\n● completed target grandes_villes [0.006 seconds, 70.69 kilobytes]\n▶ dispatched target prop_sup_25k\n● completed target prop_sup_25k [0.003 seconds, 164 bytes]\n▶ ended pipeline [0.194 seconds]\nWarning message:\nIn dir.create(\"data\") : 'data' already exists\n\n\nLes cibles définies sont calculées successivement, stockées et mises à jour automatiquement dans un dossier _targets/objects/.\n\n11.4.4 Accéder à des éléments du pipeline dans une session R\n\nOn peut facilement accéder à un objet cible, quel que soit son emplacement dans la chaîne de traitement, puisque chaque cible est stockée sous la forme d’un fichier temporaire.\nLa fonction tar_load permet de charger dans l’environnement R l’objet en question. Par exemple, si on désire tester des choses sur grandes_villes, on pourra utiliser la commande suivante\n\ntar_load(grandes_villes)\nhead(grandes_villes)\n\n# A tibble: 6 × 29\n  CODGEO LIBGEO      NBMENFISC16 NBPERSMENFISC16  MED16 PIMP16 TP6016 TP60AGE116\n  &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 01004  Ambérieu-e…        6363          14228  19721      49     17         19\n2 01033  Valserhône         6472          15255  21405.     45     16         18\n3 01053  Bourg-en-B…       18601          38014. 18249.     46     22         27\n4 01173  Gex                4894          11276. 32304.     60     11         NA\n5 01283  Oyonnax            9248          22444. 16948.     40     25         31\n6 02168  Château-Th…        6805          15070. 17643.     43     24         33\n# ℹ 21 more variables: TP60AGE216 &lt;dbl&gt;, TP60AGE316 &lt;dbl&gt;, TP60AGE416 &lt;dbl&gt;,\n#   TP60AGE516 &lt;dbl&gt;, TP60AGE616 &lt;dbl&gt;, TP60TOL116 &lt;dbl&gt;, TP60TOL216 &lt;dbl&gt;,\n#   PACT16 &lt;dbl&gt;, PTSA16 &lt;dbl&gt;, PCHO16 &lt;dbl&gt;, PBEN16 &lt;dbl&gt;, PPEN16 &lt;dbl&gt;,\n#   PPAT16 &lt;dbl&gt;, PPSOC16 &lt;dbl&gt;, PPFAM16 &lt;dbl&gt;, PPMINI16 &lt;dbl&gt;, PPLOGT16 &lt;dbl&gt;,\n#   PIMPOT16 &lt;dbl&gt;, D116 &lt;dbl&gt;, D916 &lt;dbl&gt;, RD16 &lt;dbl&gt;\n\n\nCela permettra à l’utilisateur de targets de prototyper une nouvelle étape de traitement dans sa session R puis, une fois satisfait, la mettre en production en mettant les fonctions dans le fichier XXXXX.R et en créant l’étape tar_target adéquate.\n\n\n\n\n\n\nTip\n\n\n\nPar défaut, les cibles sont stockées au format rds. Ce format présente deux inconvénients :\n\nil est spécifique à R et ne permet pas de lire les étapes intermédiaires dans un autre langage (par exemple Python) ;\nla sérialisation des objets R nécessaire pour écrire sous format rds ou lire un tel fichier est assez lente.\n\nIl est conseillé d’utiliser un autre format de stockage des cibles.\nEn premier lieu, le format par défaut qui peut être utilisé est le format qs. À l’instar du format rds, celui-ci est spécifique à R mais présente l’avantage d’être beaucoup plus rapide en termes de temps en lecture/écriture. Pour cela, il convient d’ajouter la ligne suivante au début des options du fichier _targets.R :\n\ntar_option_set(format = \"fst_dt\")\n\nPour les dataframes, il est possible d’utiliser des formats plus universels ou plus appropriés. Les formats à privilégier sont les suivants:\n\n\nparquet: format qui tend à devenir un standard dans le monde de la science des données. Ce format présente plusieurs avantages, parmi lesquels le fait qu’il est très compressé, très rapide et qu’il conserve les métadonnées du fichier ce qui permet, à la différence des formats type CSV, de conserver l’intégrité des typages des colonnes (voir la fiche Importer des fichiers parquets pour plus de détails) ;\n\nfst_tbl (utilisateurs du tidyverse) ou fst_dt (utilisateurs de data.table) : formats spécifiques à R présentant des avantages proches de ceux d’un fichier parquet. Ils préservent la nature d’un data.frame, ce qui permet de repartir d’un tibble ou d’un datatable sans avoir à faire de conversion à chaque étape du pipeline.\n\nLe choix du format de stockage d’un objet se fait directement lors de la déclaration de la cible dans _targets.R:\n\ntar_target(\n    grandes_villes, garde_grandes_villes(raw_filosofi_epci),\n    format = \"parquet\"\n)\n\nDans le dossier _targets/object, le fichier sera ainsi stocké au format exigé.\nIl n’est pas recommandé d’utiliser les formats parquet, fst_dt ou fst_tbl par défaut car ils ne permettent de stocker que des dataframes. Or, un pipeline peut stocker des objets de nature beaucoup plus diverses (listes, objets ggplot, etc.)\n\n\n\n\n\n\n\n\nNote\n\n\n\nL’utilisation du garbage collector peut parfois s’avérer utile pour nettoyer la mémoire de la session R dans laquelle tourne le pipeline. Ceci est particulièrement utile lorsque les objets manipulés sont volumineux (voir la fiche Superviser sa session R).\nDans targets, cette opération est possible en ajoutant l’argument garbage_collection = TRUE à la définition de la cible :\n\ntar_target(\n    grandes_villes, garde_grandes_villes(raw_filosofi_epci),\n    garbage_collection = TRUE\n)",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html#intégrer-un-rapport-en-rmarkdown",
    "href": "03_Fiches_thematiques/Fiche_targets.html#intégrer-un-rapport-en-rmarkdown",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "\n11.5 Intégrer un rapport en Rmarkdown",
    "text": "11.5 Intégrer un rapport en Rmarkdown\nL’un des principaux gains à utiliser targets est dans la fiabilisation du processus de production de fichiers markdown à l’issue d’une chaîne de traitement.\nDeux philosophies existent pour produire un fichier reproductible dans une chaîne de traitement :\n\nIntégrer directement le fichier à la chaîne comme une étape finale du processus de production. Cela revient à produire le RMarkdown via un tar_target particulier ;\nExécuter la chaîne de traitement, ou les parties nouvelles de la chaîne de traitement, directement depuis le fichier RMarkdown. Dans ce cas, le fichier .Rmd n’est plus exécuté depuis le _targets.R mais au contraire sert à l’exécuter.\n\n\n11.5.1 Concevoir un rapport en sortie de chaîne de traitement\nLe package tarchetypes est un complément utile. Ce package permet d’intégrer simplement des rapports Rmarkdown dans la pipeline avec tarchetypes::tar_render(). L’essentiel des calculs doit être en amont du rapport markdown, qui doit être rapide à exécuter.\nPar exemple, on peut écrire un Rmarkdown report.Rmd considéré comme une des cibles de l’analyse (par exemple, c’est le compte-rendu de l’analyse), et qui dépend d’autres cibles. On souhaite également qu’il soit reproductible, et mis à jour automatiquement en fonction des modifications sur les cibles dont il dépend.\nIl suffit d’intégrer ces cibles via tar_read(data) ou tar_load(data) appelé dans un chunk du .Rmd, et de spécifier un _targets.R sur le modèle suivant :\n\n# Fichier _targets.R\n# report.Rmd est présent dans le projet.\nlibrary(targets)\nlibrary(tarchetypes)\n\nlist(\n  tar_target(data, data.frame(a = seq(2,9), b = seq(2,9))),\n  tar_render(report, path = 'report.Rmd')\n)\n\n\n11.5.2 Utiliser des objets issus d’une chaîne de traitement dans un R Markdown\n\nCette méthode est particulièrement appropriée lorsqu’on désire prototyper un rapport en utilisant un ou plusieurs objets de la chaîne de traitement.\nPlus d’éléments sont disponibles dans la documentation officielle",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html#les-branches",
    "href": "03_Fiches_thematiques/Fiche_targets.html#les-branches",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "\n11.6 Les branches",
    "text": "11.6 Les branches\nSouvent, les cibles d’une analyse (étapes intermédiaires) sont nombreuses et ont un certain degré de redondance.\nComment créer des cibles automatiquement (sans écrire explicitement dans _targets.R chacune d’entre elles) ? targets propose de décliner les cibles en “branches”.\nOn distingue :\n\nles branches définies dynamiquement : avant l’exécution, le nombre de branches est inconnu ;\nles branches définies statiquement : le nombre de branche est défini précisément avant l’exécution.\n\nLe premier cas correspond à la répétition d’un grand nombre de tâches homogènes, le second plutôt à un petit nombre de tâches hétérogènes.\n\n\n\n\n\n\nNote\n\n\n\nLes branches statiques, qui nécessitent l’usage du package tarchetypes, ne sont pas abordées ici.\n\n\n\n11.6.1 Les branches dynamiques\nCertaines cibles peuvent être le résultat de l’application d’une même fonction à des variantes d’arguments (par exemple, un graphique de restitution pour plusieurs populations d’intérêt).\nPour cela, targets propose les branches dynamiques.\n\n11.6.2 Un exemple\nVoici un exemple minimal de pipeline qui va itérer sur N couples d’arguments une même “simulation”, en évitant de créer N cibles distinctes pour les N résultats, et plutôt créer une seule cible résultats qui donnera lieu à autant de branches que de “simulations” :\n\n#_targets.R\nlibrary(targets)\n\nsimulation &lt;- function(x, y) x * y\n\nlist(\n  tar_target(x, c(10, 20, 30)),\n  tar_target(y, c(1, 2, 3)),\n  tar_target(\n    resultat,\n    data.frame(argument_1 = x, argument_2 = y, res = simulation(x, y)),\n    pattern = map(x, y))\n)\n\nCe qui distingue ici la cible resultat de ce qui a été vu précédemment, c’est l’utilisation de l’argument pattern, qui a vocation à itérer sur les vecteurs cibles x et y grâce à map.\nDans la console R, l’utilisateur qui fait appel à tar_make() voit apparaître la déclinaison de resultat en trois branches, exécutées en parallèle.\n\ntar_make()\n\n● run target x\n● run target y\n● run branch resultat_1851c9ee\n● run branch resultat_445bc859\n● run branch resultat_1a0263ff\n● end pipeline\nOn obtient le résultat suivant:\n\ntar_read(resultat)\n\nargument_1 argument_2 res\n1         10          1  10\n2         20          2  40\n3         30          3  90\n\n11.6.3 Itérer, croiser les arguments pour créer des branches\nLes patterns peuvent être de plusieurs types : map (itérer sur les arguments ligne à ligne), cross (produit cartésien des arguments), head (pour récupérer les premiers arguments), select (pour récupérer certains arguments) …\nPar exemple, remplacer map par cross dans la pipeline précédente donne lieu après un tar_make() à\n✓ skip target x\n✓ skip target y\n✓ skip branch resultat_1851c9ee\n● run branch resultat_cca1045b\n● run branch resultat_3b73d14e\n● run branch resultat_fe2f6b6a\n✓ skip branch resultat_66951ce8\n● run branch resultat_ff612dde\n● run branch resultat_d0a65303\n● run branch resultat_0a18e8b1\n✓ skip branch resultat_7fd56d9a\n● end pipeline\nPlutôt que d’appliquer la fonction simulation itérativement aux couples d’x et y (3 branches), la fonction est appliquée au produit cartésien de x et y (3 x 3 branches). On remarque d’ailleurs que targets a compris que cela ne changeait pas certains résultats précédents (3 branches strictement identiques, qui ne sont pas recalculées).\n\ntar_read(resultat)\n\nargument_1 argument_2 res\n1         10          1  10\n2         10          2  20\n3         10          3  30\n4         20          1  20\n5         20          2  40\n6         20          3  60\n7         30          1  30\n8         30          2  60\n9         30          3  90\nLes pattern peuvent être combinés, avec par exemple pattern = cross(x, map(y, z)).\n\n#_targets.R\nlibrary(targets)\n\nsimulation &lt;- function(x, y, z) x * y + z\n\nlist(\n  tar_target(x, c(10, 20, 30)),\n  tar_target(y, c(1, 2, 3)),\n  tar_target(z, c(2, 4, 6)),\n  tar_target(\n    resultat,\n    data.frame(argument_1 = x, argument_2 = y, argument_3 = z, res = simulation(x, y, z)),\n    pattern = cross(x, map(y, z)))\n)\n\nqui donne le résultat :\nargument_1 argument_2 argument_3 res\n1         10          1          2  12\n2         10          2          4  24\n3         10          3          6  36\n4         20          1          2  22\n5         20          2          4  44\n6         20          3          6  66\n7         30          1          2  32\n8         30          2          4  64\n9         30          3          6  96\nSi l’on souhaite itérer sur des listes, plutôt que sur des vecteurs, on peut spécifier à la création de la cible qui sert d’argument aux branches, par exemple une liste de data.frames, que l’on veut itérer sur les éléments \"list\".\n\n#_targets.R\nlibrary(targets)\n\n#' Multiplie la colonne \"a\" de df par un facteur\n#' @param: df: data.frame\n#' @param: factor: int\nmultiply &lt;- function(df, factor){\n  df$a &lt;- df$a * factor\n  df\n}\n\nlist(\n  tar_target(x, list(data.frame(name = c('Marie','Marwan'), a = c(1, 2)),\n                     data.frame(name = c('Bill','Boule'), a = c(2, 4))), iteration = 'list'),\n  tar_target(y, c(2, 3)),\n  tar_target(\n    resultat,\n    multiply(x, y),\n    pattern = map(x, y))\n)\n\nEtc…",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_targets.html#pour-en-savoir-plus",
    "href": "03_Fiches_thematiques/Fiche_targets.html#pour-en-savoir-plus",
    "title": "11  Construire une chaîne de traitement reproductible avec targets",
    "section": "\n11.7 Pour en savoir plus",
    "text": "11.7 Pour en savoir plus\n\nManuel d’utilisation de targets\n\nOrganiser un projet avec targets, une chapitre de Introduction à R et au tidyverse de Julien Barnier\nHigh Performance Computing avec targets\nLandau, W. M., (2021). The targets R package: a dynamic Make-like function-oriented pipeline toolkit for reproducibility and high-performance computing. Journal of Open Source Software, 6(57), 2959, https://doi.org/10.21105/joss.02959\nVidéo de présentation de targets par Will Landau au meetup R Lille de juin 2021\nhttps://cran.r-project.org/web/packages/targets/targets.pdf\nhttps://docs.ropensci.org/tarchetypes/\nUn exemple https://github.com/InseeFrLab/lockdown-maps-R/\nLes “target factories”: https://wlandau.github.io/targetopia/contributing.html\nUn tutoriel de Noam Ross présentant l’usage de targets avec un système de stockage de type AWS (similaire au principe du SSPCloud): https://github.com/noamross/targets-minio-versioning",
    "crumbs": [
      "Introduction",
      "Mener un projet statistique avec R",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Construire une chaîne de traitement reproductible avec `targets`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html",
    "title": "12  Importer des fichiers plats (.csv, .tsv, .txt)",
    "section": "",
    "text": "12.1 Tâches concernées et recommandations\nL’utilisateur souhaite importer dans R des données stockées sous forme de fichiers plats (formats .txt, .csv, .tsv).",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importer des fichiers plats (`.csv`, `.tsv`, `.txt`)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#tâches-concernées-et-recommandations",
    "title": "12  Importer des fichiers plats (.csv, .tsv, .txt)",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nPour importer des données de taille réduite (jusqu’à 1 Go), il est recommandé d’utiliser la fonction read_delim() du package readr ;\nPour importer des données de taille plus importante (supérieure à 1 Go), il est recommandé d’utiliser la fonction fread() du package data.table.\n\nL’usage du package csvread est déconseillé, de même que l’utilisation des fonctions natives de R read.csv() et read.delim().",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importer des fichiers plats (`.csv`, `.tsv`, `.txt`)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#importer-un-fichier-avec-le-package-readr",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#importer-un-fichier-avec-le-package-readr",
    "title": "12  Importer des fichiers plats (.csv, .tsv, .txt)",
    "section": "\n12.2 Importer un fichier avec le package readr\n",
    "text": "12.2 Importer un fichier avec le package readr\n\nLe package readr propose plusieurs fonctions adaptées pour importer des fichiers plats. Parmi elles, la fonction read_delim() permet de lire les fichiers csv et cela quelque soit le délimiteur (virgule ou point-virgule) et le marqueur décimal (point ou virgule).\nIl faut charger le package readr pour utiliser cette fonction :\n\nlibrary(readr)\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous êtes complètement débutants en R, il est recommandé d’utiliser l’utilitaire d’importation de RStudio. Une fois que les données sont correctement importées, vous pourrez copier-coller le code dans votre script R et vous familiariser avec les fonctions du package readr.\n\n\n\n12.2.1 Utiliser l’assistant d’importation RStudio\n\nPour les utilisateurs débutants, RStudio propose une interface graphique très commode pour importer des fichiers plats avec readr. On y accède avec : File &gt; Import Dataset &gt; From text (readr).... On obtient la fenêtre suivante. En cliquant sur Browse (rectangle rouge), on peut définir le chemin du fichier que l’on souhaite importer.\n\nUne fois que le fichier à importer a été sélectionné, un aperçu des premières lignes du fichier s’affiche dans la fenêtre. Dans l’exemple ci-dessous, on essaie d’importer le fichier des communes du Code Officiel Géographique (version 2019). La fenêtre comprend deux panneaux très utiles :\n\nUn panneau qui permet de définir les options d’importation (rectangle orange) ;\nUn panneau qui donne le code qui réalise l’importation demandée (rectangle vert).\n\n\nLes principales options d’importation comprennent notamment :\n\n\nName : Le nom du data.frame dans lequel les donnée seront stockées ;\n\nFirst Row as Names : à cocher si la première ligne contient les noms de colonnes ;\n\nDelimiter : Indique le délimiteur des données. Pour mémoire : Comma = virgule, Semicolon = point virgule, Tab = tabulation, Whitespace = espace, Other... = autre (à définir) ;\n\nLocale... : définit les options locales d’importation, notamment l’encodage et le marqueur décimal qui sont des sources récurrentes de problèmes ;\n\nNA : indique la valeur retenue lorsqu’une valeur est manquante.\n\nEnfin, il est possible de modifier le type des données en cliquant sur la petite flèches à côté de l’en-tête de colonne (flèches noires).\n\n12.2.2 Utiliser la fonction read_delim()\n\nLa fonction read_delim() est faite pour lire toutes sortes de fichiers plats, et propose de nombreuses options pour l’adapter au fichier considéré.\nVoici les principales options de read_delim() :\n\n\n\n\n\n\n\nArgument\nValeur par défaut\nFonction\n\n\n\nfile\nAucune\nLe chemin du fichier à importer\n\n\ndelim\nAucune\nLe délimiteur du fichier plat\n\n\nescape_backslash\nFALSE\nLes caractères spéciaux du fichier plat ont-ils un échappement (\\)\n\n\ncol_names\nTRUE\nLa première ligne contient-elle les noms de colonne ?\n\n\ncol_types\nNULL\nDéfinir le type des variables\n\n\ncol_select\nNULL\nChoisir les variables à importer\n\n\nskip\n0\nSauter les n premières lignes (0 par défaut)\n\n\nn_max\nInf\nNombre maximum de lignes à importer (pas de limite par défaut)\n\n\nlocale\n\nRéglages locaux (encodage, marqueur décimal…)\n\n\n\nQuelques remarques sur les options de read_delim() :\n\n\nread_delim() essaie par défaut de deviner le type des colonnes (integer pour les nombres entiers, character pour les chaînes de caractères…). L’option col_types permet de choisir le type des colonnes, et doit être égale à un vecteur dont chaque élément est de la forme nom_variable = [type de colonne]. Les types de colonnes disponibles sont col_integer(), col_logical(), col_double(), col_character() (voir ?cols pour la liste complète). Exemple : si on importe une variable comme nombre entier et une variable comme caractère, on écrit : col_types = cols(var1 = col_integer(), var2 = col_character()).\n\nExemple : on veut importer le fichier des communes du code officiel géographique (version 2019, disponible ici), en déclarant que le fichier est encodé en UTF-8 et en imposant que le code commune (com) soit lu comme une chaîne de caractères et le code région (reg) comme un nombre entier. On écrit le code suivant :\n\n# Dans cet exemple, il faut remplacer \"mon_IDEP\" par votre IDEP\nlibrary(readr)\ncommunes &lt;- read_delim(\"Z:/mon_IDEP/communes-01012019.csv\", \n                       locale = locale(encoding =\"UTF-8\"),\n                       col_types = cols(com = col_character(),\n                                        reg = col_integer())\n                      )\nnames(communes)\n\nLa fonction read_delim() contient également l’option lazy qui lorsque elle est fixée à TRUE permet d’améliorer la vitesse de traitement d’un fichier csv. Cette idée de “lecture paresseuse”, explorée pour la première fois dans le paquet vroom, consiste à optimiser la quantité du fichier total auquel un utilisateur a besoin d’accéder en fonction de sa requête. Ce billet de blog de RStudio illustre ce concept.\nPour en savoir plus sur read_delim(), il suffit de consulter l’aide avec ?read_delim.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importer des fichiers plats (`.csv`, `.tsv`, `.txt`)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#importer-un-fichier-avec-le-package-data.table",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#importer-un-fichier-avec-le-package-data.table",
    "title": "12  Importer des fichiers plats (.csv, .tsv, .txt)",
    "section": "\n12.3 Importer un fichier avec le package data.table\n",
    "text": "12.3 Importer un fichier avec le package data.table\n\nLe package data.table permet d’importer des fichiers plats avec la fonction fread(). Cette fonction présente trois avantages :\n\nElle est très rapide pour importer de gros volumes de données (et nettement plus rapide que les fonctions du package readr). Voir ici ;\nElle permet de sélectionner facilement les colonnes qu’on veut importer (option select) ;\nElle propose un grand nombre d’options, adaptées pour les usages avancés.\n\nPour utiliser fread(), il faut charger le package data.table :\n\nlibrary(data.table)\n\nLes principales options de fread() sont les suivantes :\n\n\n\n\n\n\n\nArgument\nValeur par défaut\nFonction\n\n\n\nfile\nAucune\nLe chemin du fichier à importer\n\n\nsep\nLe caractère le plus fréquent parmi ,\\t |;:\n\nLe délimiteur du fichier\n\n\nheader\n\nfread() essaie de deviner\nLa première ligne contient-elle les noms de colonnes ?\n\n\nnrows\nInf\nNombre maximum de lignes à importer (pas de limite par défaut)\n\n\nskip\n0\nSauter les n premières lignes (0 par défaut)\n\n\nstringsAsFactors\nFALSE\nLes chaînes de caractères sont-elles traitées comme des facteurs ?\n\n\nselect\nNULL\nSélectionner les colonnes à importer\n\n\ndrop\nNULL\nSélectionner les colonnes à ne pas importer\n\n\ncolClasses\n\nfread() essaie de deviner\nDéfinir le type des variables\n\n\nencoding\n\"unknown\"\nDéfinir l’encodage du fichier (\"UTF-8\" ou \"Latin-1\")\n\n\ndec\nLe point\nDéfinir le marqueur décimal\n\n\ndata.table\nTRUE\n\nfread renvoie un data.table si TRUE, un data.frame si FALSE\n\n\n\nshowProgress\nTRUE\nIndiquer la progression de l’importation\n\n\n\nQuelques remarques sur les options de fread() :\n\n\nfread() importe par défaut toutes les colonnes\n\nL’option select permet de sélectionner les colonnes, et doit être égale à un vecteur de noms. Exemple : select = c(\"var1\", \"var2\", \"var4\") ;\nInversement, l’option drop permet de préciser quelles colonnes ne seront pas importées. Les options select et drop ne peuvent pas être utilisées en même temps.\n\n\nfread() essaie par défaut de deviner le type des colonnes (integer pour les nombres entiers, character pour les chaînes de caractères…). L’option colClasses permet de choisir le type des colonnes, et doit égale à un vecteur dont chaque élément est de la forme nom_variable = \"type\". Exemple : colClasses = c(var1 = \"character\", var2 = \"logical\", var4 = \"double\") ;\nl’option stringsAsFactors = TRUE peut ralentir sensiblement l’importation des données ; il faut l’utiliser avec circonspection.\n\nExemple 1 : on veut à nouveau importer le fichier des communes du code officiel géographique, en déclarant que le fichier est encodé en UTF-8 et en imposant que le code commune (com) soit lu comme une chaîne de caractères et le code région (reg) comme un nombre entier. On écrit le code suivant :\n\n# Dans cet exemple, il faut remplacer \"mon_IDEP\" par votre IDEP\ncommunes &lt;- fread(\"Z:/mon_IDEP/communes-01012019.csv\",\n                  colClasses = c(com = \"character\",\n                                 reg = \"integer\"),\n                  encoding = \"UTF-8\")\n\nExemple 2 : on veut réaliser la même importation que précédemment, mais en sélectionnant uniquement le code commune (com), le nom de la commune (libelle) et la région (reg). On écrit le code suivant :\n\n# Dans cet exemple, il faut remplacer \"mon_IDEP\" par votre IDEP\ncommunes &lt;- fread(\"Z:/mon_IDEP/communes-01012019.csv\",\n                  select = c(\"com\", \"libelle\", \"reg\"),\n                  colClasses = c(com = \"character\",\n                                 reg = \"integer\"),\n                  encoding = \"UTF-8\")",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importer des fichiers plats (`.csv`, `.tsv`, `.txt`)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#comparaison-de-performances-sur-grands-fichiers",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#comparaison-de-performances-sur-grands-fichiers",
    "title": "12  Importer des fichiers plats (.csv, .tsv, .txt)",
    "section": "\n12.4 Comparaison de performances sur grands fichiers",
    "text": "12.4 Comparaison de performances sur grands fichiers\nAfin de comparer la rapidité de la fonction read_delim() du package readr avec la fonction fread() de data.table, on utilise dans cette partie le fichier des prénoms de l’Insee. Celui-ci contient les données sur les prénoms attribués aux enfants nés en France entre 1900 et 2021 par département de naissance.\nCe fichier de 78 Mo contient près de 3,8 millions de lignes et 5 variables.\nLe code suivant utilise le package microbenchmark qui fournit des fonctions pour mesurer et comparer avec précision le temps d’exécution d’instructions R. Pour en savoir plus, consulter cette page.\nUne comparaison est même réalisée avec la fonction read_delim_arrow du package arrow qui permet également d’importer des fichiers csv avec le lecteur CSV Arrow C++. Pour en savoir plus, consultez ce site.\n\n# Dans cet exemple, il faut remplacer \"mon_IDEP\" par votre IDEP\nlibrary(readr)\nlibrary(data.table)\nlibrary(arrow)\nlibrary(microbenchmark)\n\nchemin_fichier &lt;- \"Z:/mon_IDEP/dpt2021.csv\"\n\nmbm &lt;- microbenchmark(\"readr\" = {\n  base &lt;-\n    read_delim(\n      file = chemin_fichier\n    )\n},\n\n\"readr_lazy\" = {\n  base &lt;-\n    read_delim(\n      file = chemin_fichier,\n      lazy = TRUE\n    )\n},\n\n\"arrow\" = {\n  base &lt;-\n    read_delim_arrow(\n      file = chemin_fichier,\n      delim = \";\"\n    )\n},\n  \n\"data.table\" = {\n  base &lt;-\n    fread(\n      file = chemin_fichier\n    )\n  \n})\n\nLorsqu’on affiche le résultat, on se rend compte que la fonction fread() est la plus rapide. À l’opposé, read_delim() est la moins performante même si l’ajout de l’option lazy=TRUE permet de diviser le temps d’exécution par près de 3. Enfin, la fonction read_delim_arrow() du package arrow se rapproche fortement de la rapidité de fread().\n&gt; mbm\nUnit: milliseconds\n       expr       min       lq      mean    median        uq      max neval\n      readr    1546.9   1630.7    1673.4    1659.1    1685.0   1921.7    20\n readr_lazy     567.8    591.7     619.8     602.7     639.9    793.7    20\n      arrow     428.1    456.1     478.6     476.5     496.0    573.6    20\n data.table     284.9    300.5     326.9     311.3     326.4    480.9    20",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importer des fichiers plats (`.csv`, `.tsv`, `.txt`)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#quelques-bonnes-pratiques",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#quelques-bonnes-pratiques",
    "title": "12  Importer des fichiers plats (.csv, .tsv, .txt)",
    "section": "\n12.5 Quelques bonnes pratiques",
    "text": "12.5 Quelques bonnes pratiques\nVoici quelques bonnes pratiques à avoir en tête pour importer des données :\n\n\nVérifier que votre machine peut charger les données : R importe les données dans la mémoire vive de la machine. Si les fichiers que vous voulez importer sont d’une taille supérieure à celle de la mémoire vive, vous ne pourrez pas les importer intégralement.\n\nTester votre code d’importation avec quelques lignes : il faut souvent tâtonner pour bien importer des données. Il est donc recommandé de commencer par importer quelques centaines ou quelques milliers de lignes (en utilisant l’option n_max des fonctions du package readr ou nrows de fread()) pour vérifier que le code est correct.\nIl est important d’importer un nombre réduit de colonnes. Bien sélectionner les colonnes permet souvent de réduire significativement la taille des données et de résoudre le problème mentionné au point précédent. Pour cela, il s’agit d’utiliser l’option col_select pour la fonction read_delim() du package readr ou l’option select de la fonction fread() de data.table.\n\nVous pouvez choisir le package que vous utilisez en fonction des outils que vous voulez utiliser pour manipuler les données : les fonction read_delim() de readr renvoit un objet tibble tandis que fread() renvoie un objet data.table. Si vous prévoyez d’utiliser des packages du tidyverse (notamment tidyr et dplyr), il est préférable d’utiliser readr. Si vous prévoyez d’utiliser data.table, il est préférable d’utiliser fread().",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importer des fichiers plats (`.csv`, `.tsv`, `.txt`)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#RessourcesImportCSV",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_plats.html#RessourcesImportCSV",
    "title": "12  Importer des fichiers plats (.csv, .tsv, .txt)",
    "section": "\n12.6 Pour en savoir plus",
    "text": "12.6 Pour en savoir plus\n\n\nSur readr :\n\nla documentation du package (en anglais) ;\nune introduction à readr (en anglais) ;\nl’aide-mémoire du package ;\n\n\n\nsur data.table :\n\nla documentation du package (en anglais).",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importer des fichiers plats (`.csv`, `.tsv`, `.txt`)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tables_sas.html",
    "href": "03_Fiches_thematiques/Fiche_import_tables_sas.html",
    "title": "13  Importer des tables SAS®",
    "section": "",
    "text": "13.1 Tâches concernées et recommandations\nL’utilisateur souhaite importer dans R des données stockées sous forme de tables SAS.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Importer des tables SAS®</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tables_sas.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_import_tables_sas.html#tâches-concernées-et-recommandations",
    "title": "13  Importer des tables SAS®",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\nDeux méthodes sont recommandées pour importer des tables SAS avec R :\n\nméthode en une étape : utiliser la fonction read_sas() du package haven.\n\nméthode en deux étapes : Exporter les données SAS en format .csv, puis les importer en R.\n\nLes particularités du format des tables SAS peuvent être source de difficultés lorsqu’on veut les importer avec R. Il est fortement recommandé de tester la méthode d’importation choisie sur un petit échantillon de données avant d’importer les données, en particulier lorsque celles-ci sont volumineuses. Il est également recommandé d’essayer l’autre méthode si la première ne fonctionne pas correctement, ou si elle est trop lente.\nEn revanche, il est fortement déconseillé d’utiliser les packages suivants pour importer des données SAS : sas7dbat, foreign, Hmisc, SASxport.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Importer des tables SAS®</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tables_sas.html#première-méthode-le-package-haven",
    "href": "03_Fiches_thematiques/Fiche_import_tables_sas.html#première-méthode-le-package-haven",
    "title": "13  Importer des tables SAS®",
    "section": "\n13.2 Première méthode : le package haven\n",
    "text": "13.2 Première méthode : le package haven\n\nUtiliser le package haven est la méthode la plus simple pour importer des tables SAS avec R. Toutefois, cette méthode n’est pas entièrement fiable, et peut aboutir à des erreurs inattendues (notamment lorsque la table SAS est compressée en BINARY).\n\n13.2.1 Utiliser la fonction read_sas()\n\nLe package haven propose la fonction read_sas() pour importer des tables SAS. Voici un exemple simple (qui ne peut être exécuté que dans AUS) :\n\n# Charger le package haven\nlibrary(haven)\n# Importer une table SAS depuis GEN\ndfRP &lt;- haven::read_sas(\"W:/A1090/GEN_A1090990_DINDISAS/RPADUDIF.sas7bdat\")\n\nVoici les principaux arguments et options de read_sas() :\n\n\n\n\n\n\n\nArgument\nValeur par défaut\nFonction\n\n\n\ndata_file\nAucune\nLe chemin de la table SAS à importer\n\n\ncol_select\nToutes les variables\nSélectionner les variables (voir ci-dessous)\n\n\nskip\n0\nSauter les n premières lignes (0 par défaut)\n\n\nn_max\nInf\nNombre maximum de lignes à importer (pas de limite par défaut)\n\n\nencoding\nNULL\nPréciser l’encodage de la table SAS (normalement read_sas() le trouve automatiquement)\n\n\n\nLa fonction read_sas() importe par défaut toutes les colonnes du fichier. Pour sélectionner les colonnes à importer, on peut utiliser l’option col_select. Cette option peut s’utiliser de plusieurs façons :\n\nSous la forme d’une liste de noms de variables. Exemple : col_select = c(\"var1\", \"var3\", \"var4\").\nAvec des outils issus de dplyr pour sélectionner les variables selon leur nom (pour en savoir plus : ?dplyr::select). Exemple : col_select = starts_with(\"TYP\") permet de sélectionner toutes les variables dont le nom commence par “TYP”.\n\nVoici un exemple de code qui importe les 100 premières lignes en sélectionnant les colonnes :\n\n# Importer une table SAS depuis GEN\n# Cet exemple fonctionne uniquement dans AUS\ndfRP &lt;- \n    haven::read_sas(\n        data_file = \"W:/A1090/GEN_A1090990_DINDISAS/RPADUDIF.sas7bdat\",\n        n_max = 100,\n        col_select = starts_with(\"TYP\"))\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous souhaitez utiliser read_sas() pour importer des données volumineuses (par exemple plus de 1 ou 2 Go), il est recommandé de faire des tests avant d’importer l’ensemble des données. Un premier test consiste à importer les premières lignes de la table en utilisant l’option n_max, puis à vérifier que cet échantillon a été correctement importé.\n\n\nLa fonction read_sas() importe les noms de variables, mais aussi les étiquettes des variables (labels). Il est possible de récupérer les étiquettes de colonnes de la table importée dans un vecteur avec :\n\nsapply(dfRP, attr, \"label\")\n\n\n13.2.2 Résoudre le problème des tables SAS® compressées en BINARY\nJusqu’à une date récente, la fonction read_sas() ne pouvait pas importer les tables SAS compressées en mode BINARY. Cette fonction le peut désormais, depuis la version 2.4.0 du package haven. Il faut donc utiliser cette version de haven (ou une version plus récente) pour importer des tables SAS compressées en BINARY.\nSi vous rencontrez une erreur à l’importation d’une table SAS, vous pouvez vérifier que la table est compressée en BINARY : clic droit sur la table SAS &gt; Propriétés &gt; Onglet Détails.\n\nSi vous êtes dans cette situation, vous avez trois pistes de solutions :\n\nMettre à jour le package haven avec la fonction install.packages(\"haven\"), puis réessayer d’importer la table SAS ;\nRéenregistrer la table SAS en n’utilisant pas le format BINARY ;\nImporter les données avec R en procédant en deux temps (voir plus bas).",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Importer des tables SAS®</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tables_sas.html#seconde-méthode-procéder-en-deux-temps",
    "href": "03_Fiches_thematiques/Fiche_import_tables_sas.html#seconde-méthode-procéder-en-deux-temps",
    "title": "13  Importer des tables SAS®",
    "section": "\n13.3 Seconde méthode : procéder en deux temps",
    "text": "13.3 Seconde méthode : procéder en deux temps\nIl existe des situations dans lesquelles la fonction read_sas() de haven ne fonctionne pas (voir ci-dessus) ou se révèle peu performante. Voici donc une seconde méthode qui consiste à procéder en deux temps :\n\nExporter les données SAS en format .csv ;\nImporter en R les données .csv.\n\nCette méthode propose un haut niveau de fiabilité et de performance, et permet d”’importer les données avec R depuis un format très courant et non dépendant d’un logiciel. Elle a toutefois l’inconvénient de nécessiter un espace de stockage pour les données intermédiaires en format csv.\n\n\n\n\n\n\nNote\n\n\n\nContrairement à ce que vous pourriez penser, cette méthode en deux étapes n’est ni spécialement complexe ni particulièrement longue. Elle peut même être plus rapide que la méthode en une étape car la fonction read_sas() de haven est relativement peu performante lorsque les tables SAS sont volumineuses.\n\n\n\n13.3.1 Etape 1 : Exporter au format csv depuis SAS®\nIl y a deux façons de faire pour exporter une table SAS vers un format .csv :\n\n\nla solution simple exporte uniquement la table SAS (dans un fichier qu’on appellera data.csv), mais pas les étiquettes des variables ;\n\nla solution complète : si l’on souhaite disposer dans R des données et des étiquettes de variables, une bonne pratique consiste à exporter deux fichiers en .csv :\n\nle fichier des données lui-même (data.csv) ;\nun fichier contenant la liste des variables associées à leur label (labelvariables.csv).\n\nIl s’agit typiquement de la structure de données que les utilisateurs des données du Household Finance and Consumption Survey ont à leur disposition.\n\n\n13.3.1.1 Solution simple\nLa solution la plus simple consiste à exporter uniquement la table de données de SAS vers un fichier .csv. Voici un exemple de code SAS qui exporte une table SAS en .csv. L’option (keep = var1 var2 var3 var8) permet de choisir les variables qu’on exporte. Vous pouvez le copier-coller dans SAS, et l’adapter.\n/* Définir le répertoire d'exportation */\n%let versR = D:/le/dossier/pour/exporter/le/fichier/csv ;\n\n/* Définir le nom du fichier CSV et son encodage */\nfilename f \"&versR./data.csv\" encoding = \"utf8\" ;\n\n/* Exporter les données en sélectionnant des colonnes */\nPROC EXPORT DATA = ma_table_SAS (keep = var1 var2 var3 var8)\n            OUTFILE = f\n            DBMS = CSV REPLACE ;\n     PUTNAMES = YES ;\nRUN ;\n\n13.3.1.2 Solution complète\nVoici un exemple de code SAS qui exporte l’intégralité d’une table SAS sous forme de trois fichiers .csv : un pour les données stricto sensu, un pour les étiquettes des variables, et un pour les formats des variables. L’option (keep = var1 var2 var3 var8) permet de choisir les variables qu’on exporte. Vous pouvez le copier-coller dans SAS, et l’adapter.\n/* Définir le répertoire d'exportation */\n%let versR = D:/le/dossier/pour/exporter/le/fichier/csv ;\n\n/* Définir le nom du fichier CSV et son encodage */\nfilename f1 \"&versR./data.csv\" encoding = \"utf8\" ;\nfilename f2 \"&versR./labelvariables.csv\" encoding = \"utf8\" ;\n\n/* Exporter les données en sélectionnant des colonnes */\nPROC EXPORT DATA = ma_table_SAS (keep = var1 var2 var3 var8)\n            OUTFILE = f1\n            DBMS = CSV REPLACE ;\n     PUTNAMES = YES ;\nRUN ;\n\n/* Exporter les étiquettes des variables */\nPROC CONTENTS DATA = ma_table_SAS OUT = labelVar ;\nRUN ;\n\nPROC EXPORT DATA = labelVar (keep = name label)\n            OUTFILE = f2\n            DBMS = CSV REPLACE ;\n     PUTNAMES = NO ;\nRUN ;\n\n13.3.2 Etape 2 : Importer les données csv en R\n\nLes méthodes pour importer les données csv en R sont détaillées dans la fiche [Importer des fichiers plats (.csv, .tsv, .txt)]. Si vous avez utilisé la méthode complète, voici comment réutiliser les étiquettes de variables avec R :\n\nImporter le fichier data.csv sous le nom df_sas ;\nImporter le fichier labelvariables.csv sous le nom labels_sas ;\nAssocier les étiquettes avec les variables avec la commande for (i in seq(labels_sas)) attr(df_sas[[i]], \"label\") &lt;- labels_sas[i].",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Importer des tables SAS®</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tables_sas.html#RessourcesImportSAS",
    "href": "03_Fiches_thematiques/Fiche_import_tables_sas.html#RessourcesImportSAS",
    "title": "13  Importer des tables SAS®",
    "section": "\n13.4 Pour en savoir plus",
    "text": "13.4 Pour en savoir plus\n\nSur haven : la documentation du package (en anglais) ;\nSur les méthodes d’importation de fichiers plats en R : voir la fiche [Importer des fichiers plats (.csv, .tsv, .txt)].",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Importer des tables SAS®</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tableurs.html",
    "href": "03_Fiches_thematiques/Fiche_import_tableurs.html",
    "title": "14  Importer des fichiers issus de tableurs (Excel, Calc)",
    "section": "",
    "text": "14.1 Tâches concernées et recommandations\nL’utilisateur souhaite importer dans R des données issues de tableurs (extension type xls, xlsx ou ods).",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importer des fichiers issus de tableurs (Excel, Calc)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tableurs.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_import_tableurs.html#tâches-concernées-et-recommandations",
    "title": "14  Importer des fichiers issus de tableurs (Excel, Calc)",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nIl est recommandé d’utiliser la fonction read.xlsx() du package openxlsx pour importer des fichiers xlsx.\n\nIl est recommandé d’utiliser la fonction read_excel() du package readxl pour importer des fichiers xlsx ou xls. Pour les fichiers xlsx, la fonction read.xlsx() du package openxlsx peut également être utilisée même s’il est un peu moins performante que read_excel() sur les gros fichiers (voir ici).\nIl est recommandé d’utiliser la fonction read_ods du package readODS pour importer des fichiers ods.\n\nIl est déconseillé d’utiliser le package xlsx.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importer des fichiers issus de tableurs (Excel, Calc)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tableurs.html#importer-un-fichier-xlsx-ou-xls",
    "href": "03_Fiches_thematiques/Fiche_import_tableurs.html#importer-un-fichier-xlsx-ou-xls",
    "title": "14  Importer des fichiers issus de tableurs (Excel, Calc)",
    "section": "\n14.2 Importer un fichier xlsx ou xls\n",
    "text": "14.2 Importer un fichier xlsx ou xls\n\nL’importation de fichiers xlsx et xls va être illustré à partir de deux jeux de données créés à partir de données présentes sur le site de l’Insee, en formats xls et xlsx. Pour reproduire les exemples ci-dessous, vous devez :\n\ntélécharger les jeux de données ici puis le sauvegarder sur votre poste ;\ndéfinir dans R les chemins des fichiers nommés chemin_xls et chemin_xlsx. Voici un exemple :\n\n\n# Attention, vous devez adapter le chemin des fichiers à votre environnement de travail\nchemin_xls  &lt;- \"C:/Users/mon_IDEP_Insee/Dossier_utilitR/mes_donnees/mes_donnees.xls\"  \nchemin_xlsx &lt;- \"C:/Users/mon_IDEP_Insee/Dossier_utilitR/mes_donnees/mes_donnees.xlsx\"\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous êtes complètement débutants en R, il est recommandé d’utiliser l’utilitaire d’importation de RStudio présentée ci-dessous. Une fois que les données sont correctement importées, vous pourrez copier-coller le code dans votre script R et vous familiariser avec les fonctions des packages openxlsx et readxl.\n\n\n\n14.2.1 Importer un fichier xlsx ou xls avec l’interface graphique de RStudio\nRStudio propose une interface graphique très commode pour lire des fichiers xls et xlsx (mais aussi des tables SAS ou des fichiers csv, mais pas les ods), reposant sur les packages haven et readxl. Le grand intérêt de cette interface est qu’elle fournit le code utilisé pour importer les données. Vous pouvez donc le copier dans vos scripts pour le réutiliser, et ainsi vous familiariser avec les fonctions d’importation.\nOn accède à cette interface avec : File &gt; Import Dataset &gt; From Excel.... Les différents menus permettent notamment de sélectionner l’onglet et la zone à importer et de nommer la table d’affectation.\nDans ce premier exemple, on importe l’onglet par défaut (Sheet1), de la table mes_donnees.xlsx (File/Url) qu’on nomme mes_donnees (Name). On garde la première ligne du fichier comme noms de colonnes (First Row as Names). Vous pouvez voir que le code d’importation apparaît en bas à droite, dans la cellule Code Preview.\n\n\nInterface d’importation de RStudio : Exemple 1\n\nDans l’exemple suivant, on n’importe qu’une plage de données (A1:D5) de l’onglet nommé Sheet3 (Sheet), et on remplace les valeurs manquantes par 1904.\n\n\nInterface d’importation de RStudio : Exemple 2\n\n\n14.2.2 Importer un fichier xlsx avec le package openxlsx\n\nPour importer un fichier au format xlsx, la fonction read.xlsx() du package openxlsx peut être utilisée pour des fichiers de tailles raisonnables. Cette fonction permet de charger les données du tableur dans un data.frame. Il ne faut pas oublier de charger le package avec library.\n\nlibrary(openxlsx)\n\n\n14.2.2.1 Comment utiliser la fonction read.xlsx()\n\nVoici les principaux arguments et options de read.xlsx() :\n\n\n\n\n\n\n\nArgument\nValeur par défaut\nFonction\n\n\n\nxlsxFile\nAucune\nChemin d’accès vers un objet classeur ou une url vers un fichier xlsx à importer\n\n\nsheet\n1\nOnglet à importer. Soit le nom de l’onglet, soit un numéro de l’onglet\n\n\nstartRow\n1\nLigne à partir de laquelle les données sont importées. Les lignes vides en haut d’un fichier sont toujours ignorées, quelle que soit la valeur de startRow\n\n\n\ncolNames\nTRUE\nSi TRUE, la première ligne de données sera utilisée comme nom de colonnes\n\n\nrowNames\nFALSE\nSi TRUE, la première colonne de données sera utilisée comme noms de lignes\n\n\ndetectDates\nFALSE\nSi TRUE, R essaiera de reconnaître les dates et d’effectuer la conversion\n\n\nskipEmptyRows\nTRUE\nSi TRUE, les lignes vides sont ignorées, sinon les lignes vides après la première ligne contenant les données renverront une ligne de NA\n\n\n\nskipEmptyCols\nFALSE\nSi TRUE, les colonnes vides sont ignorées\n\n\ncheck.names\nFALSE\nSi TRUE, les noms des variables dans la trame de données sont vérifiés et modifiés pour s’assurer qu’ils sont des noms de variables valides\n\n\nsep.names\n“.”\nUn caractère qui remplace les blancs dans les noms de colonne\n\n\n\n14.2.2.2 Quelques exemples\nLes exemples qui suivent vous présentent l’utilisation de la fonction read.xlsx dans quelques cas courants.\n\n\nUtilisation la plus simple : on importe toutes les données du premier onglet, en supposant que la première ligne contient les noms de variables.\n\n\nmesDonnees &lt;- openxlsx::read.xlsx(xlsxFile = chemin_xlsx)\nhead(mesDonnees, 3)\n\n  Sheet1 Année Population.en.milieu.d'année Mariages Divorces.prononcés.(a)\n1      1  1901                     40710000   316540                   9000\n2      2  1902                     40810000   306682                   9600\n3      3  1903                     40910000   308510                  10400\n  Naissances.vivantes Décès.tous.âges.(b) Décès.de.moins.d'un.an\n1              917075              825315                 137310\n2              904434              801379                 130086\n3              884498              794566                 129247\n  Excédent.naturel Nuptialité.(c) Natalité Mortalité Accroissement.naturel\n1            91760            7.8     22.5      20.3                   2.2\n2           103055            7.5     22.2      19.6                   2.6\n3            89932            7.5     21.6      19.4                   2.2\n  Taux.de.mortalité.infantile.pour.1.000.naissances.vivantes       date\n1                                                      151.1 2020-06-22\n2                                                      143.3 2020-06-22\n3                                                      145.3 2020-06-22\n\n\n\n\nDéfinir l’onglet à importer : on importe toutes les données du troisième onglet, en supposant que la première ligne contient les noms de variables. On définit l’onglet à importer avec le paramètre sheet en précisant soit le nom de l’onglet soit son index (sa position dans le fichier). Il est conseillé d’utiliser le nom de l’onglet plutôt que sa position.\n\n\n# Chargement du 3ème onglet\nmesDonnees &lt;- openxlsx::read.xlsx(xlsxFile = chemin_xlsx, sheet = \"Sheet3\")\n\n\n\n\n\n\n\nNote\n\n\n\nLa fonction openxlsx::getSheetNames() permet de récupérer les noms des onglets du fichier sans avoir à l’ouvrir.\n\nopenxlsx::getSheetNames(chemin_xlsx)\n\n[1] \"Sheet1\" \"Sheet2\" \"Sheet3\"\n\n\n\n\n\n\nPréciser où débute la sélection : par défaut, on importe les données à partir de la première ligne de l’onglet spécifié. On peut, avec le paramètre startRow, définir la ligne de début d’importation. Dans l’exemple ci-dessous, on n’importe les données qu’à compter de la troisième ligne. Si on laisse le paramètre colNames = TRUE, la première de ces lignes est considérée comme noms de colonnes, ce qui peut donner des résultats absurdes. Si c’est le cas, on peut utiliser le paramètre colNames=FALSE.\n\n\nmesDonnees &lt;- openxlsx::read.xlsx(xlsxFile = chemin_xlsx, startRow=3, colNames=FALSE)\n# Les 4 premières lignes du data.frame\nhead(mesDonnees, 4)\n\n  X1   X2       X3     X4    X5     X6     X7     X8     X9 X10  X11  X12 X13\n1  2 1902 40810000 306682  9600 904434 801379 130086 103055 7.5 22.2 19.6 2.6\n2  3 1903 40910000 308510 10400 884498 794566 129247  89932 7.5 21.6 19.4 2.2\n3  4 1904 41000000 312134 11100 877091 802536 134437  74555 7.6 21.4 19.6 1.8\n4  5 1905 41050000 316195 11100 865604 812338 125533  53266 7.7 21.1 19.8 1.3\n    X14        X15\n1 143.3 2020-06-22\n2 145.3 2020-06-22\n3 152.9 2020-06-22\n4 144.5 2020-06-22\n\n\n\n\nSélectionner les lignes et colonnes à importer : on peut définir les lignes et colonnes qu’on souhaite importer en le précisant, avec un vecteur numérique, avec les paramètres rows et cols.\n\n\nmesDonnees &lt;- openxlsx::read.xlsx(xlsxFile = chemin_xlsx, rows=c(1,4:6,9), cols=c(1,3:4))\nhead(mesDonnees, 4)\n\n  Sheet1 Population.en.milieu.d'année Mariages\n1      3                     40910000   308510\n2      4                     41000000   312134\n3      5                     41050000   316195\n4      8                     41190000   328877\n\n\n\n\nVérification du respect des normes syntaxiques dans les noms de variables : les noms de colonnes dans un fichier Excel ne peuvent pas toujours être utilisés directement comme noms de variables dans R. Les paramètres check.names et sep.names permettent de modifier les noms de variables pour les adapter aux règles de bonnes pratiques syntaxiques :\n\nLe paramètre check.names=TRUE modifie les noms de variables qui posent problème. Par exemple, nom-de-variable dans Excel devient nom.de.variable dans R.\nLe paramètre sep.names permet de définir le caractère par lequel remplacer les espaces.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPour l’exportation de données au format xlsx, le package openxlsx est à privilégier car il présente de multiples options très pratiques pour personnaliser les exports. Les deux vignettes du package sur ce sujet apportent quelques exemples des potentialités d’écriture de classeurs xlsx. La première présente notamment l’utilisation de la fonction write.xlsx() et la seconde illustre quelques possibilités autour de la fonction writeData().\n\n\n\n14.2.3 Importer un fichier xls avec le package readxl\n\nPour importer un fichier au format xls ou xlsx, il est recommandé d’utiliser la fonction read_excel() du package readxl. Cette fonction permet en effet d’importer des fichiers volumineux de manière plus rapide que le package openxlsx. Les données du tableur sont alors chargées dans un tibble (voir la fiche [Manipuler des données avec le tidyverse] pour en apprendre davantage sur le tibble). Il ne faut pas oublier de charger le package avec library.\n\nlibrary(readxl)\n\n\n14.2.3.1 Comment utiliser la fonction read_excel()\n\nVoici les principaux arguments et options de read_excel() :\n\n\n\n\n\n\n\nArgument\nValeur par défaut\nFonction\n\n\n\npath\nAucune\nChemin d’accès au fichier xls / xlsx à importer\n\n\nsheet\nNULL\nOnglet à importer Soit le nom de l’onglet, soit la position de l’onglet. Par défaut, sélectionne le premier onglet du fichier\n\n\nrange\nNULL\nUne plage de cellules à lire. Accepte des plages Excel typiques comme \"B3:D87\"et des plages avec le nom de l’onglet comme \"Budget! B2:G14\"\n\n\n\ncol_names\nTRUE\n\nTRUE pour utiliser la première ligne comme noms de colonne, FALSE pour obtenir les noms par défaut ou un vecteur de caractères donnant un nom à chaque colonne\n\n\ncol_types\nNULL\nPréciser le type des colonnes. Si col_types = NULL, readxl essaie de deviner le type des colonnes. Voir ?readxl::read_xls pour l’usage de cette option\n\n\nna\n“”\nVecteur de chaînes de caractères à interpréter comme des valeurs manquantes\n\n\nskip\n0\nNombre de lignes à ignorer avant d’importer les données\n\n\nn_max\nInf\nNombre maximum de lignes de données à lire\n\n\nguess_max\n\n1000 ou n_max\n\nNombre de lignes utilisées pour deviner les types de colonnes\n\n\n\n14.2.3.2 Quelques exemples\nLes exemples qui suivent vous présentent l’utilisation de la fonction read_excel dans quelques cas courants.\n\n\nUtilisation la plus simple : on importe toutes les données du premier onglet, en supposant que la première ligne contient les noms de variables.\n\n\nmesDonnees &lt;- readxl::read_excel(path = chemin_xls)\nhead(mesDonnees, 3)\n\n# A tibble: 3 × 15\n  Sheet1 Année `Population.en.milieu.d'année` Mariages `Divorces.prononcés.(a)`\n  &lt;chr&gt;  &lt;chr&gt;                          &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                   \n1 1      1901                        40710000   316540 9000                    \n2 2      1902                        40810000   306682 9600                    \n3 3      1903                        40910000   308510 10400                   \n# ℹ 10 more variables: Naissances.vivantes &lt;dbl&gt;, `Décès.tous.âges.(b)` &lt;dbl&gt;,\n#   `Décès.de.moins.d'un.an` &lt;dbl&gt;, Excédent.naturel &lt;dbl&gt;,\n#   `Nuptialité.(c)` &lt;dbl&gt;, Natalité &lt;dbl&gt;, Mortalité &lt;dbl&gt;,\n#   Accroissement.naturel &lt;dbl&gt;,\n#   Taux.de.mortalité.infantile.pour.1.000.naissances.vivantes &lt;dbl&gt;,\n#   date &lt;chr&gt;\n\n\n\n\nDéfinir l’onglet à importer : on importe toutes les données du troisième onglet, en supposant que la première ligne contient les noms de variables. On définit l’onglet à importer avec le paramètre sheet en précisant soit le nom de l’onglet soit son index (sa position dans le fichier). Il est conseillé d’utiliser le nom de l’onglet plutôt que sa position.\n\n\n# Chargement du 3ème onglet\nmesDonnees &lt;- readxl::read_excel(path = chemin_xls, sheet = \"Sheet3\")\n\n\n\n\n\n\n\nNote\n\n\n\nLa fonction readxl::excel_sheets() permet de récupérer les noms des onglets du fichier sans avoir à l’ouvrir.\n\nreadxl::excel_sheets(chemin_xls)\n\n[1] \"Sheet1\" \"Sheet2\" \"Sheet3\"\n\n\n\n\n\n\nImporter une zone spécifique du fichier : il est possible de n’importer qu’une plage de cellules en la définissant dans l’argument range. On peut également préciser l’onglet concerné, en écrivant la zone sous la forme \"Sheet3!B2:D7\". La première ligne de la plage est considérée comme en-tête de colonnes. Si ça n’est pas le cas, il faut ajouter le paramètre col_names=FALSE pour que la première ligne soit traitée comme une ligne de données.\n\n\nmesDonnees &lt;- readxl::read_excel(path = chemin_xls, range = \"Sheet3!B2:D7\")\nhead(mesDonnees, 3)\n\n# A tibble: 3 × 3\n  `1901` `40710000` `316540`\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 1902     40810000   306682\n2 1903     40910000   308510\n3 1904     41000000   312134\n\n\n\n\nDéfinir le type des colonnes : le paramètre col_types permet de définir explicitement le type des colonnes et d’ignorer les colonnes qu’on ne souhaite pas importer. Pour cela, on passe au paramètre col_types un vecteur précisant le type parmi les possibilités suivantes :\n\n\n\"skip\" : ignorer la colonne (qui ne sera pas importée) ;\n\n\"guess\" : le type de la variable est devinée par rapport à ses modalités ;\n\n\"list\" : crée une liste ;\n\n\"logical pour une variable booléenne, \"numeric\" pour une variable numérique, \"date\" pour une date et \"text\" pour une variable caractère.\n\nLe type de la variable sera appliqué aux colonnes dans l’ordre défini par le vecteur. Exemple : c(\"text\",\"text\",\"numeric\",\"guess\",\"skip\",\"logical\"). Dans le cas où on souhaite définir le même type pour toutes les colonnes, il suffit de préciser une seule fois le type attendu (exemple col_types = \"text\").\n\n\nmesDonnees &lt;- readxl::read_excel(path = chemin_xls, col_types = c(\"text\",\"list\",rep(\"skip\",9), \"text\", \"numeric\", \"text\", \"guess\"))\nhead(mesDonnees)\n\n# A tibble: 6 × 6\n  Sheet1 Année     Mortalité  Accroissement.naturel Taux.de.mortalité.in…¹ date \n  &lt;chr&gt;  &lt;list&gt;    &lt;chr&gt;                      &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;\n1 1      &lt;chr [1]&gt; 20.300000…                   2.2 151.09999999999999     2020…\n2 2      &lt;chr [1]&gt; 19.600000…                   2.6 143.30000000000001     2020…\n3 3      &lt;chr [1]&gt; 19.399999…                   2.2 145.30000000000001     2020…\n4 4      &lt;chr [1]&gt; 19.600000…                   1.8 152.90000000000001     2020…\n5 5      &lt;chr [1]&gt; 19.800000…                   1.3 144.5                  2020…\n6 6      &lt;chr [1]&gt; 20                           1   151.5                  2020…\n# ℹ abbreviated name:\n#   ¹​Taux.de.mortalité.infantile.pour.1.000.naissances.vivantes\n\n\n\n\nGestion des NA  : il est possible de préciser les valeurs qu’on souhaite considérer comme des NA avec le paramètre na. Ici, pour l’exercice, la valeur 1902 est considérée comme NA.\n\n\nmesDonnees &lt;- readxl::read_excel(path = chemin_xls, na=\"1902\")\nhead(mesDonnees)\n\n# A tibble: 6 × 15\n  Sheet1 Année `Population.en.milieu.d'année` Mariages `Divorces.prononcés.(a)`\n  &lt;chr&gt;  &lt;chr&gt;                          &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                   \n1 1      1901                        40710000   316540 9000                    \n2 2      &lt;NA&gt;                        40810000   306682 9600                    \n3 3      1903                        40910000   308510 10400                   \n4 4      1904                        41000000   312134 11100                   \n5 5      1905                        41050000   316195 11100                   \n6 6      1906                        41100000   320208 11900                   \n# ℹ 10 more variables: Naissances.vivantes &lt;dbl&gt;, `Décès.tous.âges.(b)` &lt;dbl&gt;,\n#   `Décès.de.moins.d'un.an` &lt;dbl&gt;, Excédent.naturel &lt;dbl&gt;,\n#   `Nuptialité.(c)` &lt;dbl&gt;, Natalité &lt;dbl&gt;, Mortalité &lt;dbl&gt;,\n#   Accroissement.naturel &lt;dbl&gt;,\n#   Taux.de.mortalité.infantile.pour.1.000.naissances.vivantes &lt;dbl&gt;,\n#   date &lt;chr&gt;\n\n\n\n\nNe pas importer les x premières lignes : le paramètre skip permet de définir le nombre de lignes à ignorer avant de commencer l’importation des données. Si on laisse le paramètre col_names = TRUE, la première des lignes importées est considérée comme noms de colonnes, ce qui peut donner des résultats absurdes. On peut alors utiliser le paramètre col_names=FALSE pour qu’elle ne soit pas considérée comme nom de variables.\n\n\nmesDonnees &lt;- readxl::read_excel(path = chemin_xls, skip = 5, col_names = FALSE)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n\nhead(mesDonnees, 4)\n\n# A tibble: 4 × 15\n  ...1  ...2      ...3   ...4 ...5    ...6   ...7   ...8  ...9 ...10 ...11 ...12\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 5     1905  41050000 316195 11100 865604 812338 125533 53266   7.7  21.1  19.8\n2 6     1906  41100000 320208 11900 864745 820051 131058 44694   7.8  21    20  \n3 7     1907  41100000 327723 12900 829632 830871 115501 -1239   8    20.2  20.2\n4 8     1908  41190000 328877 13600 848982 784415 115556 64567   8    20.6  19  \n# ℹ 3 more variables: ...13 &lt;dbl&gt;, ...14 &lt;dbl&gt;, ...15 &lt;chr&gt;",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importer des fichiers issus de tableurs (Excel, Calc)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tableurs.html#importer-un-fichier-ods-avec-le-package-readods",
    "href": "03_Fiches_thematiques/Fiche_import_tableurs.html#importer-un-fichier-ods-avec-le-package-readods",
    "title": "14  Importer des fichiers issus de tableurs (Excel, Calc)",
    "section": "\n14.3 Importer un fichier ods avec le package readODS\n",
    "text": "14.3 Importer un fichier ods avec le package readODS\n\n\n14.3.1 Introduction\nLe package readODS propose deux fonctions d’importation de fichiers ods : read_ods et read.ods. La documentation du package recommande d’utiliser read_ods. L’usage de cette fonction va être illustré à partir d’un jeu de données créé à partir de données présentes sur le site de l’Insee. Pour reproduire les exemples ci-dessous, vous devez :\n\ntélécharger le jeu de données en format ods ici : https://github.com/InseeFrLab/utilitR &gt; import_donnees_tabulees_tests puis le sauvegarder sur votre poste ;\ndéfinir dans R le chemin du fichier nommé chemin_ods. Voici un exemple :\n\n\n# Attention, cet exemple doit être adapté à votre environnement de travail\nchemin_ods &lt;- \"C:/Users/mon_IDEP_Insee/Dossier_utilitR/mes_donnees/mes_donnees.ods\"\n\nIl ne faut pas oublier de charger le package avec library.\n\nlibrary(readODS)\n\n\n14.3.2 Comment utiliser la fonction read_ods()\n\nVoici les principaux arguments et options de read_ods() :\n\n\n\n\n\n\n\nArgument\nValeur par défaut\nFonction\n\n\n\npath\nAucune\nLe chemin du fichier ods à importer\n\n\nsheet\n1\nOnglet à importer. Soit le nom de l’onglet, soit le numéro de l’onglet (utiliser de préférence le nom de l’onglet)\n\n\ncol_names\nTRUE\nIndique si la première ligne de l’onglet contient les noms des variables\n\n\ncol_types\nNULL\n\nNULL pour laisser R deviner le type des variables à partir de l’onglet ou se reporter à readr::type_convert pour spécifier le type des variables\n\n\nna\n\"\"\nVecteur donnant les chaîne de caractères interprétées comme des valeurs manquantes. Par défaut, read_ods convertit les cellules vides en données manquantes\n\n\nskip\n0\nLe nombre de lignes du fichier de données à ignorer avant de commencer à importer les données\n\n\nrange\nNULL\nSélection d’un rectangle à l’aide d’une plage de cellules de type Excel, comme range = \"D12:F15\".\n\n\n\n14.3.3 Quelques exemples\nLes exemples qui suivent vous présentent l’utilisation de la fonction read_ods dans quelques cas courants.\n\n\nUtilisation la plus simple : on importe toutes les données du premier onglet, en supposant que la première ligne contient les noms de variables.\n\n\n# Chargement du 2ème onglet\nmesDonnees &lt;- readODS::read_ods(path = chemin_ods)\nhead(mesDonnees, 3)\n\n# A tibble: 3 × 15\n  Sheet1 Année `Population.en.milieu.d'année` Mariages `Divorces.prononcés.(a)`\n   &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                   \n1      1 1901                        40710000   316540 9000                    \n2      2 1902                        40810000   306682 9600                    \n3      3 1903                        40910000   308510 10400                   \n# ℹ 10 more variables: Naissances.vivantes &lt;dbl&gt;, `Décès.tous.âges.(b)` &lt;dbl&gt;,\n#   `Décès.de.moins.d'un.an` &lt;dbl&gt;, Excédent.naturel &lt;dbl&gt;,\n#   `Nuptialité.(c)` &lt;dbl&gt;, Natalité &lt;dbl&gt;, Mortalité &lt;dbl&gt;,\n#   Accroissement.naturel &lt;dbl&gt;,\n#   Taux.de.mortalité.infantile.pour.1.000.naissances.vivantes &lt;dbl&gt;,\n#   date &lt;date&gt;\n\n\n\n\nDéfinir l’onglet à importer : on importe toutes les données du troisième onglet, en supposant que la première ligne contient les noms de variables. On définit l’onglet à importer avec le paramètre sheet en précisant soit le nom de l’onglet soit son index (sa position dans le fichier). Il est conseillé d’utiliser le nom de l’onglet plutôt que sa position.\n\n\n# Chargement du 3ème onglet\nmesDonnees &lt;- readODS::read_ods(path = chemin_ods, sheet = \"Sheet3\")\n\n\n\n\n\n\n\nNote\n\n\n\nLa fonction readODS::list_ods_sheets() permet de récupérer les noms des onglets du fichier sans avoir à l’ouvrir.\n\nreadODS::list_ods_sheets(chemin_ods)\n\n[1] \"Sheet1\" \"Sheet2\" \"Sheet3\"\n\n\n\n\n\n\nNe pas importer les n premières lignes  : le paramètre skip permet de préciser à compter de quelle ligne commencer l’importation. Dans l’exemple ci-dessous, on n’importe les données qu’à compter de la cinquième ligne. Si on laisse le paramètre col_names = TRUE, la première de ces lignes est considérée comme noms de colonnes, ce qui peut donner des résultats absurdes. Si c’est le cas, on peut utiliser le paramètre col_names = FALSE.\n\n\nmesDonnees &lt;- readODS::read_ods(path = chemin_ods, skip = 5, col_names = FALSE)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n\n#  Nom des colonnes\nhead(mesDonnees, 3)\n\n# A tibble: 3 × 15\n   ...1 ...2      ...3   ...4 ...5    ...6   ...7   ...8  ...9 ...10 ...11 ...12\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     5 1905  41050000 316195 11100 865604 812338 125533 53266   7.7  21.1  19.8\n2     6 1906  41100000 320208 11900 864745 820051 131058 44694   7.8  21    20  \n3     7 1907  41100000 327723 12900 829632 830871 115501 -1239   8    20.2  20.2\n# ℹ 3 more variables: ...13 &lt;dbl&gt;, ...14 &lt;dbl&gt;, ...15 &lt;date&gt;\n\n\n\n\nImporter une zone spécifique du fichier : il est possible de n’importer qu’une plage de cellules en la définissant dans l’argument range. On peut également préciser l’onglet concerné, en écrivant la zone sous la forme \"Sheet1!B2:D7\".\n\n\nmesDonnees &lt;- readODS::read_ods(path = chemin_ods, range = \"Sheet1!B2:D7\")\nhead(mesDonnees, 3)\n\n# A tibble: 3 × 3\n  `1901` `40710000` `316540`\n   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1   1902   40810000   306682\n2   1903   40910000   308510\n3   1904   41000000   312134",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importer des fichiers issus de tableurs (Excel, Calc)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_tableurs.html#RessourcesImportXLSX",
    "href": "03_Fiches_thematiques/Fiche_import_tableurs.html#RessourcesImportXLSX",
    "title": "14  Importer des fichiers issus de tableurs (Excel, Calc)",
    "section": "\n14.4 Pour en savoir plus",
    "text": "14.4 Pour en savoir plus\n\n\nPackage openxlsx ;\n\nPackage readxl ;\n\nPackage readODS ;\n\nvignette readr.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Importer des fichiers issus de tableurs (Excel, Calc)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html",
    "title": "15  Lire et écrire des fichiers Parquet",
    "section": "",
    "text": "15.1 Tâches concernées et recommandations\nNote: cette fiche n’a pas vocation à être exhaustive sur le format Parquet, mais plutôt à lister les points saillants à retenir lorsqu’un statisticien souhaite travailler avec ce format de fichier.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lire et écrire des fichiers Parquet</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#tâches-concernées-et-recommandations",
    "title": "15  Lire et écrire des fichiers Parquet",
    "section": "",
    "text": "L’utilisateur souhaite importer et exploiter dans R des données stockées au format Parquet.\nL’utilisateur souhaite convertir des données au format Parquet.\n\n\n\n\n\n\n\nTâche concernée et recommandation\n\n\n\n\nIl est recommandé d’utiliser le format Parquet pour stocker des données volumineuses, car il est plus compact que le format csv. Le package arrow permet de lire, d’écrire simplement les fichiers au format Parquet avec R;\n\nDeux approches sont recommandées pour manipuler des données volumineuses stockées en format Parquet:\n\nles packages arrow et dplyr si vous maîtrisez la syntaxe tidyverse;\nles packages DBI et duckdb si vous maîtrisez le langage SQL;\n\n\nIl est essentiel de travailler avec la dernière version d’arrow, de duckdb et de R car les packages arrow et duckdb sont en cours de développement;\nIl est préférable d’utiliser la fonction open_dataset pour accéder à des données stockées en format Parquet (plutôt que la fonction read_parquet);\nIl est recommandé de partitionner les fichiers Parquet lorsque les données sont volumineuses et lorsque les données peuvent être partitionnées selon une variable cohérente avec l’usage des données (département, secteur, année…);\nLorsqu’on importe des données volumineuses, il est recommandé de sélectionner les observations (avec filter) et les variables (avec select) pour limiter la consommation de mémoire vive.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lire et écrire des fichiers Parquet</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#quest-ce-que-parquet-et-pourquoi-sen-servir",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#quest-ce-que-parquet-et-pourquoi-sen-servir",
    "title": "15  Lire et écrire des fichiers Parquet",
    "section": "\n15.2 Qu’est-ce que Parquet et pourquoi s’en servir?",
    "text": "15.2 Qu’est-ce que Parquet et pourquoi s’en servir?\n\n15.2.1 Qu’est-ce que le format Parquet?\nParquet est un format de stockage de données, au même titre que les fichiers CSV, RDS, FST… Ce format n’est pas nouveau (création en 2013), mais il a gagné en popularité dans le monde de la data science au cours des dernières années, notamment grâce au projet open-source Apache arrow.\nLe format Parquet présente plusieurs avantages cruciaux qui en font un concurrent direct du format csv:\n\nil compresse efficacement les données, ce qui le rend très adapté au stockage de données volumineuses;\nil est conçu pour être indépendant d’un logiciel: on peut lire des fichiers Parquet avec R, Python, C++, JavaScript, Java…\nil est conçu pour que les données puissent être chargées très rapidement en mémoire.\n\n15.2.2 Caractéristiques du format Parquet\nLe format Parquet présente trois caractéristiques importantes du point de l’utilisateur:\n\nParquet stocke les données en un format binaire. Cela signifie qu’un fichier Parquet n’est pas lisible par un humain: contrairement au format csv, on ne peut pas ouvrir un fichier Parquet avec Excel, LibreOffice ou Notepad pour jeter un coup d’oeil au contenu.\nParquet repose sur un stockage orienté colonne. Ainsi seront stockées dans un premier temps toutes les données de la première colonne de la table, puis seulement dans un second temps les données de la deuxième colonne et ainsi de suite… Le blog d’upsolver fournit une illustration pour bien visualiser la différence :\n\n\n\n\n\nDifférence entre le stockage orienté ligne et colonne\n\n\n\n\n\nUn fichier Parquet contient à la fois les données et des métadonnées. Ces métadonnées écrites à la fin du fichier enregistrent une description du fichier (appelé schéma). Ces métadonnées contiennent notamment le type de chaque colonne (entier/réel/caractère) et quelques statistiques (min, max). Ce sont ces métadonnées qui font en sorte que la lecture des données Parquet soit optimisée et sans risque d’altération (voir ici pour en savoir plus).\n\nUn fichier Parquet est composé de groupe de lignes (row group) contenant également des métadonnées similaires à celles du fichier. La taille idéale d’un row group est de l’ordre de 30 000 à 1 000 000.\n\nDans un contexte analytique, cette organisation des données génère plusieurs avantages dont les principaux sont:\n\n\nUn gain de vitesse lors de la lecture des données pour un usage statistique: R peut extraire directement les colonnes demandées sans avoir à scanner toutes les lignes comme ce serait le cas avec un fichier csv ;\n\nLa possibilité d’avoir un haut niveau de compression. Le taux de compression moyen par rapport au format csv est souvent compris entre 5 et 10. Pour des fichiers volumineux il est même possible d’avoir des taux de compression bien supérieurs.\n\nInversement, le format Parquet présente deux contraintes inhabituelles pour les utilisateurs des autres formats (CSV, SAS, FST…):\n\nIl n’est pas possible d’importer uniquement les 100 premières lignes d’un fichier Parquet (comme on peut facilement le faire pour un fichier CSV); en revanche, il est possible d’afficher les 100 premières lignes d’un fichier Parquet avec la commande: open_dataset(mon_fichier_parquet) %&gt;% head(100);\nIl n’est pas possible d’ouvrir un fichier Parquet avec Excel, LibreOffice ou Notepad.\n\nPour en savoir plus notamment sur la comparaison entre les formats Parquet et csv, consultez le chapitre sur le sujet dans le cours de l’ENSAE “Python pour la data science”. Grâce aux travaux du projet Arrow, les fichiers aux format Parquet sont inter-opérables c’est-à-dire qu’ils peuvent être lus par plusieurs langages informatiques : C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, Ruby, Rust et bien entendu R. Le format Parquet est donc particulièrement adapté aux chaînes de traitement qui font appel à plusieurs langages (exemples: manipulation de données avec R puis machine learning avec Python).\nS’il est très efficace pour l’analyse de données, Parquet est en revanche peu adapté à l’ajout de données en continu ou à la modification fréquente de données existantes.\nPour cette utilisation, le statisticien privilégiera un système de gestion de base de données comme par exemple PostgreSQL.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lire et écrire des fichiers Parquet</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#écrire-des-fichiers-parquet",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#écrire-des-fichiers-parquet",
    "title": "15  Lire et écrire des fichiers Parquet",
    "section": "\n15.3 Écrire des fichiers Parquet",
    "text": "15.3 Écrire des fichiers Parquet\n\n15.3.1 Données peu volumineuses: écrire un seul fichier Parquet\nLes tables Parquet sont encore loin d’être majoritaires dans les liens de téléchargement notamment face au format csv. C’est la raison pour laquelle, nous allons dans cette section dérouler le processus pour obtenir un fichier Parquet à partir d’un fichier csv.\nDans un premier temps, on importe le fichier plat avec la fonction fread() du package data.table, conformément aux recommandations de la fiche sur les imports de fichiers plats. On obtient un objet data.table en mémoire. Dans un second temps, on exporte ces données en format Parquet avec la fonction write_parquet() du package arrow.\n\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(arrow)\n\n# Création du dossier \"Data_parquet\"\ndir.create(\"Data_parquet\")\n\n# Téléchargement du fichier zip\ndownload.file(\"https://www.insee.fr/fr/statistiques/fichier/2540004/dpt2021_csv.zip\",\n              destfile = \"Data_parquet/dpt2021_csv.zip\")\n\n# Décompression du fichier zip\nunzip(\"Data_parquet/dpt2021_csv.zip\", exdir = \"Data_parquet\")\n\n# Lecture du fichier CSV\ndpt2021 &lt;- fread(\"Data_parquet/dpt2021.csv\")\n\n# Écriture des données en format Parquet\nwrite_parquet(\n  x = dpt2021,\n  sink = \"Data_parquet/dpt2021.parquet\"\n)\n\nÀ l’issue de cette conversion, on peut noter que le fichier Parquet créé occupe un espace de stockage 10 fois moins important que le fichier csv initial (7,4 Mo contre 76,3 Mo) !\nPour les exemples qui suivent dans cette fiche, on utilise un fichier de la Base Permanente des Équipements de l’Insee que l’on va convertir au format Parquet.\nVous pouvez télécharger ce fichier avec le package doremifasol et plus particulièrement la fonction telechargerDonnees() :\n\n# remotes::install_github(\"InseeFrLab/doremifasol\", build_vignettes = TRUE)\nlibrary(doremifasol)\nlibrary(arrow)  \n\n# Téléchargement des données de la BPE\ndonnees_BPE &lt;- telechargerDonnees(\"BPE_ENS\", date = 2021)\n\n# Éecriture des données sous format Parquet\nwrite_parquet(\n  x = donnees_BPE,\n  sink = \"Data_parquet/BPE_ENS.parquet\"\n)\n\n\n15.3.2 Données volumineuses: écrire un fichier Parquet partitionné\nLe package arrow présente une fonctionnalité supplémentaire qui consiste à créer et lire un fichier Parquet partitionné. Le partitionnement des fichiers Parquet présente des avantages pratiques qui sont expliqués dans la suite de cette fiche (voir partie Lire et exploiter un fichier Parquet avec R).\nPartitionner un fichier revient à le “découper” selon une clé de partitionnement, qui prend la forme d’une ou de plusieurs variables. Cela signifie en pratique que l’ensemble des données sera stockée sous forme d’un grand nombre de fichiers Parquet (un fichier par valeur des variable de partitionnement). Par exemple, il est possible de partitionner un fichier national par département: on obtient alors un fichier Parquet par département.\n\n\n\n\n\n\nTip\n\n\n\n\n\nIl est important de bien choisir les variables de partitionnement d’un fichier Parquet. Il faut choisir des variables faciles à comprendre et qui soient cohérentes avec l’usage des données (année, département, secteur…). En effet, un partitionnement bien construit induit par la suite des gains d’efficacité sur les traitements et facilite la maintenance du fichier sur le long terme.\n\nIl est inutile de partitionner des données de petite taille. Si les données dépassent quelques millions d’observations et/ou si leur taille en CSV dépasse quelques giga-octets, il est utile de partitionner.\n\nIl ne faut pas partitionner les données en trop de fichiers. En pratique, il est rare d’avoir besoin de plus d’une ou deux variables de partitionnement.\n\nSi vous souhaitez être compatible avec tous les outils lisant du parquet, il est recommandé de ne pas partitionner sur une variable pouvant être NA ou une chaîne vide.\n\n\n\nPour créer des fichiers Parquet partitionnés, il faut utiliser la fonction write_dataset() du package arrow. Voici ce que ça donne sur le fichier de la BPE :\n\nwrite_dataset(\n  dataset = donnees_BPE, \n  path = \"Data/\", \n  partitioning = c(\"REG\"), # la variable de partitionnement\n  format=\"parquet\"\n)\n\nAvec cette instruction, on a créé autant de répertoires que de modalités différentes de la variable REG. Vous pouvez noter la structure des dossiers nommés REG==[valeur].\n\n\n\n\nArborescence d’un fichier Parquet partitionné\n\n\n\n\n15.3.3 Données volumineuses: optimiser en triant\nIl n’est pas toujours possible ou souhaitable de partitionner un fichier si la variable de partitionnement possède de trop nombreuses modalités (si celle-ci est non discrète ou possède des milliers de modalités…). Dans ces cas là, vous pouvez trier le fichier par la variable à utiliser, cela va permettre une recherche efficace à partir des métadonnées des fichiers et des groupes de lignes.\n\ndonnees_BPE |&gt;\n  arrange(EPCI) |&gt;\n  write_parquet(\n    sink = \"Data_parquet/BPE_ENS.parquet\"\n    )\n\nVous pouvez bien sûr cumuler les partitions avec des tris :\n\ndonnees_BPE |&gt;\n  arrange(EPCI) |&gt;\n  write_dataset(\n    path = \"Data/\",\n    partitioning = c(\"REG\"), # la variable de partitionnement\n    format=\"parquet\"\n  )\n\nCette méthode est quasiment aussi efficace que le partitionnement.\n\n15.3.4 Vérifier qu’un fichier parquet est correctement optimisé\nDans certains cas pathologiques quand vous manipulez de très gros volumes de données (par exemple quand vous partitionnez avec arrow::open_dataset un volume de données de plusieurs de dizaine de millions de lignes), le package arrow peut générer des fichiers parquet avec un nombre de lignes par row group très petite (inférieur à 1000 voire à 100). Cela rendra toutes les requêtes sur vos fichiers extrèmement lent.\nPour vérifier que vos fichiers ont une taille de row group correcte, vous pouvez utiliser la fonction suivante :\n\nlibrary(arrow)\n\nmean_row_group_size &lt;- function(path) {\n  a &lt;- arrow::ParquetFileReader$create(path)\n  (a$num_rows / a$num_row_groups)\n}\nmean_row_group_size('bpe2018/REG=11/part-0.parquet')\n\nVous devez obtenir une valeur au moins égale à 10 000. Si vous obtenez une valeur inférieure, vous aurez intérêt à regénérer votre fichier en passant par d’autres méthodes (par exemple générer chaque partition en faisant une boucle et un filtre).",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lire et écrire des fichiers Parquet</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#readparquet",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#readparquet",
    "title": "15  Lire et écrire des fichiers Parquet",
    "section": "\n15.4 Lire et exploiter un fichier Parquet avec R\n",
    "text": "15.4 Lire et exploiter un fichier Parquet avec R\n\n\n15.4.1 Cas des données peu volumineuses: importer les données en mémoire\nLa méthode présentée dans cette section est valable uniquement pour les fichiers peu volumineux. Elle implique en effet d’importer l’intégralité d’un fichier Parquet dans la mémoire vive de votre espace de travail avant de pouvoir travailler dessus. Il est possible d’effectuer des requêtes plus efficacement sur des fichiers Parquet. Pour cette raison, il est conseillé d’utiliser la fonction open_dataset (présentée plus bas) pour accéder à des données stockées en format Parquet, plutôt que la fonction read_parquet.\nLa fonction read_parquet() du package arrow permet d’importer des fichiers Parquet dans R. Elle possède un argument très utile col_select qui permet de sélectionner les variables à importer (par défaut toutes). Cet argument accepte soit une liste de noms de variables, soit une expression dite de tidy selection issue du tidyverse.\nPour utiliser read_parquet(), il faut charger le package arrow :\n\nlibrary(arrow)\ndonnees &lt;- arrow::read_parquet(\"Data/BPE_ENS.parquet\")\n\n\nExemple en ne sélectionnant que quelques variables à l’aide d’un vecteur de caractères :\n\n\ndonnees &lt;- arrow::read_parquet(\n  \"Data/BPE_ENS.parquet\",\n  col_select = c('AN','REG','DEP','SDOM','TYPEQU','NB_EQUIP')\n) \n\n\nExemple en ne sélectionnant que quelques variables à l’aide d’une tidy selection :\n\n\ndonnees &lt;- arrow::read_parquet(\n  \"Data/BPE_ENS.parquet\",\n  col_select = starts_with(\"DEP\")\n) \n\nDans les trois cas, le résultat obtenu est un objet directement utilisable dans R.\n\n15.4.2 Cas des données volumineuses: utiliser des requêtes dplyr\n\nIl arrive fréquemment que la méthode proposée dans la section précédente ne puisse pas être appliquée, car les données que l’on souhaite exploiter sont trop volumineuses pour être importées dans la mémoire vive dont on dispose. Par exemple, le fichier des données du recensement de la population 1968-2019 fait 3,2 Go et contient plus de 51,5 millions de lignes et 18 colonnes, ce qui est difficile à importer sur un ordinateur standard.\nLes packages arrow et dplyr proposent une approche qui permet de traiter ces données très volumineuses sans les charger dans la mémoire vive. Cette approche nécessite de charger les packages arrow et dplyr et comprend trois étapes:\n\nOn crée une connexion au fichier Parquet avec la fonction open_dataset(): comme la fonction read_parquet(), elle ouvre le fichier Parquet, mais elle n’importe pas les données contenues dans le fichier;\nOn définit une chaîne de traitement (ou requête) avec la syntaxe du tidyverse (voir la fiche Manipuler des données avec le tidyverse). Consultez cette page pour accéder à la liste des verbes issus du tidyverse connus par arrow;\nOn termine la requête avec la fonction collect(), qui indique à R que l’on souhaite récupérer le résultat de la requête sous forme d’un data.frame.\n\nVoici un exemple avec une table peu volumineuse :\n\nlibrary(dplyr)\nlibrary(arrow)\n\n# Établir la connexion aux données\ndonnees_BPE &lt;- open_dataset(\"Data/BPE_ENS.parquet\")\n\n# Définir la requête\nrequete &lt;- donnees_BPE |&gt;\n  filter(REG == \"76\") |&gt;\n  group_by(DEP) |&gt;\n  summarise(nb_equipements_total = SUM(NB_EQUIP))\n  \n# Récupérer le résultat sous forme d'un data.frame\nresultat &lt;- requete |&gt; collect()\n\nAvec cette syntaxe, la requête va automatiquement utiliser les variables du fichier Parquet dont elle a besoin (en l’occurence REG, DEP et NB_EQUIP) et minimiser l’occupation de la mémoire vive.\n\nExemple avec une table volumineuse (Recensements 1968-2019, suivre ce lien pour obtenir le code qui permet de générer “Ficdep19.parquet” de façon reproductible) :\n\n\n# Attention ce morceau de code n'est pas reproductible,\n# Il faut suivre le lien dans le texte pour reconstruire le fichier de données\nlibrary(dplyr)\n\n# Établir la connexion aux données\ndonnees_Ficdep19 &lt;- open_dataset(\"Data/Ficdep19.parquet\")\n\n# Définir la requête\nrequete2 &lt; - donnees_Ficdep19 |&gt;\n  filter(DEP_RES_21 == \"11\") |&gt;\n  group_by(SEXE) |&gt;\n  summarise(total = sum(pond)) |&gt;\n  collect()\n  \n# Récupérer le résultat sous forme d'un data.frame\nresultat2 &lt;- requete2 |&gt; collect()\n\nCette instruction s’exécute sur un ordinateur standard en quelques secondes.\n\n\n\n\n\n\nNote\n\n\n\nLes packages arrow et duckdb présentent une grande différence avec les packages standard de manipulation de données comme dplyr ou data.table: lorsqu’on exécute une requête sur une table de données, ces packages ne se contentent pas d’exécuter les commandes une à une, dans l’ordre du code, mais analysent le code pour optimiser le plan d’exécution de la requête. En pratique, cela signifie qu’arrow et duckdb essaient de n’importer que les observations nécessaires à la requête, de ne conserver que les colonnes nécessaires au calcul, etc. C’est cette optimisation du plan d’exécution (appelée predicate push-down) qui permet d’accélérer les traitements et de réduire la consommation de ressources informatiques.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lire et écrire des fichiers Parquet</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#lire-et-exploiter-un-fichier-parquet-partitionné",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#lire-et-exploiter-un-fichier-parquet-partitionné",
    "title": "15  Lire et écrire des fichiers Parquet",
    "section": "\n15.5 Lire et exploiter un fichier Parquet partitionné",
    "text": "15.5 Lire et exploiter un fichier Parquet partitionné\n\n15.5.1 Quel est l’intérêt d’utiliser des fichiers Parquet partitionnés?\nComme indiqué précédemment, les packages arrow et duckdb ne se contentent pas d’exécuter les instructions de la requête une à une, dans l’ordre du code, mais analysent la requête dans son ensemble pour optimiser le plan d’exécution de la requête. Toutefois, il n’est pas possible de charger seulement quelques lignes d’un fichier Parquet: on importe nécessairement des colonnes entières. C’est principalement sur ce point qu’utiliser un fichier Parquet partitionné facilite ce travail d’optimisation du plan d’exécution. En effet, lorsque le fichier Parquet est partitionné, arrow est capable de filtrer les lignes à importer à l’aide des clés de partitionnement, ce qui permet d’accélérer l’importation des données.\nExemple : imaginons que la Base Permanente des Équipements soit stockée sous la forme d’un fichier Parquet partitionné par région (REG), et qu’on veuille compter le nombre d’équipements de chaque type dans chaque département de la région Hauts-de-France (REG == \"32\"). On utilisera le code suivant:\n\n# Établir la connexion au fichier Parquet partitionné\ndonnees_BPE_part &lt;- open_dataset(\n  \"Data/\",\n  partitioning = arrow::schema(REG = arrow::utf8())\n)\n\n# Définir la requête\nrequete_BPE &lt;- donnees_BPE_part |&gt;\n  filter(REG == \"32\") %&gt;% # Ici, on filtre selon la clé de partitionnement\n  select(DEP, TYPEQU, NB_EQUIP) %&gt;%\n  group_by(DEP, TYPEQU) %&gt;%\n  summarise(nb_equipements = sum(NB_EQUIP))\n\n# Récupérer le résultat sous forme d'un data.frame\nresultat_BPE &lt;- requete_BPE |&gt; collect()\n\nAu moment d’exécuter cette requête, arrow va utiliser la variable de partitionnement pour ne lire que la partie REG == \"32\" du fichier partitionné (donc seulement une partie des observations). Autrement dit, le fait que le fichier Parquet soit partitionné accélère la lecture des données.\nEn conclusion, l’utilisation des fichiers Parquet partitionné présente deux avantages :\n\nElle permet de travailler sur des fichiers Parquet de plus petite taille et de consommer moins de mémoire vive;\nElle fait gagner du temps dans l’exécution des requêtes sur les fichiers volumineux (par rapport à un fichier Parquet unique).\n\n15.5.2 Comment bien utiliser les fichiers Parquet partitionnés?\n\n15.5.2.1 Avec le package arrow\n\nLa fonction open_dataset() permet d’ouvrir une connexion vers un fichier Parquet partitionné. L’utilisation de la fonction open_dataset() est similaire au cas dans lequel on travaille avec un seul fichier Parquet. Il y a toutefois deux différences:\n\nLe chemin indiqué n’est pas celui d’un fichier .parquet, mais le chemin d’un répertoire, dans lequel se trouve le fichier Parquet partitionné;\nIl est préférable d’indiquer le nom et le type de la ou des variable(s) de partitionnement.\n\nUne fois que la connexion est établie avec le fichier partitionné, il est possible de l’utiliser exactement comme s’il s’agissait d’un seul fichier Parquet. Voici un exemple de code:\n\n# Établir la connexion au fichier Parquet partitionné\ndonnees_part &lt;- open_dataset(\n  \"Data/\", # Ici, on met le chemin d'un répertoire\n  hive_style = TRUE,\n  partitioning = arrow::schema(REG = arrow::utf8()) # La variable de partitionnement\n)\n\n# Définir la requête\nrequete5 &lt;- donnees_part |&gt;\n  filter(REG == \"32\") |&gt; # Ici, on filtre selon la clé de partitionnement\n  select(DEP, TYPEQU, NB_EQUIP) %&gt;%\n  group_by(DEP) |&gt;\n  summarise(nb_total = sum(NB_EQUIP))\n\n# Récupérer le résultat sous forme d'un data.frame\nresultat5 &lt;- requete5 |&gt; collect()\n\nPour bien utiliser un fichier Parquet partitionné, il est recommandé de suivre les deux conseils suivants:\n\nAfin de tirer au mieux profit du partitionnement, il est conseillé de filtrer les données de préférence selon les variables de partitionnement (dans notre exemple, la région);\nIl est fortement recommandé de spécifier le type des variables de partitionnement avec l’argument partitioning. Cela évite des erreurs typiques: le code du département est interprété à tort comme un nombre et aboutit à une erreur à cause de la Corse… L’argument partitioning s’utilise en construisant un schéma qui précise le type de chacune des variables de partitionnement:\n\n\ndonnees_part &lt;- open_dataset(\n  \"Data/\",\n  partitioning = arrow::schema(variable1 = arrow::utf8(), variable2 = arrow::int16())\n)\n\nLes types les plus fréquents sont: nombre entier (int8(), int16(), int32(), int64()), nombre réel (float(), float32(), float64()), et chaîne de caractère (utf8(), large_utf8()). Il existe beaucoup d’autres types, vous pouvez en consulter la liste en exécutant ?arrow::float ou en consultant cette page.\n\nIl est recommandé de définir les deux options suivantes au début de votre script. Cela autorise arrow à utiliser plusieurs processeurs à la fois, ce qui accélère les traitements:\n\n\n# Autoriser arrow à utiliser plusieurs processeurs en même temps\noptions(arrow.use_threads = TRUE)\n# Définir le nombre de processeurs utilisés par arrow\n# 10 processeurs sont suffisants dans la plupart des cas\narrow:::set_cpu_count(10)",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lire et écrire des fichiers Parquet</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#pour-en-savoir-plus",
    "href": "03_Fiches_thematiques/Fiche_import_fichiers_parquet.html#pour-en-savoir-plus",
    "title": "15  Lire et écrire des fichiers Parquet",
    "section": "\n15.6 Pour en savoir plus",
    "text": "15.6 Pour en savoir plus\n\nPage officielle de duckdb\nApache Parquet pour le stockage de données volumineuses",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Lire et écrire des fichiers Parquet</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_api.html",
    "href": "03_Fiches_thematiques/Fiche_api.html",
    "title": "16  Travailler avec des API",
    "section": "",
    "text": "16.1 Rappels des notions essentielles sur les API\nL’utilisateur souhaite accéder à des données via une API.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Travailler avec des API</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_api.html#rappels-des-notions-essentielles-sur-les-api",
    "href": "03_Fiches_thematiques/Fiche_api.html#rappels-des-notions-essentielles-sur-les-api",
    "title": "16  Travailler avec des API",
    "section": "",
    "text": "16.1.1 Qu’est-ce qu’une API ?\nUne Application Programming Interface (ou API) est une interface de programmation qui permet d’utiliser une application existante pour restituer des données. Le terme d’API peut être paraître intimidant, mais il s’agit simplement d’une façon de mettre à disposition des données : plutôt que de laisser l’utilisateur consulter directement des bases de données (souvent volumineuses et complexes), l’API lui propose de formuler une requête qui est traitée par le serveur hébergeant la base de données, puis de recevoir des données en réponse à sa requête.\nD’un point de vue informatique, une API est une porte d’entrée clairement identifiée par laquelle un logiciel offre des services à d’autres logiciels (ou utilisateurs). L’objectif d’une API est de fournir un point d’accès à une fonctionnalité qui soit facile à utiliser et qui masque les détails de la mise en oeuvre. Par exemple, l’API Sirene permet de récupérer la raison sociale d’une entreprise à partir de son identifiant Siren en interrogeant le référentiel disponible sur Internet directement depuis un script R, sans avoir à connaître tous les détails du répertoire Sirene.\nÀ l’Insee comme ailleurs, la connexion entre les bases de données pour les nouveaux projets tend à se réaliser par des API. L’accès à des données par des API devient ainsi de plus en plus commun et est amené à devenir une compétence de base de tout utilisateur de données.\n\n16.1.2 Avantages des API\nLes API présentent de multiples avantages :\n\nLes API rendent les programmes plus reproductibles. En effet, grâce aux API, il est possible de mettre à jour facilement les données utilisées par un programme si celles-ci évoluent. Cette flexibilité accrue pour l’utilisateur évite au producteur de données d’avoir à réaliser de multiples extractions, et réduit le problème de la coexistence de versions différentes des données.\nGrâce aux API, l’utilisateur peut extraire facilement une petite partie d’une base de données plus conséquente.\nLes API permettent de mettre à disposition des données tout en limitant le nombre de personnes ayant accès aux bases de données elles-mêmes.\nGrâce aux API, il est possible de proposer des services sur mesure pour les utilisateurs (par exemple, un accès spécifique pour les gros utilisateurs).\n\n16.1.3 Utilisation des API\nUne API peut souvent être utilisée de deux façons : par une interface Web, et par l’intermédiaire d’un logiciel (R, Python…). Par ailleurs, les API peuvent être proposées avec un niveau de liberté variable pour l’utilisateur :\n\nsoit en libre accès (l’utilisation n’est pas contrôlée et l’utilisateur peut utiliser le service comme bon lui semble) ;\nsoit via la génération d’un compte et d’un jeton d’accès qui permettent de sécuriser l’utilisation de l’API et de limiter le nombre de requêtes.\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nLes API mises à disposition des utilisateurs par l’Insee se trouvent dans le catalogue des API.\n\n\n\n16.1.3.1 Consulter l’interface Web d’une API\nLes API peuvent proposer une interface Web, mais ce n’est pas toujours le cas. Cette interface permet notamment :\n\nde s’inscrire aux différents services ;\nde visualiser les différentes requêtes proposées par les services ;\nde lancer l’API depuis cette plateforme ;\nde proposer une documentation sur les API.\n\nL’utilisation de l’interface Web est utile dans une démarche exploratoire mais trouve rapidement ses limites, notamment lorsqu’on consulte régulièrement l’API. L’utilisateur va rapidement se rendre compte qu’il est beaucoup plus commode d’utiliser une API via un logiciel de traitement pour automatiser la consultation ou pour réaliser du téléchargement de masse. De plus, l’interface Web n’existe pas systématiquement pour toutes les API.\n\n16.1.3.2 Requêter une API\nLe mode principal de consultation d’une API consiste à adresser une requête à cette API via un logiciel adapté (R, Python, Java…). Comme pour l’utilisation d’une fonction, l’appel d’une API comprend des paramètres qui sont détaillées dans la documentation de l’API. Voici les éléments importants à avoir en tête sur les requêtes :\n\nLe point d’entrée d’un service offert par une API se présente sous la forme d’une URL (adresse web). Chaque service proposé par une API a sa propre URL. Par exemple, dans le cas de l’API Sirene, l’URL à utiliser pour obtenir des informations sur un Siren est : https://api.insee.fr/entreprises/sirene/V3/siren/.\nCette URL doit être complétée avec différents paramètres qui précisent la requête (par exemple l’identifiant Siren). Ces paramètres viennent s’ajouter à l’URL (qui peut donc devenir très longue!). Chaque service proposé par une API a ses propres paramètres, détaillés dans la documentation. S’agissant de l’API Sirene, l’utilisateur intéressé peut retrouver dans la documentation le paramétrage de l’URL.\n\n\nLorsque l’utilisateur soumet sa requête, l’API lui renvoie une réponse structurée contenant l’ensemble des informations demandées. Le résultat envoyé par une API est majoritairement aux formats JSON ou XML (deux formats dans lesquels les informations sont hiérarchisées de manière emboîtée). Plus rarement, certains services proposent une information sous forme plate (de type csv).\nDu fait de la dimension hiérarchique des formats JSON ou XML, le résultat n’est pas toujours facile à récupérer. Certains packages, comme jsonlite ou xml2, facilitent l’extraction de champs d’une sortie d’API. Dans certains cas, des packages spécifiques à une API ont été créés pour simplifier l’écriture d’une requête ou la récupération du résultat.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Travailler avec des API</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_api.html#quelques-packages-permettant-une-utilisation-simple-des-api-des-données-insee",
    "href": "03_Fiches_thematiques/Fiche_api.html#quelques-packages-permettant-une-utilisation-simple-des-api-des-données-insee",
    "title": "16  Travailler avec des API",
    "section": "\n16.2 Quelques packages permettant une utilisation simple des API des données Insee",
    "text": "16.2 Quelques packages permettant une utilisation simple des API des données Insee\nNous allons voir ici quelques packages permettant de traiter l’information d’une API facilement :\n\n16.2.1 Le package apinsee\n\nCe package est très utile pour l’utilisation des API mises à disposition sur le catalogue des API. En effet, pour ces API, pour pouvoir effectuer une requête, il est nécessaire de s’authentifier à différents niveaux :\n\nS’authentifier grâce à une clé personnelle, associée à un compte créé sur https://api.insee.fr ;\nAvoir un jeton d’accès temporaire. Ces jetons ont une durée de vie limitée et doivent régulièrement être renouvelés.\n\nCe package propose de générer facilement un jeton d’accès temporaire à partir de R. De cette façon, il n’y a plus besoin de naviguer entre le programme R et le catalogue des API pour renouveler un jeton.\nPour utiliser cette fonctionnalité, il faut configurer l’usage du package lors de la première utilisation. Plus précisément, il faut renseigner les variables d’environnement INSEE_APP_KEY et INSEE_APP_SECRET dans le fichier de configuration .Renviron de R. (voir la fiche [Personnaliser la configuration de R] pour une présentation détaillée des fichiers de configuration de R). Pour le faire, il suffit d’exécuter la commande suivante :\n\nusethis::edit_r_environ(\"user\")\n\nVotre fichier .Renviron est alors ouvert par RStudio s’il existe déjà. Si le fichier .Renviron n’existe pas encore, il est automatiquement créé (vide), enregistré et ouvert par RStudio. Il convient d’y ajouter deux variables d’environnement, INSEE_APP_KEY et INSEE_APP_SECRET. Les lignes suivantes peuvent servir de modèle, en remplaçant la deuxième partie de chaque ligne par la clé du compte à utiliser :\n# clef du consommateur\nINSEE_APP_KEY=xxxxxxxxxxxxxxxxxx\n# secret du consommateur\nINSEE_APP_SECRET=yyyyyyyyyyyyyyyyyy\n\n\n\n\n\n\nNote\n\n\n\nVoici deux remarques sur le stockage des variables d’environnement :\n\nL’option \"user\" dans l’utilisation de la fonction usethis::edit_r_environ() permet de stocker ces clés dans un fichier global, connu de tous les projets d’un utilisateur R. Cela évite, d’une part, de stocker la même information à deux endroits différents. D’autre part, cela évite d’associer des informations personnelles à un projet, qui doit être sous contrôle de version (voir la Fiche Utiliser R avec RStudio) car le fichier .Renviron est un fichier contenant des informations personnelles.\nSi vous avez choisi l’option \"project\" lors de l’appel à usethis::edit_r_environ, il faut ajouter le .Renviron dans les fichiers à ne pas suivre avec Git grâce à la commande suivante :\n\n\nusethis::edit_git_ignore(\".Renviron\")\n\n\n\nEnfin, pour créer le token temporaire, il suffit d’exécuter :\n\ntoken &lt;- apinsee::insee_auth()\n\nCe token peut ensuite être utilisé comme valeur du paramètre token de la fonction httr::config() qui sert à contrôler les paramètres d’une requête vers internet faite par R\n\nlibrary(httr)\nset_config(config(token = token))\n\nDès lors, vous pouvez accéder aux API de l’Insee auxquelles votre application a souscrit.\n\n16.2.2 Le package doremifasol\n\nCe package permet entre autres de solliciter l’API Sirène dans une procédure de requêtage intégrée.\ndoremifasol s’appuie sur le package apinsee, il faut donc renseigner les variables d’environnement décrites ci-dessus. Il reste ensuite à préciser le type d’information souhaitée (unités légales, établissements…) et la requête via l’argument argsApi. Par exemple, pour lister tous les bouchers de la ville de Tourcoing :\n\nbouchers_tourcoing &lt;-\n  telechargerDonnees(\n    \"SIRENE_SIRET\",\n    argsApi = list(q = \"codeCommuneEtablissement:59599 AND activitePrincipaleUniteLegale:47.22Z\")\n  )\n\n\n16.2.3 Le package inseeLocalData\n\nCe package permet de télécharger les données localisées à la commune, diffusées sur https://www.insee.fr dans la rubrique Chiffres détaillés, sous forme de cubes prédéfinis. Cette API est hébergée sur le catalogue des API de l’Insee. Une authentification par un jeton est donc nécessaire.\nLe package comporte une fonction unique qui permet d’importer les données présentes dans l’API Données Locales dans une liste contenant 4 objets :\n\nles données statistiques ;\nles modalités de chaque variable ;\nl’information sur la zone demandée ;\nl’information sur la source et le jeu de données demandé.\n\nExemple d’utilisation du package pour importer le nombre d’entreprises et d’établissements en 2017 (en géographie au 01/01/2017) selon l’activité en 5 catégories et une indicatrice indiquant s’il s’agit d’une entreprise individuelle ou non pour la commune de Nantes :\n\nlibrary(inseeLocalData)\n\ncroisement &lt;- \"NA5_B-ENTR_INDIVIDUELLE\"\njeu_donnees &lt;- \"GEO2017REE2017\"\nnivgeo &lt;- \"COM\"\ncodgeo &lt;- \"44109\" #CODE GEO DE NANTES\nmodalite &lt;- \"all.all\"\n\ndonneesAPI &lt;- get_dataset(jeton, jeu_donnees, croisement, modalite, nivgeo, codgeo)\n\ndonnees &lt;- donneesAPI$donnees # pour accéder aux données\nliste_code &lt;- donneesAPI$liste_code # pour accéder aux nomenclatures\ninfo_zone &lt;- donneesAPI$info_zone # pour accéder aux données géographiques\nsource &lt;- donneesAPI$source # pour accéder à la source\n\n\n\n16.2.4 Le package insee\n\nCe package permet de télécharger les données et leurs métadonnées diffusées sur le service SDMX de la Base de données Macroéconomique de l’Insee (BDM). Cette API étant ouverte, son accès ne demande pas d’identification, ni de jeton. Il est uniquement nécessaire de déterminer les données souhaitées soit via une liste de catégories (idbank), soit via une liste de données (dataset).\n\n\n\n\nInformation\nFonction\n\n\n\nListe des jeux de données\ninsee::get_dataset_list()\n\n\nListe des séries\ninsee::get_idbank_list()\n\n\n\n\n\n\nlibrary(insee)\n\n# Importer les données à partir de leur idbank ou importer un dataset à partir de leur identifiant:\ntable_bp &lt;- get_insee_dataset(\"BALANCE-PAIEMENTS\")\nindicateur_001694056 &lt;- get_insee_idbank(\"001694056\")\n\nIl est possible de rajouter des filtres à ces fonctions afin de limiter le nombre de données importées (filtre sur la période, sur la date de mise à jour, sur les filtres).\n\n\n16.2.5 Le package OECD\n\nCe package permet de télécharger les données mises à disposition sur le site de l’OCDE. Cette API étant ouverte, son accès ne demande pas d’identification, ni de jeton. Il est uniquement nécessaire de déterminer les données souhaitées.\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nCe package utilise la librairie rsmdx qui n’est pas compatible avec la technologie Direct Access. Il ne fonctionne pas en télétravail pour les postes nomades qui accèdent à internet par ce biais. En revanche il fonctionne sur site.\n\n\nVoici quelques utilisations possibles de ce package :\n\nlibrary(OECD)\n\n# Obtenir la liste des tables présentes sur le site :\ndsets &lt;- get_datasets()\n\n# Voir les métadonnées d'une table, via l'ouverture d'une page web (ici la table DUR_D) :\nbrowse_metadata(\"DUR_D\")\n\n# Importer une table de données (ici la table DUR_D) :\ndata &lt;- get_dataset(\"DUR_D\")",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Travailler avec des API</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_api.html#exemple-dutilisation-dune-api-sans-package",
    "href": "03_Fiches_thematiques/Fiche_api.html#exemple-dutilisation-dune-api-sans-package",
    "title": "16  Travailler avec des API",
    "section": "\n16.3 Exemple d’utilisation d’une API sans package\n",
    "text": "16.3 Exemple d’utilisation d’une API sans package\n\nLes exemples précédents proposaient l’accès à une API par le biais d’un package. Pour lire les données d’une API ne possédant pas de package, il faut utiliser les deux packages R suivants :\n\nle package httr pour lancer la requête ;\n\npuis le package jsonlite pour transformer en data.frame le résultat de la requête (qui est structurée en JSON).\n\n\n\n\n\n\n\nNote\n\n\n\nSelon la structure du JSON récupéré, la manipulation du résultat d’une requête peut être assez fastidieuse avec R. Python propose des outils plus performants pour retravailler des JSON (le package json notamment). Heureusement, grâce au package reticulate, il est aisé de faire tourner un code Python dans une session R et récupérer le résultat dans un format de données (par exemple data.frame) de R. L’approche par les API étant plus fréquente en Python qu’en R, on trouve également plus de packages facilitant l’accès à des données par ce biais en Python.\n\n\n\n16.3.1 Le package httr\n\nLe package httr permet de se connecter aux sites web et de se connecter aux API.\nIl est possible de configurer la connexion internet localement sans modifier les variables système :\n\n\nset_config() permet de configurer l’accès internet utilisée par les fonctions du package.\n\n\nuse_proxy() permet de déterminer le proxy à utiliser. De nombreuses institutions utilisent passent par un intermédiaire, le proxy, pour accéder à internet. L’adresse du proxy est à ajouter au requête car sinon R ne sait pas communiquer avec internet. Il s’agit d’un paramètre à ajouter dans les options httr.\n\n\n\n\n\n\n\nNote\n\n\n\nSous windows, le proxy peut être paramétré de la manière suivante:\n\nproxy &lt;- curl::ie_get_proxy_for_url()\nhttr::set_config(httr::use_proxy(proxy))\n\n\n\nLe package httr permet, lorsqu’on effectue une requête GET (une requête d’accès au résultat d’une recherche), de récupérer le résultat sous la forme d’un texte à retravailler.\n\n16.3.2 Accès à une API sans jeton\nEn général, un appel à une API via httr s’effectue ainsi de la manière suivante :\n\nhttr::content(\n  httr::GET(url),             # url correspond à l'url à interroger\n  as = \"text\",                # type de la sortie renvoyée\n  httr::content_type_json(),  # type de la réponse de l'url\n  encoding = \"UTF-8\"          # encodage de la réponse de l'url\n)\n\nPrenons par exemple l’API d’OpenFood Facts, une base de données alimentaire. Imaginons qu’on désire récupérer l’information sur un produit. Cela s’obtient de la manière suivante :\n\nurl &lt;- \"https://world.openfoodfacts.org/api/v0/product/3017620425400.json\"\n\nresultats &lt;- \n  httr::content(\n    httr::GET(url),             # url correspond à l'url à interroger\n    as=\"text\",                  # type de la sortie renvoyée\n    httr::content_type_json(),  # type de la réponse de l'url\n    encoding= \"UTF-8\"            # encodage de la réponse de l'url\n  )\n\nLe résultat est formaté sous forme de JSON, ce qui est pratique mais peu intelligible :\n\njsonlite::prettify(resultats)\n\n{\n    \"code\": \"3017620425400\",\n    \"product\": {\n        \"_id\": \"3017620425400\",\n        \"_keywords\": [\n            \"rspo\",\n            \"pate\",\n            \"cacao\",\n            \"de\",\n            \"ferrero\",\n            \"sucre\",\n            \"et\",\n            \"huile\",\n            \"durable\",\n            \"aux\",\n            \"chocolat\",\n            \"tartiner\",\n            \"au\",\n            \"petit-dejeuner\",\n            \"noisette\",\n            \"palme\",\n            \"nutella\",\n            \"produit\"\n        ],\n        \"added_countries_tags\": [\n\n        ],\n        \"additives_n\": 1,\n        \"additives_original_tags\": [\n            \"en:e322i\"\n        ],\n        \"additives_tags\": [\n            \"en:e322\",\n            \"en:e322i\"\n        ],\n        \"allergens\": \"en:milk,en:nuts,en:soybeans\",\n        \"allergens_debug_tags\": [\n\n        ],\n        \"allergens_from_ingredients\": \"en:soybeans, en:milk, en:nuts, NOISETTES , LAIT , SOJA, NOISETTES, LAIT, SOJA\",\n        \"allergens_from_user\": \"(fr) en:milk,en:nuts,en:soybeans\",\n        \"allergens_hierarchy\": [\n            \"en:milk\",\n            \"en:nuts\",\n            \"en:soybeans\"\n        ],\n        \"allergens_tags\": [\n            \"en:milk\",\n            \"en:nuts\",\n            \"en:soybeans\"\n        ],\n        \"amino_acids_prev_tags\": [\n\n        ],\n        \"amino_acids_tags\": [\n\n        ],\n        \"brands\": \"Ferrero, Nutella\",\n        \"brands_old\": \"Nutella,Ferrero\",\n        \"brands_tags\": [\n            \"ferrero\",\n            \"nutella\"\n        ],\n        \"carbon_footprint_from_known_ingredients_debug\": \"en:hazelnut-oil 13% x 2.6 = 33.8 g - \",\n        \"carbon_footprint_percent_of_known_ingredients\": 13,\n        \"categories\": \"Petit-déjeuners, Produits à tartiner, Produits à tartiner sucrés, Pâtes à tartiner, Pâtes à tartiner aux noisettes, Pâtes à tartiner au chocolat, Pâtes à tartiner aux noisettes et au cacao\",\n        \"categories_hierarchy\": [\n            \"en:breakfasts\",\n            \"en:spreads\",\n            \"en:sweet-spreads\",\n            \"fr:pates-a-tartiner\",\n            \"en:hazelnut-spreads\",\n            \"en:chocolate-spreads\",\n            \"en:cocoa-and-hazelnuts-spreads\"\n        ],\n        \"categories_lc\": \"fr\",\n        \"categories_old\": \"Petit-déjeuners, Produits à tartiner, Produits à tartiner sucrés, Pâtes à tartiner, Pâtes à tartiner aux noisettes, Pâtes à tartiner au chocolat, Pâtes à tartiner aux noisettes et au cacao\",\n        \"categories_properties\": {\n            \"agribalyse_food_code:en\": \"31032\",\n            \"agribalyse_proxy_food_code:en\": \"31032\",\n            \"ciqual_food_code:en\": \"31032\"\n        },\n        \"categories_properties_tags\": [\n            \"all-products\",\n            \"categories-known\",\n            \"agribalyse-food-code-31032\",\n            \"agribalyse-food-code-known\",\n            \"agribalyse-proxy-food-code-31032\",\n            \"agribalyse-proxy-food-code-known\",\n            \"ciqual-food-code-31032\",\n            \"ciqual-food-code-known\",\n            \"agribalyse-known\",\n            \"agribalyse-31032\"\n        ],\n        \"categories_tags\": [\n            \"en:breakfasts\",\n            \"en:spreads\",\n            \"en:sweet-spreads\",\n            \"fr:pates-a-tartiner\",\n            \"en:hazelnut-spreads\",\n            \"en:chocolate-spreads\",\n            \"en:cocoa-and-hazelnuts-spreads\"\n        ],\n        \"category_properties\": {\n            \"ciqual_food_name:en\": \"Chocolate spread with hazelnuts\"\n        },\n        \"checkers\": [\n\n        ],\n        \"checkers_tags\": [\n\n        ],\n        \"ciqual_food_name_tags\": [\n            \"chocolate-spread-with-hazelnuts\"\n        ],\n        \"cities_tags\": [\n\n        ],\n        \"code\": \"3017620425400\",\n        \"codes_tags\": [\n            \"code-13\",\n            \"3017620425xxx\",\n            \"301762042xxxx\",\n            \"30176204xxxxx\",\n            \"3017620xxxxxx\",\n            \"301762xxxxxxx\",\n            \"30176xxxxxxxx\",\n            \"3017xxxxxxxxx\",\n            \"301xxxxxxxxxx\",\n            \"30xxxxxxxxxxx\",\n            \"3xxxxxxxxxxxx\"\n        ],\n        \"compared_to_category\": \"en:cocoa-and-hazelnuts-spreads\",\n        \"complete\": 0,\n        \"completeness\": 0.7625,\n        \"correctors\": [\n\n        ],\n        \"correctors_tags\": [\n            \"teolemon\",\n            \"scanbot\",\n            \"stephane\",\n            \"tacite\",\n            \"nicolasleger\",\n            \"segundo\",\n            \"jgonzale92\",\n            \"charlesnepote\",\n            \"packbot\"\n        ],\n        \"countries\": \"France, Suisse\",\n        \"countries_debug_tags\": [\n\n        ],\n        \"countries_hierarchy\": [\n            \"en:france\",\n            \"en:switzerland\"\n        ],\n        \"countries_lc\": \"fr\",\n        \"countries_tags\": [\n            \"en:france\",\n            \"en:switzerland\"\n        ],\n        \"created_t\": 1350234941,\n        \"creator\": \"openfoodfacts-contributors\",\n        \"data_quality_bugs_tags\": [\n\n        ],\n        \"data_quality_errors_tags\": [\n\n        ],\n        \"data_quality_info_tags\": [\n            \"en:packaging-data-incomplete\",\n            \"en:ingredients-percent-analysis-ok\",\n            \"en:carbon-footprint-from-known-ingredients-but-not-from-meat-or-fish\",\n            \"en:environmental-score-extended-data-not-computed\",\n            \"en:food-groups-1-known\",\n            \"en:food-groups-2-known\",\n            \"en:food-groups-3-unknown\"\n        ],\n        \"data_quality_tags\": [\n            \"en:packaging-data-incomplete\",\n            \"en:ingredients-percent-analysis-ok\",\n            \"en:carbon-footprint-from-known-ingredients-but-not-from-meat-or-fish\",\n            \"en:environmental-score-extended-data-not-computed\",\n            \"en:food-groups-1-known\",\n            \"en:food-groups-2-known\",\n            \"en:food-groups-3-unknown\",\n            \"en:environmental-score-origins-of-ingredients-origins-are-100-percent-unknown\",\n            \"en:environmental-score-packaging-unscored-shape\",\n            \"en:environmental-score-production-system-no-label\"\n        ],\n        \"data_quality_warnings_tags\": [\n            \"en:environmental-score-origins-of-ingredients-origins-are-100-percent-unknown\",\n            \"en:environmental-score-packaging-unscored-shape\",\n            \"en:environmental-score-production-system-no-label\"\n        ],\n        \"debug_param_sorted_langs\": [\n            \"fr\"\n        ],\n        \"ecoscore_data\": {\n            \"adjustments\": {\n                \"origins_of_ingredients\": {\n                    \"aggregated_origins\": [\n                        {\n                            \"epi_score\": \"0\",\n                            \"origin\": \"en:unknown\",\n                            \"percent\": 100,\n                            \"transportation_score\": 0\n                        }\n                    ],\n                    \"epi_score\": 0,\n                    \"epi_value\": -5,\n                    \"origins_from_categories\": [\n                        \"en:unknown\"\n                    ],\n                    \"origins_from_origins_field\": [\n                        \"en:unknown\"\n                    ],\n                    \"transportation_score\": 0,\n                    \"transportation_scores\": {\n                        \"ad\": 0,\n                        \"al\": 0,\n                        \"at\": 0,\n                        \"ax\": 0,\n                        \"ba\": 0,\n                        \"be\": 0,\n                        \"bg\": 0,\n                        \"ch\": 0,\n                        \"cy\": 0,\n                        \"cz\": 0,\n                        \"de\": 0,\n                        \"dk\": 0,\n                        \"dz\": 0,\n                        \"ee\": 0,\n                        \"eg\": 0,\n                        \"es\": 0,\n                        \"fi\": 0,\n                        \"fo\": 0,\n                        \"fr\": 0,\n                        \"gg\": 0,\n                        \"gi\": 0,\n                        \"gr\": 0,\n                        \"hr\": 0,\n                        \"hu\": 0,\n                        \"ie\": 0,\n                        \"il\": 0,\n                        \"im\": 0,\n                        \"is\": 0,\n                        \"it\": 0,\n                        \"je\": 0,\n                        \"lb\": 0,\n                        \"li\": 0,\n                        \"lt\": 0,\n                        \"lu\": 0,\n                        \"lv\": 0,\n                        \"ly\": 0,\n                        \"ma\": 0,\n                        \"mc\": 0,\n                        \"md\": 0,\n                        \"me\": 0,\n                        \"mk\": 0,\n                        \"mt\": 0,\n                        \"nl\": 0,\n                        \"no\": 0,\n                        \"pl\": 0,\n                        \"ps\": 0,\n                        \"pt\": 0,\n                        \"ro\": 0,\n                        \"rs\": 0,\n                        \"se\": 0,\n                        \"si\": 0,\n                        \"sj\": 0,\n                        \"sk\": 0,\n                        \"sm\": 0,\n                        \"sy\": 0,\n                        \"tn\": 0,\n                        \"tr\": 0,\n                        \"ua\": 0,\n                        \"uk\": 0,\n                        \"us\": 0,\n                        \"va\": 0,\n                        \"world\": 0,\n                        \"xk\": 0\n                    },\n                    \"transportation_value\": 0,\n                    \"transportation_values\": {\n                        \"ad\": 0,\n                        \"al\": 0,\n                        \"at\": 0,\n                        \"ax\": 0,\n                        \"ba\": 0,\n                        \"be\": 0,\n                        \"bg\": 0,\n                        \"ch\": 0,\n                        \"cy\": 0,\n                        \"cz\": 0,\n                        \"de\": 0,\n                        \"dk\": 0,\n                        \"dz\": 0,\n                        \"ee\": 0,\n                        \"eg\": 0,\n                        \"es\": 0,\n                        \"fi\": 0,\n                        \"fo\": 0,\n                        \"fr\": 0,\n                        \"gg\": 0,\n                        \"gi\": 0,\n                        \"gr\": 0,\n                        \"hr\": 0,\n                        \"hu\": 0,\n                        \"ie\": 0,\n                        \"il\": 0,\n                        \"im\": 0,\n                        \"is\": 0,\n                        \"it\": 0,\n                        \"je\": 0,\n                        \"lb\": 0,\n                        \"li\": 0,\n                        \"lt\": 0,\n                        \"lu\": 0,\n                        \"lv\": 0,\n                        \"ly\": 0,\n                        \"ma\": 0,\n                        \"mc\": 0,\n                        \"md\": 0,\n                        \"me\": 0,\n                        \"mk\": 0,\n                        \"mt\": 0,\n                        \"nl\": 0,\n                        \"no\": 0,\n                        \"pl\": 0,\n                        \"ps\": 0,\n                        \"pt\": 0,\n                        \"ro\": 0,\n                        \"rs\": 0,\n                        \"se\": 0,\n                        \"si\": 0,\n                        \"sj\": 0,\n                        \"sk\": 0,\n                        \"sm\": 0,\n                        \"sy\": 0,\n                        \"tn\": 0,\n                        \"tr\": 0,\n                        \"ua\": 0,\n                        \"uk\": 0,\n                        \"us\": 0,\n                        \"va\": 0,\n                        \"world\": 0,\n                        \"xk\": 0\n                    },\n                    \"value\": -5,\n                    \"values\": {\n                        \"ad\": -5,\n                        \"al\": -5,\n                        \"at\": -5,\n                        \"ax\": -5,\n                        \"ba\": -5,\n                        \"be\": -5,\n                        \"bg\": -5,\n                        \"ch\": -5,\n                        \"cy\": -5,\n                        \"cz\": -5,\n                        \"de\": -5,\n                        \"dk\": -5,\n                        \"dz\": -5,\n                        \"ee\": -5,\n                        \"eg\": -5,\n                        \"es\": -5,\n                        \"fi\": -5,\n                        \"fo\": -5,\n                        \"fr\": -5,\n                        \"gg\": -5,\n                        \"gi\": -5,\n                        \"gr\": -5,\n                        \"hr\": -5,\n                        \"hu\": -5,\n                        \"ie\": -5,\n                        \"il\": -5,\n                        \"im\": -5,\n                        \"is\": -5,\n                        \"it\": -5,\n                        \"je\": -5,\n                        \"lb\": -5,\n                        \"li\": -5,\n                        \"lt\": -5,\n                        \"lu\": -5,\n                        \"lv\": -5,\n                        \"ly\": -5,\n                        \"ma\": -5,\n                        \"mc\": -5,\n                        \"md\": -5,\n                        \"me\": -5,\n                        \"mk\": -5,\n                        \"mt\": -5,\n                        \"nl\": -5,\n                        \"no\": -5,\n                        \"pl\": -5,\n                        \"ps\": -5,\n                        \"pt\": -5,\n                        \"ro\": -5,\n                        \"rs\": -5,\n                        \"se\": -5,\n                        \"si\": -5,\n                        \"sj\": -5,\n                        \"sk\": -5,\n                        \"sm\": -5,\n                        \"sy\": -5,\n                        \"tn\": -5,\n                        \"tr\": -5,\n                        \"ua\": -5,\n                        \"uk\": -5,\n                        \"us\": -5,\n                        \"va\": -5,\n                        \"world\": -5,\n                        \"xk\": -5\n                    },\n                    \"warning\": \"origins_are_100_percent_unknown\"\n                },\n                \"packaging\": {\n                    \"non_recyclable_and_non_biodegradable_materials\": 0,\n                    \"packagings\": [\n                        {\n                            \"environmental_score_material_score\": 0,\n                            \"environmental_score_shape_ratio\": 0.2,\n                            \"food_contact\": 0,\n                            \"material\": \"en:plastic\",\n                            \"non_recyclable_and_non_biodegradable\": \"maybe\",\n                            \"shape\": \"en:lid\"\n                        },\n                        {\n                            \"environmental_score_material_score\": 72,\n                            \"environmental_score_shape_ratio\": 0.1,\n                            \"food_contact\": 1,\n                            \"material\": \"en:heavy-aluminium\",\n                            \"shape\": \"en:seal\"\n                        },\n                        {\n                            \"environmental_score_material_score\": 81,\n                            \"environmental_score_shape_ratio\": 1,\n                            \"food_contact\": 1,\n                            \"material\": \"en:glass\",\n                            \"shape\": \"en:pot\"\n                        },\n                        {\n                            \"environmental_score_material_score\": 0,\n                            \"environmental_score_shape_ratio\": 1,\n                            \"food_contact\": 1,\n                            \"material\": \"en:unknown\",\n                            \"shape\": \"en:jar\"\n                        }\n                    ],\n                    \"score\": -41.8,\n                    \"value\": -14,\n                    \"warning\": \"unscored_shape\"\n                },\n                \"production_system\": {\n                    \"labels\": [\n\n                    ],\n                    \"value\": 0,\n                    \"warning\": \"no_label\"\n                },\n                \"threatened_species\": {\n\n                }\n            },\n            \"agribalyse\": {\n                \"agribalyse_food_code\": \"31032\",\n                \"agribalyse_proxy_food_code\": \"31032\",\n                \"co2_agriculture\": 6.8500101,\n                \"co2_consumption\": 0,\n                \"co2_distribution\": 0.017263204,\n                \"co2_packaging\": 0.17071537,\n                \"co2_processing\": 0.3052603,\n                \"co2_total\": 7.564940134,\n                \"co2_transportation\": 0.22169116,\n                \"code\": \"31032\",\n                \"dqr\": \"2.54\",\n                \"ef_agriculture\": 0.44346677,\n                \"ef_consumption\": 0,\n                \"ef_distribution\": 0.0046101581,\n                \"ef_packaging\": 0.018565697,\n                \"ef_processing\": 0.047606624,\n                \"ef_total\": 0.5347352491,\n                \"ef_transportation\": 0.020486,\n                \"is_beverage\": 0,\n                \"name_en\": \"Chocolate spread with hazelnuts\",\n                \"name_fr\": \"Pâte à tartiner chocolat et noisette\",\n                \"score\": 51,\n                \"version\": \"3.1.1\"\n            },\n            \"grade\": \"d\",\n            \"grades\": {\n                \"ad\": \"d\",\n                \"al\": \"d\",\n                \"at\": \"d\",\n                \"ax\": \"d\",\n                \"ba\": \"d\",\n                \"be\": \"d\",\n                \"bg\": \"d\",\n                \"ch\": \"d\",\n                \"cy\": \"d\",\n                \"cz\": \"d\",\n                \"de\": \"d\",\n                \"dk\": \"d\",\n                \"dz\": \"d\",\n                \"ee\": \"d\",\n                \"eg\": \"d\",\n                \"es\": \"d\",\n                \"fi\": \"d\",\n                \"fo\": \"d\",\n                \"fr\": \"d\",\n                \"gg\": \"d\",\n                \"gi\": \"d\",\n                \"gr\": \"d\",\n                \"hr\": \"d\",\n                \"hu\": \"d\",\n                \"ie\": \"d\",\n                \"il\": \"d\",\n                \"im\": \"d\",\n                \"is\": \"d\",\n                \"it\": \"d\",\n                \"je\": \"d\",\n                \"lb\": \"d\",\n                \"li\": \"d\",\n                \"lt\": \"d\",\n                \"lu\": \"d\",\n                \"lv\": \"d\",\n                \"ly\": \"d\",\n                \"ma\": \"d\",\n                \"mc\": \"d\",\n                \"md\": \"d\",\n                \"me\": \"d\",\n                \"mk\": \"d\",\n                \"mt\": \"d\",\n                \"nl\": \"d\",\n                \"no\": \"d\",\n                \"pl\": \"d\",\n                \"ps\": \"d\",\n                \"pt\": \"d\",\n                \"ro\": \"d\",\n                \"rs\": \"d\",\n                \"se\": \"d\",\n                \"si\": \"d\",\n                \"sj\": \"d\",\n                \"sk\": \"d\",\n                \"sm\": \"d\",\n                \"sy\": \"d\",\n                \"tn\": \"d\",\n                \"tr\": \"d\",\n                \"ua\": \"d\",\n                \"uk\": \"d\",\n                \"us\": \"d\",\n                \"va\": \"d\",\n                \"world\": \"d\",\n                \"xk\": \"d\"\n            },\n            \"missing\": {\n                \"labels\": 1,\n                \"origins\": 1,\n                \"packagings\": 1\n            },\n            \"missing_data_warning\": 1,\n            \"score\": 32,\n            \"scores\": {\n                \"ad\": 32,\n                \"al\": 32,\n                \"at\": 32,\n                \"ax\": 32,\n                \"ba\": 32,\n                \"be\": 32,\n                \"bg\": 32,\n                \"ch\": 32,\n                \"cy\": 32,\n                \"cz\": 32,\n                \"de\": 32,\n                \"dk\": 32,\n                \"dz\": 32,\n                \"ee\": 32,\n                \"eg\": 32,\n                \"es\": 32,\n                \"fi\": 32,\n                \"fo\": 32,\n                \"fr\": 32,\n                \"gg\": 32,\n                \"gi\": 32,\n                \"gr\": 32,\n                \"hr\": 32,\n                \"hu\": 32,\n                \"ie\": 32,\n                \"il\": 32,\n                \"im\": 32,\n                \"is\": 32,\n                \"it\": 32,\n                \"je\": 32,\n                \"lb\": 32,\n                \"li\": 32,\n                \"lt\": 32,\n                \"lu\": 32,\n                \"lv\": 32,\n                \"ly\": 32,\n                \"ma\": 32,\n                \"mc\": 32,\n                \"md\": 32,\n                \"me\": 32,\n                \"mk\": 32,\n                \"mt\": 32,\n                \"nl\": 32,\n                \"no\": 32,\n                \"pl\": 32,\n                \"ps\": 32,\n                \"pt\": 32,\n                \"ro\": 32,\n                \"rs\": 32,\n                \"se\": 32,\n                \"si\": 32,\n                \"sj\": 32,\n                \"sk\": 32,\n                \"sm\": 32,\n                \"sy\": 32,\n                \"tn\": 32,\n                \"tr\": 32,\n                \"ua\": 32,\n                \"uk\": 32,\n                \"us\": 32,\n                \"va\": 32,\n                \"world\": 32,\n                \"xk\": 32\n            },\n            \"status\": \"known\"\n        },\n        \"ecoscore_grade\": \"d\",\n        \"ecoscore_score\": 32,\n        \"ecoscore_tags\": [\n            \"d\"\n        ],\n        \"editors\": [\n            \"\",\n            \"segundo\",\n            \"teolemon\",\n            \"tacite\",\n            \"manu1400\",\n            \"scanbot\",\n            \"nicolasleger\",\n            \"stephane\"\n        ],\n        \"editors_tags\": [\n            \"segundo\",\n            \"openfoodfacts-contributors\",\n            \"scanbot\",\n            \"teolemon\",\n            \"packbot\",\n            \"stephane\",\n            \"manu1400\",\n            \"tacite\",\n            \"nicolasleger\",\n            \"jgonzale92\",\n            \"charlesnepote\"\n        ],\n        \"emb_codes\": \"\",\n        \"emb_codes_20141016\": \"\",\n        \"emb_codes_debug_tags\": [\n\n        ],\n        \"emb_codes_orig\": \"\",\n        \"emb_codes_tags\": [\n\n        ],\n        \"entry_dates_tags\": [\n            \"2012-10-14\",\n            \"2012-10\",\n            \"2012\"\n        ],\n        \"expiration_date\": \"\",\n        \"expiration_date_debug_tags\": [\n\n        ],\n        \"food_groups\": \"en:sweets\",\n        \"food_groups_tags\": [\n            \"en:sugary-snacks\",\n            \"en:sweets\"\n        ],\n        \"fruits-vegetables-nuts_100g_estimate\": 0,\n        \"generic_name\": \"Pâte à tartiner aux noisettes\",\n        \"generic_name_fr\": \"Pâte à tartiner aux noisettes\",\n        \"generic_name_fr_debug_tags\": [\n\n        ],\n        \"id\": \"3017620425400\",\n        \"image_front_small_url\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.200.jpg\",\n        \"image_front_thumb_url\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.100.jpg\",\n        \"image_front_url\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.400.jpg\",\n        \"image_small_url\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.200.jpg\",\n        \"image_thumb_url\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.100.jpg\",\n        \"image_url\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.400.jpg\",\n        \"images\": {\n            \"1\": {\n                \"sizes\": {\n                    \"100\": {\n                        \"h\": 75,\n                        \"w\": 100\n                    },\n                    \"400\": {\n                        \"h\": 300,\n                        \"w\": 400\n                    },\n                    \"full\": {\n                        \"h\": 1920,\n                        \"w\": 2560\n                    }\n                },\n                \"uploaded_t\": 1350234942,\n                \"uploader\": \"openfoodfacts-contributors\"\n            },\n            \"2\": {\n                \"sizes\": {\n                    \"100\": {\n                        \"h\": 100,\n                        \"w\": 75\n                    },\n                    \"400\": {\n                        \"h\": 400,\n                        \"w\": 300\n                    },\n                    \"full\": {\n                        \"h\": 960,\n                        \"w\": 720\n                    }\n                },\n                \"uploaded_t\": 1358527087,\n                \"uploader\": \"openfoodfacts-contributors\"\n            },\n            \"front\": {\n                \"geometry\": \"1632x1914-678-6\",\n                \"imgid\": \"1\",\n                \"normalize\": null,\n                \"rev\": \"11\",\n                \"sizes\": {\n                    \"100\": {\n                        \"h\": 100,\n                        \"w\": 85\n                    },\n                    \"200\": {\n                        \"h\": 200,\n                        \"w\": 171\n                    },\n                    \"400\": {\n                        \"h\": 400,\n                        \"w\": 341\n                    },\n                    \"full\": {\n                        \"h\": 1914,\n                        \"w\": 1632\n                    }\n                },\n                \"white_magic\": null\n            },\n            \"front_fr\": {\n                \"geometry\": \"1632x1914-678-6\",\n                \"imgid\": \"1\",\n                \"normalize\": null,\n                \"rev\": \"11\",\n                \"sizes\": {\n                    \"100\": {\n                        \"h\": 100,\n                        \"w\": 85\n                    },\n                    \"200\": {\n                        \"h\": 200,\n                        \"w\": 171\n                    },\n                    \"400\": {\n                        \"h\": 400,\n                        \"w\": 341\n                    },\n                    \"full\": {\n                        \"h\": 1914,\n                        \"w\": 1632\n                    }\n                },\n                \"white_magic\": null\n            }\n        },\n        \"informers\": [\n            \"manu1400\",\n            \"stephane\"\n        ],\n        \"informers_tags\": [\n            \"manu1400\",\n            \"stephane\",\n            \"teolemon\",\n            \"tacite\",\n            \"nicolasleger\",\n            \"segundo\",\n            \"jgonzale92\",\n            \"charlesnepote\"\n        ],\n        \"ingredients\": [\n            {\n                \"ciqual_proxy_food_code\": \"31016\",\n                \"ecobalyse_code\": \"8f075c25-9ebf-430c-b41d-51d165c6e0d8\",\n                \"id\": \"en:sugar\",\n                \"is_in_taxonomy\": 1,\n                \"percent_estimate\": 39.275,\n                \"percent_max\": 57.9,\n                \"percent_min\": 20.65,\n                \"rank\": 1,\n                \"text\": \"Sucre\",\n                \"vegan\": \"yes\",\n                \"vegetarian\": \"yes\"\n            },\n            {\n                \"ciqual_food_code\": \"16129\",\n                \"ecobalyse_code\": \"45658c32-66d9-4305-a34b-21d6a4cef89c\",\n                \"from_palm_oil\": \"yes\",\n                \"id\": \"en:palm-oil\",\n                \"is_in_taxonomy\": 1,\n                \"percent_estimate\": 24.225,\n                \"percent_max\": 35.45,\n                \"percent_min\": 13,\n                \"rank\": 2,\n                \"text\": \"Huile de palme\",\n                \"vegan\": \"yes\",\n                \"vegetarian\": \"yes\"\n            },\n            {\n                \"ciqual_food_code\": \"15004\",\n                \"ecobalyse_code\": \"hazelnut-unshelled-non-eu\",\n                \"id\": \"en:hazelnut\",\n                \"is_in_taxonomy\": 1,\n                \"percent\": 13,\n                \"percent_estimate\": 13,\n                \"percent_max\": 13,\n                \"percent_min\": 13,\n                \"rank\": 3,\n                \"text\": \"NOISETTES\",\n                \"vegan\": \"yes\",\n                \"vegetarian\": \"yes\"\n            },\n            {\n                \"ciqual_food_code\": \"19054\",\n                \"ecobalyse_code\": \"b6776a28-ec84-4bf3-988c-07b3c29f6136\",\n                \"id\": \"en:skimmed-milk-powder\",\n                \"is_in_taxonomy\": 1,\n                \"percent\": 8.7,\n                \"percent_estimate\": 8.7,\n                \"percent_max\": 8.7,\n                \"percent_min\": 8.7,\n                \"rank\": 4,\n                \"text\": \"LAIT écrémé en poudre\",\n                \"vegan\": \"no\",\n                \"vegetarian\": \"yes\"\n            },\n            {\n                \"ciqual_proxy_food_code\": \"18100\",\n                \"id\": \"en:fat-reduced-cocoa\",\n                \"is_in_taxonomy\": 1,\n                \"percent\": 7.4,\n                \"percent_estimate\": 7.4,\n                \"percent_max\": 7.4,\n                \"percent_min\": 7.4,\n                \"rank\": 5,\n                \"text\": \"cacao maigre\",\n                \"vegan\": \"yes\",\n                \"vegetarian\": \"yes\"\n            },\n            {\n                \"has_sub_ingredients\": \"yes\",\n                \"id\": \"en:emulsifier\",\n                \"is_in_taxonomy\": 1,\n                \"percent_estimate\": 3.7,\n                \"percent_max\": 7.4,\n                \"percent_min\": 0,\n                \"rank\": 6,\n                \"text\": \"émulsifiant\"\n            },\n            {\n                \"id\": \"en:vanillin\",\n                \"is_in_taxonomy\": 1,\n                \"percent_estimate\": 3.69999999999999,\n                \"percent_max\": 7.4,\n                \"percent_min\": 0,\n                \"rank\": 7,\n                \"text\": \"vanilline\"\n            },\n            {\n                \"has_sub_ingredients\": \"yes\",\n                \"id\": \"en:e322i\",\n                \"is_in_taxonomy\": 1,\n                \"percent_estimate\": 3.7,\n                \"percent_max\": 7.4,\n                \"percent_min\": 0,\n                \"text\": \"lécithine\",\n                \"vegan\": \"maybe\",\n                \"vegetarian\": \"maybe\"\n            },\n            {\n                \"ciqual_food_code\": \"42200\",\n                \"id\": \"en:soya-lecithin\",\n                \"is_in_taxonomy\": 1,\n                \"percent_estimate\": 3.7,\n                \"percent_max\": 7.4,\n                \"percent_min\": 0,\n                \"text\": \"lécithine de SOJA\",\n                \"vegan\": \"yes\",\n                \"vegetarian\": \"yes\"\n            }\n        ],\n        \"ingredients_analysis\": {\n            \"en:non-vegan\": [\n                \"en:skimmed-milk-powder\"\n            ],\n            \"en:palm-oil\": [\n                \"en:palm-oil\"\n            ],\n            \"en:vegan-status-unknown\": [\n                \"en:vanillin\"\n            ],\n            \"en:vegetarian-status-unknown\": [\n                \"en:vanillin\"\n            ]\n        },\n        \"ingredients_analysis_tags\": [\n            \"en:palm-oil\",\n            \"en:non-vegan\",\n            \"en:vegetarian-status-unknown\"\n        ],\n        \"ingredients_debug\": [\n            \"Sucre\",\n            \",\",\n            null,\n            null,\n            null,\n            \" Huile de palme\",\n            \",\",\n            null,\n            null,\n            null,\n            \" NOISETTES 13%\",\n            \",\",\n            null,\n            null,\n            null,\n            \" LAIT écrémé en poudre 8\",\n            \",\",\n            null,\n            null,\n            null,\n            \"7%\",\n            \",\",\n            null,\n            null,\n            null,\n            \" cacao maigre 7\",\n            \",\",\n            null,\n            null,\n            null,\n            \"4%\",\n            \",\",\n            null,\n            null,\n            null,\n            \" émulsifiant \",\n            \":\",\n            \":\",\n            null,\n            null,\n            \"  lécithine \",\n            \"(\",\n            \"(\",\n            null,\n            null,\n            \"SOJA)\",\n            \",\",\n            null,\n            null,\n            null,\n            \" vanilline.\"\n        ],\n        \"ingredients_from_or_that_may_be_from_palm_oil_n\": 1,\n        \"ingredients_from_palm_oil_n\": 1,\n        \"ingredients_from_palm_oil_tags\": [\n            \"huile-de-palme\"\n        ],\n        \"ingredients_hierarchy\": [\n            \"en:sugar\",\n            \"en:added-sugar\",\n            \"en:disaccharide\",\n            \"en:palm-oil\",\n            \"en:oil-and-fat\",\n            \"en:vegetable-oil-and-fat\",\n            \"en:palm-oil-and-fat\",\n            \"en:hazelnut\",\n            \"en:nut\",\n            \"en:tree-nut\",\n            \"en:skimmed-milk-powder\",\n            \"en:dairy\",\n            \"en:milk-powder\",\n            \"en:fat-reduced-cocoa\",\n            \"en:plant\",\n            \"en:cocoa\",\n            \"en:emulsifier\",\n            \"en:vanillin\",\n            \"en:e322i\",\n            \"en:e322\",\n            \"en:soya-lecithin\"\n        ],\n        \"ingredients_ids_debug\": [\n            \"sucre\",\n            \"huile-de-palme\",\n            \"noisettes-13\",\n            \"lait-ecreme-en-poudre-8\",\n            \"7\",\n            \"cacao-maigre-7\",\n            \"4\",\n            \"emulsifiant\",\n            \"lecithine\",\n            \"soja\",\n            \"vanilline\"\n        ],\n        \"ingredients_lc\": \"fr\",\n        \"ingredients_n\": 9,\n        \"ingredients_n_tags\": [\n            \"9\",\n            \"1-10\"\n        ],\n        \"ingredients_non_nutritive_sweeteners_n\": 0,\n        \"ingredients_original_tags\": [\n            \"en:sugar\",\n            \"en:palm-oil\",\n            \"en:hazelnut\",\n            \"en:skimmed-milk-powder\",\n            \"en:fat-reduced-cocoa\",\n            \"en:emulsifier\",\n            \"en:vanillin\",\n            \"en:e322i\",\n            \"en:soya-lecithin\"\n        ],\n        \"ingredients_percent_analysis\": 1,\n        \"ingredients_sweeteners_n\": 0,\n        \"ingredients_tags\": [\n            \"en:sugar\",\n            \"en:added-sugar\",\n            \"en:disaccharide\",\n            \"en:palm-oil\",\n            \"en:oil-and-fat\",\n            \"en:vegetable-oil-and-fat\",\n            \"en:palm-oil-and-fat\",\n            \"en:hazelnut\",\n            \"en:nut\",\n            \"en:tree-nut\",\n            \"en:skimmed-milk-powder\",\n            \"en:dairy\",\n            \"en:milk-powder\",\n            \"en:fat-reduced-cocoa\",\n            \"en:plant\",\n            \"en:cocoa\",\n            \"en:emulsifier\",\n            \"en:vanillin\",\n            \"en:e322i\",\n            \"en:e322\",\n            \"en:soya-lecithin\"\n        ],\n        \"ingredients_text\": \"Sucre, Huile de palme, NOISETTES 13%, LAIT écrémé en poudre 8,7%, cacao maigre 7,4%, émulsifiant : lécithine (SOJA), vanilline.\",\n        \"ingredients_text_debug\": \"Sucre, Huile de palme, NOISETTES 13%, LAIT écrémé en poudre 8,7%, cacao maigre 7,4%, émulsifiant :  lécithine (SOJA), vanilline.\",\n        \"ingredients_text_fr\": \"Sucre, Huile de palme, NOISETTES 13%, LAIT écrémé en poudre 8,7%, cacao maigre 7,4%, émulsifiant : lécithine (SOJA), vanilline.\",\n        \"ingredients_text_fr_debug_tags\": [\n\n        ],\n        \"ingredients_text_with_allergens\": \"Sucre, Huile de palme, &lt;span class=\\\"allergen\\\"&gt;NOISETTES&lt;\\/span&gt; &lt;\\/span&gt;13%, &lt;span class=\\\"allergen\\\"&gt;LAIT&lt;\\/span&gt; &lt;\\/span&gt;écrémé en poudre 8,7%, cacao maigre 7,4%, émulsifiant : lécithine (&lt;span class=\\\"allergen\\\"&gt;SOJA&lt;\\/span&gt;), vanilline.\",\n        \"ingredients_text_with_allergens_fr\": \"Sucre, Huile de palme, &lt;span class=\\\"allergen\\\"&gt;NOISETTES&lt;\\/span&gt; &lt;\\/span&gt;13%, &lt;span class=\\\"allergen\\\"&gt;LAIT&lt;\\/span&gt; &lt;\\/span&gt;écrémé en poudre 8,7%, cacao maigre 7,4%, émulsifiant : lécithine (&lt;span class=\\\"allergen\\\"&gt;SOJA&lt;\\/span&gt;), vanilline.\",\n        \"ingredients_that_may_be_from_palm_oil_n\": 0,\n        \"ingredients_that_may_be_from_palm_oil_tags\": [\n\n        ],\n        \"ingredients_with_specified_percent_n\": 3,\n        \"ingredients_with_specified_percent_sum\": 29.1,\n        \"ingredients_with_unspecified_percent_n\": 4,\n        \"ingredients_with_unspecified_percent_sum\": 70.9,\n        \"ingredients_without_ciqual_codes\": [\n            \"en:e322i\",\n            \"en:emulsifier\",\n            \"en:vanillin\"\n        ],\n        \"ingredients_without_ciqual_codes_n\": 3,\n        \"ingredients_without_ecobalyse_ids\": [\n            \"en:e322i\",\n            \"en:emulsifier\",\n            \"en:fat-reduced-cocoa\",\n            \"en:soya-lecithin\",\n            \"en:vanillin\"\n        ],\n        \"ingredients_without_ecobalyse_ids_n\": 5,\n        \"interface_version_created\": \"20120622\",\n        \"interface_version_modified\": \"20150316.jqm2\",\n        \"known_ingredients_n\": 9,\n        \"labels\": \"en:Sustainable, Huile de palme durable, RSPO\",\n        \"labels_hierarchy\": [\n            \"en:sustainable\",\n            \"en:sustainable-palm-oil\",\n            \"en:roundtable-on-sustainable-palm-oil\"\n        ],\n        \"labels_lc\": \"fr\",\n        \"labels_old\": \"en:Sustainable, Huile de palme durable, RSPO\",\n        \"labels_tags\": [\n            \"en:sustainable\",\n            \"en:sustainable-palm-oil\",\n            \"en:roundtable-on-sustainable-palm-oil\"\n        ],\n        \"lang\": \"fr\",\n        \"lang_debug_tags\": [\n\n        ],\n        \"languages\": {\n            \"en:french\": 4\n        },\n        \"languages_codes\": {\n            \"fr\": 4\n        },\n        \"languages_hierarchy\": [\n            \"en:french\"\n        ],\n        \"languages_tags\": [\n            \"en:french\",\n            \"en:1\"\n        ],\n        \"last_edit_dates_tags\": [\n            \"2022-02-10\",\n            \"2022-02\",\n            \"2022\"\n        ],\n        \"last_editor\": \"packbot\",\n        \"last_image_dates_tags\": [\n            \"2013-01-18\",\n            \"2013-01\",\n            \"2013\"\n        ],\n        \"last_image_t\": 1358527087,\n        \"last_modified_by\": \"packbot\",\n        \"last_modified_t\": 1703163083,\n        \"last_updated_t\": 1743541326,\n        \"lc\": \"fr\",\n        \"link\": \"http://www.ferrero.fr/nutella\",\n        \"link_debug_tags\": [\n\n        ],\n        \"main_countries_tags\": [\n\n        ],\n        \"manufacturing_places\": \"\",\n        \"manufacturing_places_debug_tags\": [\n\n        ],\n        \"manufacturing_places_tags\": [\n\n        ],\n        \"max_imgid\": \"2\",\n        \"minerals_prev_tags\": [\n\n        ],\n        \"minerals_tags\": [\n\n        ],\n        \"misc_tags\": [\n            \"en:environmental-score-computed\",\n            \"en:environmental-score-missing-data-labels\",\n            \"en:environmental-score-missing-data-origins\",\n            \"en:environmental-score-missing-data-packagings\",\n            \"en:environmental-score-missing-data-warning\",\n            \"en:nutriscore-2021-e-2023-e\",\n            \"en:nutriscore-2021-same-as-2023\",\n            \"en:nutriscore-computed\",\n            \"en:nutrition-all-nutriscore-values-known\",\n            \"en:nutrition-fruits-vegetables-legumes-estimate-from-ingredients\",\n            \"en:nutrition-fruits-vegetables-nuts-estimate-from-ingredients\",\n            \"en:packagings-not-complete\",\n            \"en:packagings-not-empty\",\n            \"en:packagings-not-empty-but-not-complete\",\n            \"en:packagings-number-of-components-4\",\n            \"en:some-ingredients-with-specified-percent\"\n        ],\n        \"no_nutrition_data\": \"\",\n        \"nova_group\": 4,\n        \"nova_group_debug\": \"\",\n        \"nova_groups\": \"4\",\n        \"nova_groups_markers\": {\n            \"3\": [\n                [\n                    \"ingredients\",\n                    \"en:sugar\"\n                ],\n                [\n                    \"ingredients\",\n                    \"en:milk-powder\"\n                ]\n            ],\n            \"4\": [\n                [\n                    \"additives\",\n                    \"en:e322\"\n                ],\n                [\n                    \"ingredients\",\n                    \"en:emulsifier\"\n                ]\n            ]\n        },\n        \"nova_groups_tags\": [\n            \"en:4-ultra-processed-food-and-drink-products\"\n        ],\n        \"nucleotides_prev_tags\": [\n\n        ],\n        \"nucleotides_tags\": [\n\n        ],\n        \"nutrient_levels\": {\n            \"fat\": \"high\",\n            \"salt\": \"low\",\n            \"saturated-fat\": \"high\",\n            \"sugars\": \"high\"\n        },\n        \"nutrient_levels_tags\": [\n            \"en:fat-in-high-quantity\",\n            \"en:saturated-fat-in-high-quantity\",\n            \"en:sugars-in-high-quantity\",\n            \"en:salt-in-low-quantity\"\n        ],\n        \"nutriments\": {\n            \"carbohydrates\": 57.3,\n            \"carbohydrates_100g\": 57.3,\n            \"carbohydrates_unit\": \"g\",\n            \"carbohydrates_value\": 57.3,\n            \"carbon-footprint-from-known-ingredients_100g\": 33.8,\n            \"carbon-footprint-from-known-ingredients_product\": 254,\n            \"energy\": 2273,\n            \"energy-kj\": 2273,\n            \"energy-kj_100g\": 2273,\n            \"energy-kj_unit\": \"kJ\",\n            \"energy-kj_value\": 2273,\n            \"energy-kj_value_computed\": 2272.5,\n            \"energy_100g\": 2273,\n            \"energy_unit\": \"kJ\",\n            \"energy_value\": 2273,\n            \"fat\": 31.6,\n            \"fat_100g\": 31.6,\n            \"fat_unit\": \"g\",\n            \"fat_value\": 31.6,\n            \"fiber\": 3.4,\n            \"fiber_100g\": 3.4,\n            \"fiber_unit\": \"g\",\n            \"fiber_value\": 3.4,\n            \"fruits-vegetables-legumes-estimate-from-ingredients_100g\": 0,\n            \"fruits-vegetables-legumes-estimate-from-ingredients_serving\": 0,\n            \"fruits-vegetables-nuts-estimate-from-ingredients_100g\": 13,\n            \"fruits-vegetables-nuts-estimate-from-ingredients_serving\": 13,\n            \"nova-group\": 4,\n            \"nova-group_100g\": 4,\n            \"nova-group_serving\": 4,\n            \"nutrition-score-fr\": 30,\n            \"nutrition-score-fr_100g\": 30,\n            \"proteins\": 6,\n            \"proteins_100g\": 6,\n            \"proteins_unit\": \"g\",\n            \"proteins_value\": 6,\n            \"salt\": 0.09398,\n            \"salt_100g\": 0.09398,\n            \"salt_unit\": \"g\",\n            \"salt_value\": 0.09398,\n            \"saturated-fat\": 10.9,\n            \"saturated-fat_100g\": 10.9,\n            \"saturated-fat_unit\": \"g\",\n            \"saturated-fat_value\": 10.9,\n            \"sodium\": 0.037592,\n            \"sodium_100g\": 0.037592,\n            \"sodium_unit\": \"g\",\n            \"sodium_value\": 0.037592,\n            \"sugars\": 56.7,\n            \"sugars_100g\": 56.7,\n            \"sugars_unit\": \"g\",\n            \"sugars_value\": 56.7\n        },\n        \"nutriscore\": {\n            \"2021\": {\n                \"category_available\": 1,\n                \"data\": {\n                    \"energy\": 2273,\n                    \"energy_points\": 6,\n                    \"energy_value\": 2273,\n                    \"fiber\": 3.4,\n                    \"fiber_points\": 3,\n                    \"fiber_value\": 3.4,\n                    \"fruits_vegetables_nuts_colza_walnut_olive_oils\": 13,\n                    \"fruits_vegetables_nuts_colza_walnut_olive_oils_points\": 0,\n                    \"fruits_vegetables_nuts_colza_walnut_olive_oils_value\": 13,\n                    \"is_beverage\": 0,\n                    \"is_cheese\": 0,\n                    \"is_fat\": 0,\n                    \"is_water\": 0,\n                    \"negative_points\": 26,\n                    \"positive_points\": 3,\n                    \"proteins\": 6,\n                    \"proteins_points\": 3,\n                    \"proteins_value\": 6,\n                    \"saturated_fat\": 10.9,\n                    \"saturated_fat_points\": 10,\n                    \"saturated_fat_value\": 10.9,\n                    \"sodium\": 37.592,\n                    \"sodium_points\": 0,\n                    \"sodium_value\": 37.6,\n                    \"sugars\": 56.7,\n                    \"sugars_points\": 10,\n                    \"sugars_value\": 56.7\n                },\n                \"grade\": \"e\",\n                \"nutrients_available\": 1,\n                \"nutriscore_applicable\": 1,\n                \"nutriscore_computed\": 1,\n                \"score\": 23\n            },\n            \"2023\": {\n                \"category_available\": 1,\n                \"data\": {\n                    \"components\": {\n                        \"negative\": [\n                            {\n                                \"id\": \"energy\",\n                                \"points\": 6,\n                                \"points_max\": 10,\n                                \"unit\": \"kJ\",\n                                \"value\": 2273\n                            },\n                            {\n                                \"id\": \"sugars\",\n                                \"points\": 15,\n                                \"points_max\": 15,\n                                \"unit\": \"g\",\n                                \"value\": 56.7\n                            },\n                            {\n                                \"id\": \"saturated_fat\",\n                                \"points\": 10,\n                                \"points_max\": 10,\n                                \"unit\": \"g\",\n                                \"value\": 10.9\n                            },\n                            {\n                                \"id\": \"salt\",\n                                \"points\": 0,\n                                \"points_max\": 20,\n                                \"unit\": \"g\",\n                                \"value\": 0.09\n                            }\n                        ],\n                        \"positive\": [\n                            {\n                                \"id\": \"fiber\",\n                                \"points\": 1,\n                                \"points_max\": 5,\n                                \"unit\": \"g\",\n                                \"value\": 3.4\n                            },\n                            {\n                                \"id\": \"fruits_vegetables_legumes\",\n                                \"points\": 0,\n                                \"points_max\": 5,\n                                \"unit\": \"%\",\n                                \"value\": 0\n                            }\n                        ]\n                    },\n                    \"count_proteins\": 0,\n                    \"count_proteins_reason\": \"negative_points_greater_than_or_equal_to_11\",\n                    \"is_beverage\": 0,\n                    \"is_cheese\": 0,\n                    \"is_fat_oil_nuts_seeds\": 0,\n                    \"is_red_meat_product\": 0,\n                    \"is_water\": 0,\n                    \"negative_points\": 31,\n                    \"negative_points_max\": 55,\n                    \"positive_nutrients\": [\n                        \"fiber\",\n                        \"fruits_vegetables_legumes\"\n                    ],\n                    \"positive_points\": 1,\n                    \"positive_points_max\": 10\n                },\n                \"grade\": \"e\",\n                \"nutrients_available\": 1,\n                \"nutriscore_applicable\": 1,\n                \"nutriscore_computed\": 1,\n                \"score\": 30\n            }\n        },\n        \"nutriscore_2021_tags\": [\n            \"e\"\n        ],\n        \"nutriscore_2023_tags\": [\n            \"e\"\n        ],\n        \"nutriscore_data\": {\n            \"components\": {\n                \"negative\": [\n                    {\n                        \"id\": \"energy\",\n                        \"points\": 6,\n                        \"points_max\": 10,\n                        \"unit\": \"kJ\",\n                        \"value\": 2273\n                    },\n                    {\n                        \"id\": \"sugars\",\n                        \"points\": 15,\n                        \"points_max\": 15,\n                        \"unit\": \"g\",\n                        \"value\": 56.7\n                    },\n                    {\n                        \"id\": \"saturated_fat\",\n                        \"points\": 10,\n                        \"points_max\": 10,\n                        \"unit\": \"g\",\n                        \"value\": 10.9\n                    },\n                    {\n                        \"id\": \"salt\",\n                        \"points\": 0,\n                        \"points_max\": 20,\n                        \"unit\": \"g\",\n                        \"value\": 0.09\n                    }\n                ],\n                \"positive\": [\n                    {\n                        \"id\": \"fiber\",\n                        \"points\": 1,\n                        \"points_max\": 5,\n                        \"unit\": \"g\",\n                        \"value\": 3.4\n                    },\n                    {\n                        \"id\": \"fruits_vegetables_legumes\",\n                        \"points\": 0,\n                        \"points_max\": 5,\n                        \"unit\": \"%\",\n                        \"value\": 0\n                    }\n                ]\n            },\n            \"count_proteins\": 0,\n            \"count_proteins_reason\": \"negative_points_greater_than_or_equal_to_11\",\n            \"grade\": \"e\",\n            \"is_beverage\": 0,\n            \"is_cheese\": 0,\n            \"is_fat_oil_nuts_seeds\": 0,\n            \"is_red_meat_product\": 0,\n            \"is_water\": 0,\n            \"negative_points\": 31,\n            \"negative_points_max\": 55,\n            \"positive_nutrients\": [\n                \"fiber\",\n                \"fruits_vegetables_legumes\"\n            ],\n            \"positive_points\": 1,\n            \"positive_points_max\": 10,\n            \"score\": 30\n        },\n        \"nutriscore_grade\": \"e\",\n        \"nutriscore_score\": 30,\n        \"nutriscore_score_opposite\": -30,\n        \"nutriscore_tags\": [\n            \"e\"\n        ],\n        \"nutriscore_version\": \"2023\",\n        \"nutrition_data\": \"on\",\n        \"nutrition_data_per\": \"100g\",\n        \"nutrition_data_per_debug_tags\": [\n\n        ],\n        \"nutrition_data_prepared\": \"\",\n        \"nutrition_data_prepared_per\": \"100g\",\n        \"nutrition_data_prepared_per_debug_tags\": [\n\n        ],\n        \"nutrition_grade_fr\": \"e\",\n        \"nutrition_grades\": \"e\",\n        \"nutrition_grades_tags\": [\n            \"e\"\n        ],\n        \"nutrition_score_beverage\": 0,\n        \"nutrition_score_debug\": \"\",\n        \"nutrition_score_warning_fruits_vegetables_legumes_estimate_from_ingredients\": 1,\n        \"nutrition_score_warning_fruits_vegetables_legumes_estimate_from_ingredients_value\": 0,\n        \"nutrition_score_warning_fruits_vegetables_nuts_estimate_from_ingredients\": 1,\n        \"nutrition_score_warning_fruits_vegetables_nuts_estimate_from_ingredients_value\": 13,\n        \"origins\": \"\",\n        \"origins_hierarchy\": [\n\n        ],\n        \"origins_lc\": \"fr\",\n        \"origins_old\": \"\",\n        \"origins_tags\": [\n\n        ],\n        \"other_nutritional_substances_tags\": [\n\n        ],\n        \"packaging\": \"Plastique, Verre, Bocal, Couvercle, Couvercle en plastique, Opercule aluminium, Pot en verre\",\n        \"packaging_hierarchy\": [\n            \"en:plastic\",\n            \"en:glass\",\n            \"en:jar\",\n            \"en:lid\",\n            \"fr:Couvercle en plastique\",\n            \"fr:Opercule aluminium\",\n            \"fr:Pot en verre\"\n        ],\n        \"packaging_lc\": \"fr\",\n        \"packaging_materials_tags\": [\n            \"en:aluminium\",\n            \"en:glass\",\n            \"en:plastic\"\n        ],\n        \"packaging_old\": \"Plastique, Verre, Bocal, Couvercle, Couvercle en plastique, Opercule aluminium, Pot en verre\",\n        \"packaging_old_before_taxonomization\": \"Bocal,Verre,Couvercle,Plastique, couvercle en plastique, opercule aluminium, pot en verre\",\n        \"packaging_recycling_tags\": [\n\n        ],\n        \"packaging_shapes_tags\": [\n            \"en:jar\",\n            \"en:lid\",\n            \"en:pot\",\n            \"en:seal\"\n        ],\n        \"packaging_tags\": [\n            \"en:plastic\",\n            \"en:glass\",\n            \"en:jar\",\n            \"en:lid\",\n            \"fr:couvercle-en-plastique\",\n            \"fr:opercule-aluminium\",\n            \"fr:pot-en-verre\"\n        ],\n        \"packagings\": [\n            {\n                \"food_contact\": 0,\n                \"material\": \"en:plastic\",\n                \"shape\": \"en:lid\"\n            },\n            {\n                \"food_contact\": 1,\n                \"material\": \"en:aluminium\",\n                \"shape\": \"en:seal\"\n            },\n            {\n                \"food_contact\": 1,\n                \"material\": \"en:glass\",\n                \"shape\": \"en:pot\"\n            },\n            {\n                \"food_contact\": 1,\n                \"shape\": \"en:jar\"\n            }\n        ],\n        \"packagings_materials\": {\n            \"all\": {\n\n            },\n            \"en:glass\": {\n\n            },\n            \"en:metal\": {\n\n            },\n            \"en:plastic\": {\n\n            },\n            \"en:unknown\": {\n\n            }\n        },\n        \"packagings_n\": 4,\n        \"photographers\": [\n\n        ],\n        \"photographers_tags\": [\n            \"openfoodfacts-contributors\"\n        ],\n        \"pnns_groups_1\": \"Sugary snacks\",\n        \"pnns_groups_1_tags\": [\n            \"sugary-snacks\",\n            \"known\"\n        ],\n        \"pnns_groups_2\": \"Sweets\",\n        \"pnns_groups_2_tags\": [\n            \"sweets\",\n            \"known\"\n        ],\n        \"popularity_key\": 1,\n        \"popularity_tags\": [\n            \"at-least-5-scans-2019\",\n            \"top-75-percent-scans-2019\",\n            \"top-80-percent-scans-2019\",\n            \"top-85-percent-scans-2019\",\n            \"top-90-percent-scans-2019\",\n            \"top-100000-fr-scans-2019\",\n            \"top-country-fr-scans-2019\",\n            \"at-least-5-fr-scans-2019\",\n            \"bottom-25-percent-scans-2020\",\n            \"bottom-20-percent-scans-2020\",\n            \"bottom-15-percent-scans-2020\",\n            \"top-90-percent-scans-2020\",\n            \"top-country-fr-scans-2020\"\n        ],\n        \"product_name\": \"Nutella\",\n        \"product_name_fr\": \"Nutella\",\n        \"product_name_fr_debug_tags\": [\n\n        ],\n        \"product_quantity\": \"750\",\n        \"product_quantity_unit\": \"g\",\n        \"product_type\": \"food\",\n        \"purchase_places\": \"France\",\n        \"purchase_places_debug_tags\": [\n\n        ],\n        \"purchase_places_tags\": [\n            \"france\"\n        ],\n        \"quantity\": \"750 g\",\n        \"quantity_debug_tags\": [\n\n        ],\n        \"removed_countries_tags\": [\n\n        ],\n        \"rev\": 20,\n        \"scans_n\": 1,\n        \"schema_version\": 996,\n        \"selected_images\": {\n            \"front\": {\n                \"display\": {\n                    \"fr\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.400.jpg\"\n                },\n                \"small\": {\n                    \"fr\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.200.jpg\"\n                },\n                \"thumb\": {\n                    \"fr\": \"https://images.openfoodfacts.org/images/products/301/762/042/5400/front_fr.11.100.jpg\"\n                }\n            }\n        },\n        \"serving_size_debug_tags\": [\n\n        ],\n        \"sortkey\": 1610107187,\n        \"states\": \"en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed, en:packaging-code-to-be-completed, en:characteristics-to-be-completed, en:origins-to-be-completed, en:categories-completed, en:brands-completed, en:packaging-completed, en:quantity-completed, en:product-name-completed, en:photos-to-be-validated, en:packaging-photo-to-be-selected, en:nutrition-photo-to-be-selected, en:ingredients-photo-to-be-selected, en:front-photo-selected, en:photos-uploaded\",\n        \"states_hierarchy\": [\n            \"en:to-be-completed\",\n            \"en:nutrition-facts-completed\",\n            \"en:ingredients-completed\",\n            \"en:expiration-date-to-be-completed\",\n            \"en:packaging-code-to-be-completed\",\n            \"en:characteristics-to-be-completed\",\n            \"en:origins-to-be-completed\",\n            \"en:categories-completed\",\n            \"en:brands-completed\",\n            \"en:packaging-completed\",\n            \"en:quantity-completed\",\n            \"en:product-name-completed\",\n            \"en:photos-to-be-validated\",\n            \"en:packaging-photo-to-be-selected\",\n            \"en:nutrition-photo-to-be-selected\",\n            \"en:ingredients-photo-to-be-selected\",\n            \"en:front-photo-selected\",\n            \"en:photos-uploaded\"\n        ],\n        \"states_tags\": [\n            \"en:to-be-completed\",\n            \"en:nutrition-facts-completed\",\n            \"en:ingredients-completed\",\n            \"en:expiration-date-to-be-completed\",\n            \"en:packaging-code-to-be-completed\",\n            \"en:characteristics-to-be-completed\",\n            \"en:origins-to-be-completed\",\n            \"en:categories-completed\",\n            \"en:brands-completed\",\n            \"en:packaging-completed\",\n            \"en:quantity-completed\",\n            \"en:product-name-completed\",\n            \"en:photos-to-be-validated\",\n            \"en:packaging-photo-to-be-selected\",\n            \"en:nutrition-photo-to-be-selected\",\n            \"en:ingredients-photo-to-be-selected\",\n            \"en:front-photo-selected\",\n            \"en:photos-uploaded\"\n        ],\n        \"stores\": \"\",\n        \"stores_debug_tags\": [\n\n        ],\n        \"stores_tags\": [\n\n        ],\n        \"teams\": \"pain-au-chocolat,shark-attack,stakano,chocolatine\",\n        \"teams_tags\": [\n            \"pain-au-chocolat\",\n            \"shark-attack\",\n            \"stakano\",\n            \"chocolatine\"\n        ],\n        \"traces\": \"\",\n        \"traces_debug_tags\": [\n\n        ],\n        \"traces_from_ingredients\": \"\",\n        \"traces_from_user\": \"(fr) \",\n        \"traces_hierarchy\": [\n\n        ],\n        \"traces_tags\": [\n\n        ],\n        \"unique_scans_n\": 1,\n        \"unknown_ingredients_n\": 0,\n        \"unknown_nutrients_tags\": [\n\n        ],\n        \"update_key\": \"brands\",\n        \"vitamins_prev_tags\": [\n\n        ],\n        \"vitamins_tags\": [\n\n        ]\n    },\n    \"status\": 1,\n    \"status_verbose\": \"product found\"\n}\n \n\n\nPour en faire une information exploitable, il est nécessaire de retraiter le résultat de la requête. Par exemple, pour n’extraire que le libellé et le nutriscore d’un produit, ainsi que son indice de transformation NOVA, il faut utiliser une boucle sur les différentes caractéristiques à extraire :\n\ndf &lt;- data.frame(\n  lapply(c(\"product_name\",\"nova_groups\",\"nutriscore_grade\"), function(x){\n    jsonlite::fromJSON(resultats, flatten = TRUE)$product[[x]]\n  })\n)\ncolnames(df) &lt;- c(\"product_name\",\"nova_groups\",\"nutriscore_grade\")\ndf\n\n  product_name nova_groups nutriscore_grade\n1      Nutella           4                e\n\n\n\n16.3.3 Accès à une API avec jeton\nPour les API protégées par des jetons, il faut rajouter un paramètre d’identification. Les jetons d’authentification (token) étant des informations personnelles, il ne faut pas les faire figurer dans un script. Comme expliqué précédemment, ils peuvent être stockés sous forme de variable d’environnement, par exemple sous le nom MON_JETON_SECRET. Il suffit alors d’utiliser Sys.getenv pour récupérer la valeur derrière le nom MON_JETON_SECRET :\n\njeton &lt;- Sys.getenv(\"MON_JETON_SECRET\") # création d'une variable contenant le jeton\n\nauth_header &lt;- httr::add_headers('Authorization'= paste('Bearer',jeton)) # création d'une variable d'authentification\n\nres &lt;- httr::content(httr::GET(url),\n                     auth_header, # ajout de la variable d'authentification\n                     as=\"text\", \n                     httr::content_type_json(), \n                     encoding='UTF-8')\n\n\n16.3.4 Le package jsonlite\n\nCe package propose principalement la fonction fromJSON qui permet de convertir une résultat en format json en un objet R.\n\n16.3.5 Temporisation\nPour l’utilisation d’API avec un jeton, comme celle proposées sur le catalogue des API de l’Insee, le nombre de requête par minute est limité (30 pour un compte standard sur le catalogue des API). Pour ne pas être bloqué par cette limite, il est important de temporiser les appels successifs en introduisant une latence. La fonction permettant cela est Sys.sleep. Par exemple, pour laisser 30 secondes d’attente, taper Sys.sleep(30).\nExemple d’utilisation sur l’API interne RMèS\nUne API interne sur les métadonnées de l’Insee est disponible (programme RMès). Elle permet d’obtenir de manière simplifiée des métadonnées (sources, concepts, liste de codes géographiques). Comme cette API est disponible en interne à l’Insee, il n’y a pas, comme pour accéder à celles sur internet, de proxy.\nVoici un exemple de requête pour accéder aux données au niveau division de la Naf-rev2 :\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(httr)\nlibrary(jsonlite)\n\n# L'API est disponible en interne. Il n'y a pas de proxy pour cette API\nset_config(use_proxy(\"\"))\n\n# url de la requête : ici on souhaite afficher la naf rev 2\nurl &lt;- \"url_de_la_requete\"\n\n\n# connexion à l'API pour récupérer les données en JSON\nres &lt;- content(GET(url),\n                     as=\"text\", content_type_json(), encoding='UTF-8')\n\n# transformation des données pour les transformer en dataframe\nres_ok &lt;- as.data.frame(fromJSON(res))\n\n# travail de la table pour obtenir le niveau division de la NAF-rev2\ndivision &lt;- res_ok %&gt;% \n  filter(str_detect(uri,'division')) %&gt;% \n  select(c('code', 'intituleFr'))\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nL’URL de cette API est uniquement disponible en interne à l’Insee. Elle n’est pas rendu publique pour des raisons de sécurité.\n\n\n\n16.3.6 Exemple d’utilisation de l’API Sirene\n\nCet exemple va aller chercher les liens de succession pour un établissement :\n\nlibrary(apinsee)\nlibrary(httr)\nlibrary(jsonlite)\n\nurl &lt;- \"https://api.insee.fr/entreprises/sirene/V3/siret/liensSuccession?q=siretEtablissementPredecesseur:39478192600016\"\nurl &lt;- URLencode(url, reserved = TRUE)\ntoken &lt;- insee_auth()\nset_config(config(token = token))\nres &lt;- content(GET(url, config(token = token)), \n               as=\"text\", \n               content_type_json(), \n               encoding='UTF-8')\n\nres &lt;- fromJSON(res)\nsortie &lt;- as.data.frame(res$liensSuccession)\n\nL’API Sirene permet d’effectuer des recherches multicritères. Dans ce cas, il faut séparer les codes par %20OR%20 (code HTML signifiant OR) :\n\nurl &lt;- \"https://api.insee.fr/entreprises/sirene/V3/siret/liensSuccession?q=siretEtablissementPredecesseur%3A39478192600016%20OR%20siretEtablissementPredecesseur%3A39488939800027\"\ntoken &lt;- apinsee::insee_auth()\nset_config(config(token = token))\nres &lt;- content(httr::GET(url, httr::config(token = token)), \n               as=\"text\", \n               content_type_json(), \n               encoding='UTF-8')\n\nres&lt;-fromJSON(res)\nsortie &lt;- as.data.frame(res$liensSuccession)",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Travailler avec des API</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html",
    "title": "17  Se connecter à une base de données",
    "section": "",
    "text": "17.1 Tâches concernées et recommandations\nL’utilisateur souhaite accéder à des données stockées dans une base de données (sous forme Oracle, PostgreSQL, etc.). Il veut effectuer de la manipulation de données, mais également calculer des indicateurs, réaliser des estimations et en récupérer le résultat.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#tâches-concernées-et-recommandations",
    "title": "17  Se connecter à une base de données",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\n\nIl est recommandé d’utiliser le package DBI qui fournit une boîte à outils génériques pour de multiples bases de données. Ce package doit être complété par un autre package spécifique au type de base de données concernées :\n\npour une base de données de type Oracle : ROracle ;\npour une base de données de type PostgreSQL : RPostgres (ou odbc pour les utilisateurs qui souhaitent visualiser l’architecture de la base de données dans RStudio).\n\n\nLes utilisateurs débutants peuvent utiliser le package dbplyr pour se familiariser avec les requêtes SQL.\nL’usage de ces packages peut être conditionné à l’installation préalable de librairies système sur lesquelles l’utilisateur standard n’a pas la main. Il faudra le cas échéant contacter les services informatiques pour obtenir l’installation de ces librairies système.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#rappel-des-notions-essentielles-sur-les-bases-de-données",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#rappel-des-notions-essentielles-sur-les-bases-de-données",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.2 Rappel des notions essentielles sur les bases de données",
    "text": "17.2 Rappel des notions essentielles sur les bases de données\n\n17.2.1 Structure de bases de données\nLes systèmes de gestion de bases de données (ou SGBD) sont des logiciels permettant de stocker, gérer, manipuler et partager des informations organisées d’une certaine manière (la manière la plus courante étant l’organisation sous forme de tables à deux dimensions - lignes et colonnes - appelée bases de données relationnelles). Cette gestion de l’information se fait de manière transparente pour l’utilisateur, et vise en général à optimiser les opérations sur des bases de données de grande taille.\nDe ce point de vue, le logiciel de gestion de bases de données est indépendant, au sens où il n’est pas dépendant de la forme que prendra le logiciel permettant à l’utilisateur de consulter et de manipuler l’information de la base de données. Il faut ainsi distinguer deux notions essentielles :\n\nla notion de client : schématiquement la machine sur laquelle l’utilisateur effectue des requêtes visant à consulter, manipuler, modifier la base de données ;\nla notion de serveur : schématiquement la machine sur laquelle la base de données se situe et qui va se charger d’exécuter les requêtes que le client lui adresse.\n\nRien n’interdit que le client et le serveur soient la même machine (bien que ce ne soit pas la pratique courante). Néanmoins, cette distinction est essentielle pour la transparence des opérations qu’effectuent le SGBD. Ainsi, plusieurs clients de différentes formes, avec des caractéristiques techniques différentes, peuvent interroger la même base de données. Pour le dire plus simplement dans le contexte de l’Insee, il est possible de se connecter sur une même base de données avec différents logiciels (qui sont ici les clients), tels que SAS, R, Python ou d’autres logiciels spécifiquement dédiés au requêtage et à la gestion de bases de données (PgAdmin, DBeaver, etc.). C’est là tout l’intérêt du système de gestion de bases de données : il permet de s’affranchir des questions du support d’enregistrement de l’information, en offrant une solution accessible par différents logiciels, rendant ainsi le format d’enregistrement agnostique vis-à-vis du langage utilisé, et en parallèle permettant de séparer la manipulation des données et l’analyse qui en est faite.\nAinsi, une requête est une commande plus ou moins complexe permettant de manipuler, de transformer et/ou de générer des données stockées dans la base de données gérées par le SGBD auquel s’adresse la requête. Cette requête est adressée au serveur par le client. Elle peut ou non aboutir à une transmission d’information du serveur vers le client. Là encore, l’intérêt d’une requête s’adressant à un serveur distinct du client est qu’elle permet à l’utilisateur de s’affranchir de toutes les questions qui peuvent surgir lorsqu’on cherche à manipuler un volume de données important. Ces questions sont particulièrement prégnantes sur des logiciels comme R, qui sont très exigeants du point de vue des ressources informatiques. Ainsi, pour une même opération, l’utilisateur aura le choix entre plusieurs approches :\n\nadresser une requête au serveur pour télécharger les données qui l’intéressent, et réaliser les opérations voulues sur le client directement (mais faisant alors face aux problèmes de ressources sus-mentionnés) ;\nadresser une requête au serveur pour que ce dernier accomplisse les opérations directement sur la base de données, et éventuellement compléter la requête de manière à en récupérer le résultat (sous une forme éventuellement agrégée, selon l’usage qu’on souhaite in fine avoir de l’information ainsi transformée) ;\nsoit un mélange des deux, visant à tirer partie à la fois des performances de gestion de gros volumes du serveur, mais également des outils spécifiques (en particulier en matière de statistiques ou d’économétrie) offert par le client.\n\nReste la question de l’interfaçage entre le client et le serveur, c’est-à-dire la façon dont les deux vont communiquer de façon intelligible, supposé être transparent pour l’utilisateur. Cet interfaçage est assuré par un driver du côté client, qui permet à celui-ci d’envoyer des requêtes que le serveur peut interpréter. Il est donc essentiel de s’assurer que ce driver existe du côté client et qu’il est utilisé par le logiciel. En effet, il existe un grand nombre de SGBD, dont les caractéristiques techniques varient, par exemple MySQL, Oracle, Postgres, SQLite, DuckDB, etc. C’est ici que le package DBI entre en jeu. Ce package contient un certain nombre de fonctions génériques permettant de communiquer avec un serveur de base de données, quel que soit le type de base de données en question. Ce package doit être utilisé avec un autre package qui contient le driver correspondant au type de la base de données que l’on souhaite requêter. Dès lors que ce driver existe et est correctement chargé, les fonctions de DBI permettent d’établir la connexion avec le serveur et de lancer des requêtes.\n\n17.2.2 Qu’est-ce que SQL ?\nSQL (pour Structured Query Language ou langage de requête structuré) est un langage informatique spécifiquement dédié à la manipulation, ajout, modification et suppression de données dans un SGBD. Ce langage peut varier dans les fonctions qu’il offre selon la solution technique utilisée (le type de base de données), mais la structure de requête reste globalement la même d’un type de base de données à l’autre. L’utilité du langage SQL vient du fait qu’il permet de formuler des requêtes qui seront exécutées sur le serveur ; sa maîtrise permet à l’utilisateur de déporter sur le serveur la charge de calcul pour la manipulation des données. D’autres solutions existent en R (cf. infra) mais c’est le langage SQL qui offre in fine le plus de possibilités, puisqu’il s’agit du langage natif de requêtage d’une base de données.\nDans les grandes lignes, une requête SQL ressemble généralement à ce qui suit :\n\nSELECT var1, var2\nFROM ma_table\nWHERE var3=1\nORDER BY var1 ;\n\nUn grand nombre de sites internet proposent des tutoriels sur le langage SQL.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#comment-travailler-avec-des-bases-de-données-avec-dbi",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#comment-travailler-avec-des-bases-de-données-avec-dbi",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.3 Comment travailler avec des bases de données avec DBI\n",
    "text": "17.3 Comment travailler avec des bases de données avec DBI\n\nPour interagir avec une base de données avec R, les fonctionnalités se décomposent en deux grands ensembles :\n\nune interface utilisateur (front-end) : le package DBI contient les fonctions nécessaires à l’utilisateur de R pour établir, gérer et terminer la connexion à la base de données, ainsi que pour lancer des requêtes sur la base, quelle que soit la nature de celle-ci. Il s’agit d’un composant indépendant des caractéristiques techniques de la base de données ;\nune interface technique (back-end) : il faut utiliser des drivers qui varient selon le type de la base de données (PostgreSQL, Oracle, MySQL, etc.) et qui permettent de transcrire les commandes issues du front-end d’une manière qui soit intelligible pour la base de données. Ces drivers sont contenus dans des packages (RPostgres, ROracle, odbc).\n\nLe travail sur une base de données avec DBI se déroule toujours en trois temps, détaillés dans la suite de cette fiche :\n\nOn se connecte à la base de données avec la fonction dbConnect ;\nOn envoie des requêtes au serveur. Il y a deux méthodes pour écrire des requêtes :\n\nécrire des requêtes en langage SQL en utilisant les fonctions dbGetQuery et dbSendQuery de DBI ;\nécrire des requêtes selon la syntaxe dplyr en utilisant le package dbplyr ;\n\n\nOn se déconnecte de la base de données à l’aide de la fonction dbDisconnect.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#se-connecter-à-une-base-de-données",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#se-connecter-à-une-base-de-données",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.4 Se connecter à une base de données",
    "text": "17.4 Se connecter à une base de données\nLa fonction dbConnect du package DBI permet d’établir la connexion à la base de données. Si les éléments fournis sont corrects, R va créer un objet de type DBIConnection qu’il faudra ensuite utiliser pour chaque requête effectuée sur la base de données.\n\n17.4.1 Se connecter à une base de données PostgreSQL…\n\n17.4.1.1 … avec le package RPostgres\n\nLe package RPostgres fournit directement les drivers permettant de se connecter à une base de données PostgreSQL. Il s’installe facilement (pas de dépendance de librairies système) et la syntaxe pour une connexion est relativement simple. En revanche, ce package ne permet pas de visualiser facilement l’architecture de la base de données.\n\n\n\n\n\n\nTip\n\n\n\nLorsqu’on découvre une base de données Postgres, il est fréquent de vouloir en visualiser l’architecture (liste des tables, liste des variables, schémas…). RStudio n’est pas très adapté sur ce point. Une très bonne alternative est le logiciel PgAdmin. Son utilisation est simple.\n\n\nPour se connecter à une base de données Postgres avec RPostgres, il faut utiliser la fonction dbConnect de DBI avec les informations suivantes :\n\nl’URL de la base de données (autrement appelé host) : elle la forme d’une adresse mabasededonnees.monsite.fr ou d’une adresse IP 123.123.456.789 ;\nle port de la base de données : par exemple 1983 ;\nle nom de la base de données : nom.basededonnees par exemple ;\nl’identifiant de connexion (autrement appelé user) : monIdep ;\nle mot de passe associé : mdp123.\n\nEn voici un exemple :\n\nconn &lt;- DBI::dbConnect(drv = RPostgres::Postgres(),\n                       host = \"mabasededonnees.monsite.fr\",\n                       port = 1983,\n                       dbname = \"nom.basededonnees\",\n                       user = rstudioapi::askForPassword(\"Nom d'utilisateur:\"),\n                       password = rstudioapi::askForPassword(\"Mot de passe: \"))\n\n\n\n\n\n\n\nTip\n\n\n\nEn général, on se connecte à une base de données à l’aide d’un identifiant et d’un mot de passe. Une bonne pratique consiste à ne pas inscrire ces éléments directement dans le code (en particulier si on veut partager le code en question avec d’autres personnes). Dans ce cas, on pourra utiliser la fonction askForPassword() du package rstudioapi, qui permet d’obtenir une boîte de dialogue interactive pour la saisie de ces éléments.\n\n\n\n17.4.1.2 … avec le package odbc\n\nLe package odbc permet de visualiser dans RStudio l’architecture de la base de données et de lister facilement les schémas, tables et colonnes disponibles. Cela rend de fait beaucoup plus facile le travail de requêtes par la suite. Toutefois, l’usage de ce package peut être rendu complexe car il est dépendant de librairies système qui ne sont pas nécessairement disponibles sur votre poste. Pour plus de détails, il faut consulter cette page, pour le système d’exploitation utilisé. Si vous souhaitez utiliser odbc, vous devrez peut-être contacter les services informatiques pour obtenir l’installation des librairies nécessaires.\nLe détail de la syntaxe est fournie à cette adresse, et dans l’exemple ci-dessus, se formule ainsi :\n\nconnexion &lt;- DBI::dbConnect(odbc::odbc(),\n                            driver = \"PostgreSQL Unicode\"\n                            server = \"mabasededonnees.monsite.fr\",\n                            port = 1983,\n                            database = \"nom.basededonnees\",\n                            uid = rstudioapi::askForPassword(),\n                            pwd = rstudioapi::askForPassword())\n\nL’utilisateur obtient alors la visualisation de la base de données (ici par exemple le clone Fidéli) dans l’onglet Connections en haut à droite :\n\n\n17.4.2 Se connecter à une base de données Oracle\nLe package ROracle permet de se connecter à une base de données Oracle. Il convient de vérifier que les librairies système nécessaires sont bien installées sur le serveur (se reporter ici pour plus de détails).\nLa connexion se réalise de manière très similaire à celle pour une base de données Postgres :\n\nconn &lt;- DBI::dbConnect(drv = ROracle::Oracle(),\n                       host = \"mabasededonnees.monsite.fr\",\n                       port = 1521,\n                       dbname = \"nom.basededonnees\",\n                       user = rstudioapi::askForPassword(\"Nom d'utilisateur:\"),\n                       password = rstudioapi::askForPassword(\"Mot de passe: \"))",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#exécuter-des-requêtes",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#exécuter-des-requêtes",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.5 Exécuter des requêtes",
    "text": "17.5 Exécuter des requêtes\n\n17.5.1 Exécuter des requêtes avec DBI\n\nPour lancer des requêtes sur la base de données, l’utilisateur a la possibilité d’utiliser les fonctions dbSendQuery et dbGetQuery du package DBI. La fonction dbSendQuery demande au serveur PostgreSQL d’exécuter une requête sans renvoyer de résultats. La fonction dbGetQuery demande au serveur PostgreSQL de renvoyer vers R les résultats de la requête. Les deux fonctions ont la même syntaxe : dbSendQuery(conn, requete) :\n\n\nconn est l’objet DBIConnection créé par la fonction dbConnect (voir Se connecter à une base de données) ;\n\nrequete est une requête SQL sous forme de chaîne de caractères (exemple : \"SELECT var2, var3 FROM table1\").\n\nAinsi, pour un utilisateur qui est connecté à la base de données PostgreSQL qui contient le schéma de diffusion - une sorte de librairie SAS - de Fidéli (s_diff_2018) et qui souhaite requêter la table des logements (d_logement) pour compter le nombre de logements par commune et selon le critère du type de local (critère n°2 dans la documentation utilisateur), on peut lancer les requêtes SQL suivantes :\n\nq &lt;- dbSendQuery(conn, \n                 \"create temp table count_log as \n                  select csdep, cne, \n                  case when natloc in ('MA', 'ME', 'AP') then 1 else 0 end as logement \n                  from s_diff_2018.d_logement\")\ndbClearResult(q)\n\ncount_log &lt;- dbGetQuery(conn, \n                        \"select distinct concat(csdep, cne) as code_com,\n                         sum(logement) as nb_logement \n                         from count_log\")\n\nLa première requête crée une table temporaire count_log qui contient le département et la commune de chaque local, ainsi qu’une variable indicatrice indiquant s’il s’agit d’un logement. Cette requête est envoyée au serveur avec dbSendQuery et ne renvoie donc aucun résultat vers R. La création de cette table temporaire nécessite d’avoir les droits minimaux en écriture dans la base.\nLa seconde requête compte le nombre de logements par commune à partir de la table temporaire et renvoie vers R un objet de type data.frame appelé count_log donnant le nombre de logements (nb_logement) par commune (code_com).\nDans le cas où l’utilisateur n’a pas les droits pour créer des tables temporaires, il est toujours possible de procéder à des sous-requêtes (sub-query) en SQL. Cela consiste à enchasser la première requête dans la seconde sans passer par la création d’une table temporaire, de la manière suivante :\n\nq &lt;- dbSendQuery(conn, \n            \"select distinct concat(csdep, cne) as code_com,\n                         sum(logement) as nb_logement \n                         from (select csdep, cne, \n                         case when natloc in ('MA', 'ME', 'AP') then 1 else 0 end as logement \n                         from s_diff_2018.d_logement) as a\")\n\ndbFetch(q)\ndbClearResult(q)\n\n\n\n\n\n\n\nNote\n\n\n\nIl est fréquent que les bases de données contiennent des données volumineuses, dont le téléchargement et le traitement peuvent dépasser les capacités de votre poste local. C’est pourquoi il est recommandé d’éviter de télécharger les données brutes et de réaliser les traitements en R. Dans la mesure du possible, il vaut mieux faire exécuter les traitements par la base de données, et ne récupérer en R qu’un résultat agrégé.\nLe code donné en exemple ci-dessus réalise l’intégralité du traitement statistique sur le serveur. Cela permet d’obtenir un résultat agrégé exploitable en R (pour faire de la cartographie, de l’économétrie ou générer des tables de résultats), tout en limitant la taille des données échangées entre le serveur et R et la quantité de calculs réalisés par R.\n\n\n\n17.5.2 Exécuter des requêtes avec dbplyr\n\nIl n’est pas indispensable de connaître SQL pour manipuler des données avec une connexion à une base de données sur R. En effet, le package dbplyr permet de manipuler des tables dans une base de données avec une syntaxe très similaire à celle du package dplyr. Ainsi, pour reprendre l’exemple du décompte des logements dans le clone Fidéli, il convient dans un premier temps de référencer l’existence de cette table en mémoire de R :\n\nlibrary(dplyr)\nlibrary(dbplyr)\nd_logement &lt;- tbl(conn, in_schema(\"s_diff_2018\", \"d_logement\"))\n\nUne fois la référence créée, on peut manipuler les données de la table d_logement avec la syntaxe dplyr comme si cette table était un data.frame standard. La différence entre dplyr et dbplyr est que les instructions sont automatiquement converties en requêtes SQL et envoyées au serveur. Il est à noter que les requêtes ne sont pas exécutées immédiatement, mais uniquement lorsque c’est nécessaire (il s’agit du concept de lazy evaluation sur lequel repose le package dplyr). Ainsi pour poursuivre l’exemple :\n\ncount_log &lt;- d_logement %&gt;%\n  mutate(logement = ifelse(natloc %in% c('MA', 'ME', 'AP'), 1, 0)) %&gt;%\n  select(csdep, cne, logement) %&gt;%\n  compute()\n\ncount_log &lt;- count_log %&gt;%\n  group_by(csdep, cne) %&gt;%\n  summarise(nb_logement = sum(logement)) %&gt;%\n  collect()\n\nDans cet exemple, la nature de l’objet count_log change entre la première et la seconde instruction. Au départ, il s’agit d’une interprétation de l’instruction sous forme dplyr en requête SQL (un objet sql_tbl) stockée dans une table temporaire grâce à la commande compute, qui ensuite devient un objet data.frame suite à la commande collect (qui peut se voir comme un équivalent de la commande dbGetQuery, en plus large puisqu’elle déclenche la soumission de la requête SQL).\nLe package dbplyr présente l’avantage d’offrir une syntaxe très proche de celle de dplyr. Cette syntaxe peut néanmoins, comme indiqué dans la documentation, présenter des limites dans l’interprétation des commandes en requêtes SQL. L’utilisateur devra être particulièrement attentif à ce point.\nL’enchâssement d’une sous-requête dans une requête SQL est très adapté au pipe de la syntaxe dplyr, et se traduit très naturellement ainsi :\n\ncount_log &lt;- d_logement %&gt;%\n  mutate(logement = ifelse(natloc %in% c('MA', 'ME', 'AP'), 1, 0)) %&gt;%\n  select(csdep, cne, logement) %&gt;%\n  group_by(csdep, cne) %&gt;%\n  summarise(nb_logement = sum(logement)) %&gt;%\n  collect()\n\n\n\n\n\n\n\nTip\n\n\n\nComme indiqué ci-dessus, le package dbplyr convertit automatiquement vos instructions en requête SQL. Il est possible d’afficher cette requête avec la fonction show_query. Cela vous permet de vous familiariser avec le langage SQL, et de voir que la requête SQL générée par dbplyr est souvent loin d’être optimale.\n\n\n\n17.5.3 Comparaison de DBI et de dbplyr\n\nL’utilisation des fonctions dbSendQuery et dbGetQuery du package DBI permet de profiter pleinement des avantages du langage SQL :\n\nindépendant de R : les requêtes SQL qui ont été élaborées en R pourront être aisément réutilisées dans un autre contexte (en Python par exemple) ;\npuissance du langage : les requêtes écrites directement en SQL permettent d’utiliser toutes les fonctions de SQL pour le traitement des données par la base de données.\n\nLe principal inconvénient de DBI et du langage SQL est que les détails du langage SQL peuvent varier légèrement d’un type de base de données à l’autre (MySQL, SQLite, Postgres, DuckDB…), ce qui peut entraîner des confusions et des bugs (par exemple si on reprend un exemple trouvé sur internet).\nLe package dbplyr a pour avantage de proposer une syntaxe simple, très proche de dplyr, ce qui réduit le coût d’apprentissage. Par ailleurs ce package couvre l’essentiel des besoins standards en matière de base de données. Il a toutefois deux inconvénients : ce package ralentit l’apprentissage de SQL par l’utilisateur et empêche de réaliser des requêtes complexes réservées aux agents maîtrisant SQL. Par ailleurs, la syntaxe de dbplyr est spécifique à R et à l’écosystème tidyverse, créant ainsi une dépendance au langage.\n\n\n\n\n\n\nTip\n\n\n\nSi vous découvrez le langage SQL et le travail sur des bases de données, le package dbplyr peut vous aider à être rapidement opérationnel. Toutefois, il est vivement recommandé d’apprendre SQL en parallèle.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#se-déconnecter-avec-dbi",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#se-déconnecter-avec-dbi",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.6 Se déconnecter avec DBI\n",
    "text": "17.6 Se déconnecter avec DBI\n\nIl est recommandé de se déconnecter de la base de données une fois que l’ensemble des traitements a été effectué, de la manière suivante :\n\ndbDisconnect(conn)",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#quelques-bonnes-pratiques",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#quelques-bonnes-pratiques",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.7 Quelques bonnes pratiques",
    "text": "17.7 Quelques bonnes pratiques\nEn général, on se connecte à une base de données à l’aide d’un identifiant et d’un mot de passe. Une bonne pratique consiste à ne pas inscrire ces éléments directement dans le code (en particulier si on veut partager le code en question avec d’autres personnes). Dans ce cas, on pourra utiliser la fonction askForPassword() du package rstudioapi, qui permet d’obtenir une boîte de dialogue interactive pour la saisie de ces éléments. Une autre façon de procéder est d’utiliser un fichier .Renviron pour disposer de ces informations sous forme de variables d’environnement.\nIl est fréquent que les bases de données contiennent des données volumineuses, dont le téléchargement et le traitement peuvent dépasser les capacités de votre poste local. C’est pourquoi il est recommandé d’éviter de télécharger les données brutes et de réaliser les traitements en R. Dans la mesure du possible, il vaut mieux faire exécuter les traitements par la base de données, et ne récupérer en R qu’un résultat agrégé (ceci nécessite parfois d’avoir des droits en écriture sur la base de données).",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#sources",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#sources",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.8 Sources",
    "text": "17.8 Sources\nCette fiche reprend partiellement ce tutoriel RStudio (en anglais).",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_connexion_bdd.html#RessourcesBDD",
    "href": "03_Fiches_thematiques/Fiche_connexion_bdd.html#RessourcesBDD",
    "title": "17  Se connecter à une base de données",
    "section": "\n17.9 Pour en savoir plus",
    "text": "17.9 Pour en savoir plus\n\nla documentation de RStudio sur la connexion aux bases de données (en anglais) :\n\nune introduction aux bases de données ;\nune introduction à DBI (en anglais) ;\nla documentation associée à Postgres ;\n\n\nla documentation du package DBI (en anglais) ;\nle site de RStudio sur DBI ;\nla documentation du package RPostgres (en anglais) ;\nla documentation au sujet du package dbplyr.",
    "crumbs": [
      "Introduction",
      "Importer des données avec R",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Se connecter à une base de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_tidyverse.html",
    "href": "03_Fiches_thematiques/Fiche_tidyverse.html",
    "title": "18  Manipuler des données avec le tidyverse",
    "section": "",
    "text": "18.1 Tâches concernées et recommandations\nL’utilisateur souhaite manipuler des données structurées sous forme de data.frame (sélectionner des variables, sélectionner des observations, créer des variables, joindre des tables, résumer l’information).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Manipuler des données avec le `tidyverse`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_tidyverse.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_tidyverse.html#tâches-concernées-et-recommandations",
    "title": "18  Manipuler des données avec le tidyverse",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nPour des tables de données de taille petite et moyenne (inférieure à 1 Go ou moins d’un million d’observations), il est recommandé d’utiliser les packages tibble, dplyr et tidyr qui font l’objet de la présente fiche ;\nPour des tables de données de grande taille (plus de 1 Go ou plus d’un million d’observations), il est recommandé d’utiliser soit le package data.table présenté dans la fiche Manipuler des données avec data.table, soit les packages arrow et duckdb présentés dans les fiches Manipuler des données avec arrow et Manipuler des données avec duckdb.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCertains exemples de cette fiche utilisent les données disponibles dans le package doremifasolData ; vous ne pourrez reproduire ces exemples que si ce package est installé sur la machine sur laquelle vous travaillez. Si vous ne savez pas si ce package est déjà installé, consultez la fiche Comment utiliser la documentation utilitR.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Manipuler des données avec le `tidyverse`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_tidyverse.html#présentation-des-packages-dplyr-tidyr-et-tibble",
    "href": "03_Fiches_thematiques/Fiche_tidyverse.html#présentation-des-packages-dplyr-tidyr-et-tibble",
    "title": "18  Manipuler des données avec le tidyverse",
    "section": "\n18.2 Présentation des packages dplyr, tidyr et tibble\n",
    "text": "18.2 Présentation des packages dplyr, tidyr et tibble\n\n\n18.2.1 Introduction\nLes packages dplyr, tidyr et tibble font partie du tidyverse, une suite de packages dont l’objectif est de faciliter la manipulation de tables de données. De façon schématique, on peut dire que :\n\nle package tibble propose une nouvelle structure de données qui est une formalisation plus poussée du data.frame (le tibble), et des fonctions pour manipuler les tibbles ;\nle package dplyr propose des fonctions pour manipuler des données qui sont déjà organisées sous forme de table (sélectionner des lignes et des colonnes, calculer des statistiques descriptives) ;\nle package tidyr propose des fonctions pour mettre en ordre des données et pour les organiser sous forme de tables (créer des tables à partir de données brutes ou de listes, décomposer une colonne ou une ligne en plusieurs, restructurer les données en format long ou wide).\n\nCes packages présentent plusieurs avantages :\n\nLa syntaxe est structurée autour d’opérations élémentaires (les verbes), ce qui la rend très lisible ;\nLa grammaire du tidyverse s’inspire du langage SQL pour fournir des commandes dont l’enchaînement est facile à comprendre, même sans faire tourner le code. Par exemple, la ligne matable %&gt;% select(mavariable) %&gt;% summarise() est facilement compréhensible ;\nLes opérations par groupe sont facilitées par l’instruction group_by() ;\nLes noms des colonnes peuvent être appelés directement (pas besoin d’adopter la syntaxe nomtable$nomvariable comme dans la grammaire de base). Là aussi, la lecture du code en est facilitée ;\nPlusieurs verbes peuvent facilement être combinés en utilisant l’opérateur %&gt;% (pipe) qui permet d’enchaîner les instructions (voir le paragraphe Enchaîner les manipulations avec l’opérateur pipe) ;\nLa suite de packages tidyverse couvre la plupart des opérations qu’un statisticien est amené à réaliser (mais pas toutes.\n\nPour se servir de ces packages, il ne faut pas oublier de les charger avec la fonction library.\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\n\nLes exemples de cette fiche s’appuient sur les données disponibles dans le package doremifasolData. On utilise en premier lieu la base permanente des équipements 2018.\n\nlibrary(doremifasolData)\n\nLes objets récupérés en sortie de doremifasolData sont tous des data.frame, bien que le package s’appuie sur des fonctions issues de tidyverse. Il est donc nécessaire de les convertir en tibble avec la fonction as_tibble du package tibble.\n\n18.2.2 Le tibble : un data.frame amélioré\nLe tidyverse propose une version améliorée du data.frame de base : le tibble. Il s’agit d’un data.frame standard, avec quelques propriétés supplémentaires qui rendent son utilisation plus facile. Le package tibble contient par ailleurs des fonctions utiles pour manipuler les tibbles. Sauf exception, toutes les fonctions de R qui manipulent un data.frame fonctionneront sans problème avec un tibble.\nPour convertir un data.frame en tibble on utilise la fonction tibble::as_tibble(). Dans l’exemple suivant, on charge la table de la base permanente des équipements puis on la convertit en tibble.\n\n# Charger la base permanente des équipements\nbpe_ens_2018 &lt;- doremifasolData::bpe_ens_2018\n# Convertir ce data.frame en tibble\nbpe_ens_2018_tbl &lt;- as_tibble(bpe_ens_2018)\n\nIl y a deux différences principales entre un tibble et un data.frame :\n\n\nLa sélection (subsetting) fonctionne différemment : sélectionner une colonne dans un data.frame renvoie un vecteur, alors que sélectionner une colonne dans un tibble renvoie un tibble à une seule colonne. Si on veut vraiment récupérer un vecteur à partir d’un tibble à une colonne, on utilise la fonction pull.\n\n# Si on sélectionne une colonne dans un data.frame,\n# on obtient un vecteur.\nbpe_ens_2018[ , 1]\n\n [1] \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\"\n [ reached getOption(\"max.print\") -- omitted 1035554 entries ]\n\n\n\n# Si on sélectionne une colonne dans un tibble,\n# on obtient un tibble avec une seule colonne.\nbpe_ens_2018_tbl[ , 1]\n\n# A tibble: 1,035,564 × 1\n  REG  \n  &lt;chr&gt;\n1 84   \n2 84   \n3 84   \n4 84   \n5 84   \n6 84   \n# ℹ 1,035,558 more rows\n\n\n\npull(bpe_ens_2018_tbl[ , 1])\n\n [1] \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\" \"84\"\n [ reached getOption(\"max.print\") -- omitted 1035554 entries ]\n\n\n\nL’affichage des tibbles est meilleur que celui des data.frames. Vous pouvez par exemple remarquer que même sans la fonction head(), l’affichage d’un tibble affiche toujours les dimensions de celui-ci (nombres de lignes et de colonnes), ainsi que le type des variables (en-dessous des noms des colonnes).\n\n\n# Afficher les premières lignes d'un dataframe\nhead(bpe_ens_2018)\n\n  REG DEP DEPCOM DCIRIS   AN TYPEQU NB_EQUIP\n1  84  01  01001  01001 2018   A401        2\n [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]\n\n\n\n# Afficher les premières lignes d'un tibble\nbpe_ens_2018_tbl\n\n# A tibble: 1,035,564 × 7\n  REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 84    01    01001  01001   2018 A401          2\n2 84    01    01001  01001   2018 A404          4\n3 84    01    01001  01001   2018 A504          1\n4 84    01    01001  01001   2018 A507          1\n5 84    01    01001  01001   2018 B203          1\n6 84    01    01001  01001   2018 C104          1\n# ℹ 1,035,558 more rows\n\n\n\n18.2.3 Comment utiliser les fonctions du tidyverse\n\nLes fonctions (ou verbes) du tidyverse partagent quatre caractéristiques :\n\nElles prennent toujours un tibble ou un data.frame en entrée et renvoient toujours un tibble ou un data.frame en sortie ;\nElles ont toujours la table de données pour premier argument : verbe(tibble ou data.frame, ...) ;\nElles renvoient toujours une copie de la table de données fournie en entrée et ne modifient jamais les données d’entrée, sauf si on le demande explicitement avec l’opérateur d’assignation &lt;- (voir remarque) ;\nLes noms de variables peuvent être cités sans guillemets (sauf dans le cas des jointures).\n\n\n\n\n\n\n\nNote\n\n\n\nPar défaut, les fonctions du tidyverse renvoient une copie des données manipulées. Cela signifie que vous devez utiliser l’opérateur d’assignation &lt;- si vous voulez modifier une table. Voici deux exemples simples pour bien comprendre la différence.\nLe premier code sélectionne la variable TYPEEQ dans la table bpe_ens_2018_tbl et renvoie un data.frame contenant uniquement cette variable. En revanche, la table bpe_ens_2018_tbl n’est pas modifiée.\n\nselect(bpe_ens_2018_tbl, TYPEQU)\n\nLe second code modifie la table bpe_ens_2018_tbl en utilisant l’opérateur &lt;- :\n\nbpe_ens_2018_tbl &lt;- select(bpe_ens_2018_tbl, TYPEQU)\n\nCe fonctionnement des fonctions du tidyverse a pour avantage qu’il est difficile d’écraser ses données par mégarde. Il a pour inconvénient d’être gourmand en mémoire vive (RAM) car les données sont temporairement dupliquées. C’est pour cette raison que le tidyverse n’est pas adapté à la manipulation de données volumineuses.\n\n\n\n18.2.4 Enchaîner les manipulations avec l’opérateur pipe\n\n\n18.2.4.1 Présentation de l’opérateur pipe\n\nLorsqu’on enchaîne les manipulations sur une table de données, un problème est que le code devient peu lisible car il y a beaucoup d’opérations imbriquées les unes dans les autres, avec un grand nombre de parenthèses ou de crochets. L’opérateur pipe (noté %&gt;%) du package magrittr permet de résoudre ce problème en réécrivant les opérations de façon plus lisible. Le principe de l’opérateur pipe est très simple :\n\nle terme qui précède l’opérateur est utilisé comme premier argument de la fonction qui suit l’opérateur ;\nles opérations peuvent être enchaînées en enchaînant les opérateurs pipe ;\nl’opérateur pipe fonctionne quelle que soit la nature de l’argument ;\nl’opérateur pipe fonctionne également à l’intérieur de parenthèses.\n\nVoici un petit tableau qui vous donne des exemples :\n\n\n\n\n\n\nCe code est équivalent à…\n… ce code\n\n\n\nfonction(x)\nx %&gt;% fonction()\n\n\nfonction3(fonction2(fonction1(x)))\nx %&gt;% fonction1() %&gt;% fonction2() %&gt;% fonction3()\n\n\nmutate(tibble, y = log(x))\ntibble %&gt;% mutate(y = x %&gt;% log())\n\n\n\n18.2.4.2 Comment utiliser l’opérateur pipe avec le tidyverse\n\nUn traitement statistique avec les packages du tidyverse prend généralement la forme d’une succession de verbes séparés par l’opérateur pipe (%&gt;%). Il est possible d’aller à la ligne en mettant le pipe en bout de ligne (mais pas en début de ligne).\nVoici un exemple détaillé pour comprendre l’utilisation du pipe. Toutes les fonctions utilisées sont présentées ailleurs dans cette fiche. Ce code se lit comme ceci : on part de la base permanente des équipements 2018, puis on la transforme en tibble, puis on conserve uniquement les stations services TYPEQU == \"B316\", puis on groupe les observations par département group_by(DEP), puis on calcule la somme du nombre de stations-services par département summarise(nb_equip_total = sum(NB_EQUIP, na.rm = TRUE)).\n\nnombre &lt;- bpe_ens_2018 %&gt;%\n  as_tibble() %&gt;%\n  filter(TYPEQU == \"B316\") %&gt;% \n  group_by(DEP) %&gt;% \n  summarise(nombre_station_serv = sum(NB_EQUIP, na.rm = TRUE)) \nnombre\n\n# A tibble: 96 × 2\n  DEP   nombre_station_serv\n  &lt;chr&gt;               &lt;dbl&gt;\n1 01                    110\n2 02                     91\n3 03                     78\n4 04                     48\n5 05                     43\n6 06                    148\n# ℹ 90 more rows",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Manipuler des données avec le `tidyverse`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_tidyverse.html#manipuler-des-tables-de-données-avec-dplyr",
    "href": "03_Fiches_thematiques/Fiche_tidyverse.html#manipuler-des-tables-de-données-avec-dplyr",
    "title": "18  Manipuler des données avec le tidyverse",
    "section": "\n18.3 Manipuler des tables de données avec dplyr\n",
    "text": "18.3 Manipuler des tables de données avec dplyr\n\nLe package dplyr permet de manipuler facilement des données organisées sous forme de table, c’est-à-dire comprenant une colonne par variable et une ligne par observation. Si ce n’est pas le cas, vous pouvez utiliser le package tidyr pour organiser vos données (voir la section Mettre en ordre des données avec tidyr).\n\n18.3.1 Manipuler une seule table avec dplyr\n\nDans dplyr, les manipulations simples de données sont résumées en quelques verbes :\n\n\nselect() : sélectionner des variables par leur nom ;\n\nrename() : renommer des variables ;\n\nfilter() : sélectionner des observations selon une ou plusieurs conditions ;\n\narrange() : trier la table selon une ou plusieurs variables ;\n\nmutate() : ajouter des variables qui sont fonction d’autres variables ;\n\nsummarise() : calculer une statistique à partir de données ;\n\ngroup_by() : faire des opérations par groupe.\n\n\n18.3.1.1 Sélectionner des variables : select()\n\nLa fonction select() permet de sélectionner des variables par leur nom, ou par une condition sur leur nom. Cette fonction est principalement utilisée de deux façons :\n\n\nAvec une liste de noms de variables. Le code suivant sélectionne le code commune, le type d’équipement et le nombre d’équipement dans la base permanente des équipements :\n\nbpe_ens_2018_tbl %&gt;% \n  select(DEPCOM, TYPEQU, NB_EQUIP)\n\n# A tibble: 1,035,564 × 3\n  DEPCOM TYPEQU NB_EQUIP\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 01001  A401          2\n2 01001  A404          4\n3 01001  A504          1\n4 01001  A507          1\n5 01001  B203          1\n6 01001  C104          1\n# ℹ 1,035,558 more rows\n\n\n\n\nAvec la position des colonnes. Le code suivant sélectionne les cinq premières colonnes :\n\nbpe_ens_2018_tbl %&gt;% \n  select(1:5)\n\n# A tibble: 1,035,564 × 5\n  REG   DEP   DEPCOM DCIRIS    AN\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;\n1 84    01    01001  01001   2018\n2 84    01    01001  01001   2018\n3 84    01    01001  01001   2018\n4 84    01    01001  01001   2018\n5 84    01    01001  01001   2018\n6 84    01    01001  01001   2018\n# ℹ 1,035,558 more rows\n\n\n\n\nAvec une condition logique. Par exemple, la fonction starts_with(\"DEP\") permet de sélectionner toutes les variables dont le nom commence par “DEP”.\n\nbpe_ens_2018_tbl %&gt;% \n  select(starts_with(\"DEP\"))\n\n# A tibble: 1,035,564 × 2\n  DEP   DEPCOM\n  &lt;chr&gt; &lt;chr&gt; \n1 01    01001 \n2 01    01001 \n3 01    01001 \n4 01    01001 \n5 01    01001 \n6 01    01001 \n# ℹ 1,035,558 more rows\n\n\nLe tableau suivant donne la liste des conditions utilisables avec select() :\n\n\n\n\nFonction\nSignification\n\n\n\nselect(starts_with(\"...\")\ndont le nom commence par “…”\n\n\nselect(ends_with(\"...\")\ndont le nom se termine par “…”\n\n\nselect(contains(\"...\")\ncontient “…”\n\n\nselect(matches(\"...\")\nvérifie une expression régulière (cf. fiche Manipuler des données textuelles\n\n\n\nselect(all_of(...))\nsélectionne les colonnes listées dans un vecteur en paramètre\n\n\nselect(any_of(...))\nidentique à all_of(), mais sans erreur si la colonne n’existe pas\n\n\nselect(everything())\ntoutes les colonnes (utile pour mettre une nouvelle colonne devant les autres)\n\n\n\n\nOn peut aussi renommer des colonnes avec la syntaxe select(data, nouveau_nom = selection_colonne) :\n\n\n  bpe_ens_2018_tbl %&gt;% \n    select(dept = DEP, depcom = DEPCOM)\n\n# A tibble: 1,035,564 × 2\n  dept  depcom\n  &lt;chr&gt; &lt;chr&gt; \n1 01    01001 \n2 01    01001 \n3 01    01001 \n4 01    01001 \n5 01    01001 \n6 01    01001 \n# ℹ 1,035,558 more rows\n\n\n\n18.3.1.2 Renommer des variables : rename()\n\nLa fonction rename() permet de renommer des variables. La syntaxe est la suivante : rename(data, nouveau_nom = ancien nom). Voici un exemple :\n\nbpe_ens_2018_tbl %&gt;% \n  rename(code_commune = DEPCOM)\n\n# A tibble: 1,035,564 × 7\n  REG   DEP   code_commune DCIRIS    AN TYPEQU NB_EQUIP\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 84    01    01001        01001   2018 A401          2\n2 84    01    01001        01001   2018 A404          4\n3 84    01    01001        01001   2018 A504          1\n4 84    01    01001        01001   2018 A507          1\n5 84    01    01001        01001   2018 B203          1\n6 84    01    01001        01001   2018 C104          1\n# ℹ 1,035,558 more rows\n\n\nLa fonction rename_with() permet de renommer un groupe de colonnes avec une fonction. La syntaxe est la suivante : rename_with(data, nom_fonction, selection_colonnes). Voici un exemple qui met en minuscules tous les noms de colonnes :\n\nbpe_ens_2018_tbl %&gt;% \n  rename_with(tolower)\n\n# A tibble: 1,035,564 × 7\n  reg   dep   depcom dciris    an typequ nb_equip\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 84    01    01001  01001   2018 A401          2\n2 84    01    01001  01001   2018 A404          4\n3 84    01    01001  01001   2018 A504          1\n4 84    01    01001  01001   2018 A507          1\n5 84    01    01001  01001   2018 B203          1\n6 84    01    01001  01001   2018 C104          1\n# ℹ 1,035,558 more rows\n\n\n\n18.3.1.3 Sélectionner des observations : filter()\n\nLe verbe filter() permet de sélectionner des observations selon une ou plusieurs conditions logiques. Voici un exemple de code qui sélectionne les magasins de chaussures (TYPEQU == \"B304\") dans le premier arrondissement de Paris (DEPCOM == \"75101\") dans la BPE.\n\nbpe_ens_2018_tbl %&gt;% \n  filter(DEPCOM == \"75101\" & TYPEQU == \"B304\")\n\n# A tibble: 12 × 7\n  REG   DEP   DEPCOM DCIRIS        AN TYPEQU NB_EQUIP\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 11    75    75101  75101_0101  2018 B304          2\n2 11    75    75101  75101_0103  2018 B304          1\n3 11    75    75101  75101_0201  2018 B304         26\n4 11    75    75101  75101_0202  2018 B304          6\n5 11    75    75101  75101_0203  2018 B304         17\n6 11    75    75101  75101_0204  2018 B304         13\n# ℹ 6 more rows\n\n\nVoici quelques utilisations fréquentes de filter() :\n\n\nAction\nCode\n\n\n\nFiltrer sur les modalités qualitatives d’une colonne\nfilter(DEP %in% c(\"75\", \"92\"))\n\n\nFiltrer sur les modalités quantitatives d’une colonne\nfilter(NB_EQUIP == 1)\n\n\nFiltrer sur une variable caractère (voir la fiche données textuelles)\nfilter(str_detect(TYPEQU, \"^A\"))\n\n\nFiltrer sur deux conditions (et)\nfilter(DEP == \"75\" & NB_EQUIP == 1)\n\n\nFiltrer sur une alternative (ou)\nfilter(DEP == \"75\" | NB_EQUIP == 1)\n\n\nConserver les observations pour lesquelles la variable est manquante\nfilter(is.na(pop_2016))\n\n\nConserver les observations pour lesquelles la variable est renseignée\nfilter( !is.na(pop_2016))\n\n\n\n18.3.1.4 Trier la table : arrange()\n\nLe verbe arrange() permet de trier les observations de la table selon une ou plusieurs colonnes. Par défaut, arrange trie par ordre croissant. Il faut utiliser desc(nom_de_variable) pour trier par ordre décroissant. Le code suivant trie la BPE selon le code commune et le type d’équipement.\n\nbpe_ens_2018_tbl %&gt;% \n  arrange(DEPCOM, TYPEQU)\n\n# A tibble: 1,035,564 × 7\n  REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 84    01    01001  01001   2018 A401          2\n2 84    01    01001  01001   2018 A404          4\n3 84    01    01001  01001   2018 A504          1\n4 84    01    01001  01001   2018 A507          1\n5 84    01    01001  01001   2018 B203          1\n6 84    01    01001  01001   2018 C104          1\n# ℹ 1,035,558 more rows\n\n\nVoici quelques utilisations fréquentes de arrange() :\n\n\n\n\n\n\nAction\nCode\n\n\n\nTrier sur une colonne en ordre croissant\narrange(NB_EQUIP)\n\n\nTrier sur plusieurs colonnes en ordre croissant\narrange(DEPCOM, NB_EQUIP)\n\n\nTrier sur une colonne en ordre décroissant\narrange(desc(NB_EQUIP))\n\n\n\nComme il n’est pas nécessaire dans R de trier les tables avant de faire une jointure (merge), le tri est utile surtout pour afficher à l’écran un tableau synthétique de données.\n\n18.3.1.5 Ajouter et modifier des colonnes : mutate()\n\nLa fonction mutate() permet de créer de nouvelles colonnes ou de modifier des colonnes existantes. Il est possible d’utiliser toutes sortes de fonctions à l’intérieur d’une étape mutate(). Le code suivant crée une variable NB_EQUIP_3PLUS qui vaut TRUE si le nombre d’équipement est supérieur ou égal à 3, et FALSE sinon.\n\nbpe_ens_2018_tbl %&gt;% \n  mutate(NB_EQUIP_3PLUS = (NB_EQUIP &gt;= 3))\n\n# A tibble: 1,035,564 × 8\n  REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP NB_EQUIP_3PLUS\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;         \n1 84    01    01001  01001   2018 A401          2 FALSE         \n2 84    01    01001  01001   2018 A404          4 TRUE          \n3 84    01    01001  01001   2018 A504          1 FALSE         \n4 84    01    01001  01001   2018 A507          1 FALSE         \n5 84    01    01001  01001   2018 B203          1 FALSE         \n6 84    01    01001  01001   2018 C104          1 FALSE         \n# ℹ 1,035,558 more rows\n\n\nPour créer une nouvelle variable, on utilise un nom de variable qui n’existe pas encore dans la table. Pour modifier une variable qui existe déjà, on utilise directement le nom de cette variable.\nVoici quelques utilisations fréquentes de mutate() :\n\n\nAction\nCode\n\n\n\nCalculer une somme cumulée\nmutate(NB_EQUIP_CUM = cumsum(NB_EQUIP, na.rm = TRUE))\n\n\nCalculer un total\nmutate(NB_EQUIP_TOT = sum(NB_EQUIP, na.rm = TRUE))\n\n\nSommer deux variables\nmutate(NB_EQUIP_DOUBLE = NB_EQUIP + NB_EQUIP)\n\n\nExtraire une sous-chaine de caractères (voir la fiche données textuelles)\nmutate(CATEGORIE_EQ = str_sub(TYPEQU, 1L, 1L))\n\n\n\n18.3.1.6 Calculer des statistiques : summarise()\n\nLa fonction summarise() permet de calculer une ou plusieurs statistiques à partir de la table de données. Cette fonction est souvent utilisée après la fonction group_by() pour calculer des statistiques par groupe, et elle conduit à une agrégation de la table en fonction des groupes définis par la fonction group_by (par défaut une agrégation sur l’ensemble de la table). Le code suivant calcule le nombre total d’équipements dans la BPE sum(NB_EQUIP, na.rm = TRUE), et le nombre total de boulangeries sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE).\n\nbpe_ens_2018_tbl %&gt;% \n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP, na.rm = TRUE),\n    NB_BOULANGERIES_TOT = sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  NB_EQUIP_TOT NB_BOULANGERIES_TOT\n         &lt;dbl&gt;               &lt;dbl&gt;\n1      2504782               48568\n\n\nIl est possible d’utiliser un grand nombre de fonctions différentes avec summarise(). Ces fonctions peuvent être combinées entre elles, et il est possible d’en définir de nouvelles. Voici quelques fonctions courantes :\n\n\nFonction\nCode\n\n\n\nMoyenne\nmean()\n\n\nMédiane\nmedian()\n\n\nEcart-type\nsd()\n\n\nMinimum\nmin()\n\n\nMaximum\nmax()\n\n\nValeur de la première valeur\nfirst()\n\n\nValeur de la première valeur\nlast()\n\n\nNombre de lignes\nn()\n\n\nNombre de valeurs distinctes\nn_distinct()\n\n\nSomme\nsum()\n\n\nSomme cumulée\ncumsum()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLes fonctions mutate() et summarise() calculent toutes les deux de nouvelles variables. Il arrive donc fréquemment qu’on les confonde, ou qu’on ne sache pas laquelle il faut utiliser. Comme indiqué précédemment, l’une, mutate(), conduit à l’ajout d’une variable supplémentaire, l’autre, summarise(), définit une procédure d’agrégation de la donnée. Voici une règle simple pour savoir quelle fonction utiliser :\n\n\nSi vous voulez résumer une information contenue dans une table, il faut utiliser summarise(). Exemple : calculer le nombre total d’équipements pour chaque commune.\n\nbpe_ens_2018_tbl %&gt;%  \n  group_by(DEPCOM) %&gt;% \n  summarise(NB_EQUIP_TOT = sum(NB_EQUIP, na.rm = TRUE))\n\n\n\nSi vous voulez ajouter une information dans une table (en conservant toutes les autres variables), il faut utiliser mutate(). Exemple : ajouter dans la table bpe_ens_2018_tbl une colonne donnant le nombre total d’équipements pour chaque commune.\n\nbpe_ens_2018_tbl %&gt;%  \ngroup_by(DEPCOM) %&gt;% \nmutate(NB_EQUIP_TOT = sum(NB_EQUIP, na.rm = TRUE))\n\n\n\n\n\n\n18.3.1.7 Faire des opérations par groupe : group_by()\n\nLa fonction group_by() permet de définir des groupes dans la table de données pour faire des opérations par groupe. L’utilisation de group_by() rend très utiles les opérations avec summarise(). Le code suivant groupe les données de la BPE par département group_by(DEP) puis calcule le nombre total d’équipements sum(NB_EQUIP, na.rm = TRUE) et le nombre total de boulangeries sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE).\n\nbpe_ens_2018_tbl %&gt;% \n  group_by(DEP) %&gt;%\n  summarise(NB_EQUIP_TOT = sum(NB_EQUIP, na.rm = TRUE),\n            NB_BOULANGERIES_TOT = sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE))\n\n# A tibble: 101 × 3\n  DEP   NB_EQUIP_TOT NB_BOULANGERIES_TOT\n  &lt;chr&gt;        &lt;dbl&gt;               &lt;dbl&gt;\n1 01           21394                 401\n2 02           15534                 339\n3 03           12216                 299\n4 04            8901                 185\n5 05            8785                 175\n6 06           66766                 966\n# ℹ 95 more rows\n\n\nLa fonction group_by peut également modifier le comportement des fonctions filter ou mutate, comme pour summarise. L’instruction suivante permet ainsi d’ajouter à la BPE une variable égale au nombre total de boulangeries dans le département.\n\nbpe_ens_2018_tbl %&gt;% \n  group_by(DEP) %&gt;%\n  mutate(NB_BOULANGERIES_DEP = sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE))\n\n# A tibble: 1,035,564 × 8\n# Groups:   DEP [101]\n  REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP NB_BOULANGERIES_DEP\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;               &lt;dbl&gt;\n1 84    01    01001  01001   2018 A401          2                 401\n2 84    01    01001  01001   2018 A404          4                 401\n3 84    01    01001  01001   2018 A504          1                 401\n4 84    01    01001  01001   2018 A507          1                 401\n5 84    01    01001  01001   2018 B203          1                 401\n6 84    01    01001  01001   2018 C104          1                 401\n# ℹ 1,035,558 more rows\n\n\nL’instruction suivante permet de ne conserver que les communes pour lesquelles le nombre d’équipements est le plus important de leur région.\n\nbpe_ens_2018_tbl %&gt;% \n  group_by(REG) %&gt;%\n  filter(NB_EQUIP == max(NB_EQUIP))\n\n# A tibble: 18 × 7\n# Groups:   REG [18]\n  REG   DEP   DEPCOM DCIRIS        AN TYPEQU NB_EQUIP\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 27    25    25056  25056_0102  2018 B302        104\n2 94    2A    2A041  2A041       2018 A504        110\n3 75    33    33063  33063_0505  2018 A504        172\n4 76    34    34003  34003_0201  2018 A504        287\n5 53    35    35288  35288_0101  2018 A504        132\n6 52    44    44109  44109_0106  2018 A504        236\n# ℹ 12 more rows\n\n\nAinsi, les opérations à la suite d’un group_by(), comme ici sum et max, sont calculées par groupe de lignes (ici suivant la colonne DEP).\n\n\n\n\n\n\nNote\n\n\n\nTrois remarques sur l’utilisation de group_by() :\n\nsi vous ne savez pas si une table comporte des groupes, vous pouvez afficher la liste des variables de groupe avec la fonction group_vars() ;\nsi vous appliquez une instruction group_by() à une table qui comporte déjà des groupes, alors les groupes sont redéfinis ;\nil est prudent d’appliquer la fonction ungroup() à vos données une fois que les opérations par groupe ont été réalisées, afin que les opérations suivantes ne soient pas effectuées par groupe par mégarde.\n\n\n\n\n18.3.1.8 Appliquer une fonction dplyr sur un groupe de colonnes : across()\n\nLa fonction across() permet de sélectionner un groupe de colonnes sur lequel on veut appliquer une fonction dplyr. Ainsi on peut appliquer une fonction sur des colonnes en les sélectionnant comme le fait select(). Si on veut sélectionner des colonnes en fonction de leur type, on utilise en outre where(). Le code suivant groupe les données de la BPE avec les deux colonnes dont le nom commence par “DEP”, puis effectue la somme de toutes les colonnes de type numérique.\n\nbpe_ens_2018_tbl %&gt;% \n  group_by(across(starts_with('DEP'))) %&gt;%\n  summarise(across(where(is.numeric), sum, na.rm = TRUE)) \n\n# A tibble: 34,065 × 4\n# Groups:   DEP [101]\n  DEP   DEPCOM     AN NB_EQUIP\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 01    01001   22198       15\n2 01    01002    4036        2\n3 01    01004  472212      655\n4 01    01005   64576       68\n5 01    01006    6054        3\n6 01    01007   58522       60\n# ℹ 34,059 more rows\n\n\n\n18.3.2 Manipuler plusieurs tables avec dplyr\n\n\n18.3.2.1 Concaténer deux tables\nLe package dplyr propose la fonction bind_rows() pour superposer deux ou plusieurs tables (en empilant des observations). Deux remarques sur l’utilisation de cette fonction :\n\n\nbind_rows() combine les tables en fonction du nom des colonnes, l’ordre des colonnes n’a pas donc d’importance ;\nsi une colonne est manquante dans une des tables, alors des valeurs manquantes sont générées dans la table de sortie.\n\n\n\n\n\n\n\nNote\n\n\n\nLe package dplyr contient également la fonction bind_cols() qui permet de juxtaposer des tables (qui doivent avoir le même nombre d’observations). Il est conseillé de ne pas se servir de cette fonction. En effet, cette fonction juxtapose les colonnes par position (la première ligne d’une table est juxtaposée à la première de l’autre table), sans aucun contrôle. Si les différentes tables ne sont pas triées de la même façon, la table de sortie sera incohérente. Pour rapprocher deux tables, il est fortement conseillé d’utiliser les fonctions de jointures : inner_join, left_join, full_join… Ces fonctions sont présentées dans le paragraphe Joindre des tables.\n\n\n\n18.3.2.2 Joindre des tables\nAvec dplyr, les jointures se réalisent grâce aux fonctions left_join, right_join, inner_join, full_join et anti_join. Ces fonctions prennent les arguments suivants :\n\nle nom des deux data.frame à joindre ;\nles variables de jointure, défini par l’argument by. Lorsque la variable de jointure ne porte pas le même nom dans les deux tables, on utilise le paramètre by = c(\"var_x\" = \"var_y\"). S’il y a plusieurs variables de jointures, on écrit by = c(\"var_x1\" = \"var_y1\", \"var_x2\" = \"var_y2\").\n\nIl est préférable d’utiliser ces fonctions sur des objets tibble plutôt que data.frame. On va donc convertir les deux tables avant de présenter un exemple :\n\nlibrary(dplyr)\nfilosofi_com_2016_tbl &lt;- as_tibble(doremifasolData::filosofi_com_2016)\ncog_com_2019_tbl &lt;- as_tibble(doremifasolData::cog_com_2019)\n\nVoici un exemple dans lequel on utilise la fonction left_join pour réaliser une jointure à gauche entre la table des données Filosofi et la table des communes du COG.\n\ntable_jointe_tbl &lt;- filosofi_com_2016_tbl %&gt;% \nleft_join(y = cog_com_2019_tbl, \nby = c(\"CODGEO\" = \"com\"))\nhead(table_jointe_tbl)\n\n# A tibble: 6 × 39\n  CODGEO LIBGEO      NBMENFISC16 NBPERSMENFISC16  MED16 PIMP16 TP6016 TP60AGE116\n  &lt;chr&gt;  &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 01001  L'Abergeme…         313            796. 22679      NA     NA         NA\n2 01002  L'Abergeme…         101            248  24382.     NA     NA         NA\n3 01004  Ambérieu-e…        6363          14228  19721      49     17         19\n4 01005  Ambérieux-…         633           1662. 23378      NA     NA         NA\n5 01006  Ambléon              NA             NA     NA      NA     NA         NA\n6 01007  Ambronay           1087           2684  22146.     57     NA         NA\n# ℹ 31 more variables: TP60AGE216 &lt;dbl&gt;, TP60AGE316 &lt;dbl&gt;, TP60AGE416 &lt;dbl&gt;,\n#   TP60AGE516 &lt;dbl&gt;, TP60AGE616 &lt;dbl&gt;, TP60TOL116 &lt;dbl&gt;, TP60TOL216 &lt;dbl&gt;,\n#   PACT16 &lt;dbl&gt;, PTSA16 &lt;dbl&gt;, PCHO16 &lt;dbl&gt;, PBEN16 &lt;dbl&gt;, PPEN16 &lt;dbl&gt;,\n#   PPAT16 &lt;dbl&gt;, PPSOC16 &lt;dbl&gt;, PPFAM16 &lt;dbl&gt;, PPMINI16 &lt;dbl&gt;, PPLOGT16 &lt;dbl&gt;,\n#   PIMPOT16 &lt;dbl&gt;, D116 &lt;dbl&gt;, D916 &lt;dbl&gt;, RD16 &lt;dbl&gt;, typecom &lt;chr&gt;,\n#   reg &lt;chr&gt;, dep &lt;chr&gt;, arr &lt;chr&gt;, tncc &lt;chr&gt;, ncc &lt;chr&gt;, nccenr &lt;chr&gt;,\n#   libelle &lt;chr&gt;, can &lt;chr&gt;, comparent &lt;chr&gt;\n\n\nLa syntaxe pour réaliser les autres types de jointure est très similaire :\n\n\n\n\n\n\nType de jointure\nSyntaxe dplyr\n\n\n\n\nJointure à gauche\nleft_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\nJointure à droite\nright_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\nJointure externe\nfull_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\nAnti-jointure\nanti_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\n\nLa jointure de deux ou plusieurs tables est une opération lourde qui doit être soigneusement préparée. Il est recommandé de consulter la fiche [Joindre des tables de données] qui présente un certain nombre de règles et de bonnes pratiques sur les jointures.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Manipuler des données avec le `tidyverse`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_tidyverse.html#mettre-en-ordre-des-données-avec-tidyr",
    "href": "03_Fiches_thematiques/Fiche_tidyverse.html#mettre-en-ordre-des-données-avec-tidyr",
    "title": "18  Manipuler des données avec le tidyverse",
    "section": "\n18.4 Mettre en ordre des données avec tidyr\n",
    "text": "18.4 Mettre en ordre des données avec tidyr\n\nLe package tidyr fournit de multiples fonctions pour retraiter et restructurer des données afin de les organiser sous forme de table (on parle de tidy data). Ces fonctions permettent de résoudre un grand nombre de problèmes : retraiter des colonnes et des lignes, restructurer des tables, convertir des listes imbriquées en tables de données… Elles prennent la forme de verbes qui complètent ceux de dplyr et s’intègrent parfaitement dans les séries de pipes (%&gt;%), les pipelines, permettant d’enchaîner les opérations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18.4.1 Retraiter des colonnes avec tidyr\n\n\n18.4.1.1 separate : scinder une colonne en plusieurs\nIl arrive que plusieurs informations réunies en une seule colonne et qu’on souhaite les séparer. La fonction separate permet d’effectuer cette opération. Elle prend trois arguments principaux :\n\nle nom de la colonne à scinder ;\nun vecteur indiquant les noms des nouvelles variables à créer ;\nle séparateur sep indique à quel endroit la variable doit être scindée. Par défaut separate scinde au niveau des caractères non-alphanumérique (espace, symbole, etc.). Si l’on indique un nombre entier n, alors la colonne est scindée après le n-ième caractère.\n\nVoici un exemple qui utilise la table des communes du Code Officiel Géographique. Dans cette table, la colonne com (code commune Insee) contient deux informations : le numéro du département et le numéro de la commune.\n\ncog_com_2019_tbl &lt;- doremifasolData::cog_com_2019 %&gt;% as_tibble()\ncog_com_2019_tbl\n\n# A tibble: 37,930 × 11\n  typecom com   reg   dep   arr   tncc  ncc       nccenr libelle can   comparent\n  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    \n1 COM     01001 84    01    012   5     ABERGEME… Aberg… L'Aber… 0108  &lt;NA&gt;     \n2 COM     01002 84    01    011   5     ABERGEME… Aberg… L'Aber… 0101  &lt;NA&gt;     \n3 COM     01004 84    01    011   1     AMBERIEU… Ambér… Ambéri… 0101  &lt;NA&gt;     \n4 COM     01005 84    01    012   1     AMBERIEU… Ambér… Ambéri… 0122  &lt;NA&gt;     \n5 COM     01006 84    01    011   1     AMBLEON   Amblé… Ambléon 0104  &lt;NA&gt;     \n6 COM     01007 84    01    011   1     AMBRONAY  Ambro… Ambron… 0101  &lt;NA&gt;     \n# ℹ 37,924 more rows\n\n\nVoici comment on peut utiliser separate pour scinder com en deux nouvelles colonnes code_dep et code_com. Vous pouvez noter que la colonne com a disparu, car par défaut separate supprime la colonne scindée. Si on veut la conserver, il faut ajouter l’option remove = FALSE.\n\ncog_com_2019_tbl %&gt;% \n  separate(com, c(\"code_dep\", \"code_com\"), sep = 2)\n\n# A tibble: 37,930 × 12\n  typecom code_dep code_com reg   dep   arr   tncc  ncc     nccenr libelle can  \n  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;\n1 COM     01       001      84    01    012   5     ABERGE… Aberg… L'Aber… 0108 \n2 COM     01       002      84    01    011   5     ABERGE… Aberg… L'Aber… 0101 \n3 COM     01       004      84    01    011   1     AMBERI… Ambér… Ambéri… 0101 \n4 COM     01       005      84    01    012   1     AMBERI… Ambér… Ambéri… 0122 \n5 COM     01       006      84    01    011   1     AMBLEON Amblé… Ambléon 0104 \n6 COM     01       007      84    01    011   1     AMBRON… Ambro… Ambron… 0101 \n# ℹ 37,924 more rows\n# ℹ 1 more variable: comparent &lt;chr&gt;\n\n\n\n18.4.1.2 unite : regrouper plusieurs colonnes en une seule\nLa fonction unite permet est de réaliser l’opération inverse de separate : regrouper plusieurs colonnes en une seule. Elle prend trois arguments principaux :\n\nle nom de la colonne à créer ;\nun vecteur indiquant les noms des variables à regrouper ;\nle séparateur sep qui indique quel séparateur doit être introduit entre les variables regroupées (par défaut, unite utilise le caractère _).\n\nVoici un exemple où l’on regroupe le code commune Insee et le nom officiel de la commune, avec ” - ” comme séparateur. Vous pouvez noter que les colonnes com et ncc ont disparu, car par défaut unite supprime les colonnes regroupées. Si on veut les conserver, il faut ajouter l’option remove = FALSE.\n\ncog_com_2019_tbl %&gt;% \n  unite(code_et_nom, c(\"com\", \"ncc\"), sep = \" - \") \n\n# A tibble: 37,930 × 10\n  typecom code_et_nom     reg   dep   arr   tncc  nccenr libelle can   comparent\n  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    \n1 COM     01001 - ABERGE… 84    01    012   5     Aberg… L'Aber… 0108  &lt;NA&gt;     \n2 COM     01002 - ABERGE… 84    01    011   5     Aberg… L'Aber… 0101  &lt;NA&gt;     \n3 COM     01004 - AMBERI… 84    01    011   1     Ambér… Ambéri… 0101  &lt;NA&gt;     \n4 COM     01005 - AMBERI… 84    01    012   1     Ambér… Ambéri… 0122  &lt;NA&gt;     \n5 COM     01006 - AMBLEON 84    01    011   1     Amblé… Ambléon 0104  &lt;NA&gt;     \n6 COM     01007 - AMBRON… 84    01    011   1     Ambro… Ambron… 0101  &lt;NA&gt;     \n# ℹ 37,924 more rows\n\n\n\n18.4.1.3 replace_na : remplacer des valeurs manquantes\nLa fonction replace_na permet de remplacer des valeurs manquantes (NA). Cette fonction peut être utilisée de deux façons, que l’on va illustrer avec les données suivantes :\n\ndf &lt;- tibble(x = c(1, 2, 3, NA), y = c(\"a\", NA, \"b\", NA))\ndf\n\n# A tibble: 4 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 &lt;NA&gt; \n3     3 b    \n4    NA &lt;NA&gt; \n\n\nPremier usage : on remplace les valeurs manquantes dans une colonne d’un data.frame. Dans ce cas, la fonction prend deux arguments : le nom de la variable et la valeur utilisée pour remplacer les valeurs manquantes. Voici un exemple :\n\ndf %&gt;% \n  mutate(x = replace_na(x, 888))\n\n# A tibble: 4 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 &lt;NA&gt; \n3     3 b    \n4   888 &lt;NA&gt; \n\n\nSecond usage : on remplace les valeurs manquantes dans toutes les colonnes d’un data.frame. Dans ce cas, la fonction prend deux arguments : le nom du data.frame, et une liste donnant pour chaque variable la valeur à utiliser pour remplacer les valeurs manquantes. Voici un exemple :\n\ndf %&gt;% \n  replace_na(list(x = 888, y = \"zzz\"))\n\n# A tibble: 4 × 2\n      x y    \n  &lt;dbl&gt; &lt;chr&gt;\n1     1 a    \n2     2 zzz  \n3     3 b    \n4   888 zzz  \n\n\n\n18.4.1.4 complete : compléter des combinaisons de variables manquantes\nLa fonction complete permet de compléter des combinaisons manquantes de valeurs de plusieurs colonnes (autrement dit, de compléter un produit cartésien incomplet). Dans le tableau de donnée suivant, seuls les pays ayant remporté la Coupe du Monde de football sont renseignés, avec l’année de leur victoire. Supposons qu’on veuille compléter ce tableau, en indiquant 0 lorsqu’un pays n’a pas remporté la Coupe du Monde.\n\ndf &lt;- tibble(pays = c(\"France\", \"France\", \"Espagne\", \"Angleterre\"),\n             annee = c(1998, 2018, 2010, 1966),\n             victoire = c(1, 1, 1, 1))\ndf\n\n# A tibble: 4 × 3\n  pays       annee victoire\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 France      1998        1\n2 France      2018        1\n3 Espagne     2010        1\n4 Angleterre  1966        1\n\n\nOn peut l’utiliser de cette manière :\n\ndf %&gt;% complete(pays, annee)\n\n# A tibble: 12 × 3\n  pays       annee victoire\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 Angleterre  1966        1\n2 Angleterre  1998       NA\n3 Angleterre  2010       NA\n4 Angleterre  2018       NA\n5 Espagne     1966       NA\n6 Espagne     1998       NA\n# ℹ 6 more rows\n\n\nOn voit que les combinaisons manquantes ont bien été ajoutées par complete. Par défaut les lignes insérées comprennent des valeurs manquantes NA pour les colonnes restantes. On peut néanmoins choisir une autre valeur avec l’argument fill, qui prend la forme d’une liste nommée. Dans le cas présent, la liste ne comprend qu’un seul élément car il n’y a qu’une seule variable à compléter :\n\ndf %&gt;% complete(pays, annee, fill = list(victoire = 0))\n\n# A tibble: 12 × 3\n  pays       annee victoire\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 Angleterre  1966        1\n2 Angleterre  1998        0\n3 Angleterre  2010        0\n4 Angleterre  2018        0\n5 Espagne     1966        0\n6 Espagne     1998        0\n# ℹ 6 more rows\n\n\nSi l’on ne souhaite pas inclure toutes les colonnes dans le calcul des combinaisons de valeurs, on peut utiliser l’option nesting (taper ?tidyr::complete pour les détails).\n\n18.4.2 Restructurer des données avec tidyr\n\n\n18.4.2.1 Principe de la restructuration des données\nUne table de données stocke des informations sous forme de lignes et de colonnes. Au cours d’un traitement statistique, il est souvent nécessaire de restructurer les données, en transformant en colonnes certaines informations qui figuraient en ligne (ou inversement). Les deux principales opérations de restructuration des données peuvent être illustrées par les deux transformations suivantes :\n\n\nTransformation wide to long  :\n\n\n\nTransformation wide to long\n\n\n\nTransformation long to wide  :\n\n\n\nTransformation long to wide\n\nPour illustrer ces transformations, nous allons utiliser les données du répertoire Filosofi 2016 agrégées au niveau des EPCI (table filosofi_epci_2016), et disponibles dans le package doremifasolData. On convertit cette table en tibble et on conserve uniquement certaines variables grâce à la fonction select.\n\nfilosofi_epci_2016_tbl &lt;- as_tibble(filosofi_epci_2016) %&gt;% \n  select(CODGEO, TP6016, TP60AGE116, TP60AGE216,\n         TP60AGE316, TP60AGE416, TP60AGE516, TP60AGE616)\n\n\n18.4.2.2 pivot_longer : transformer des colonnes en lignes\nLa fonction pivot_longer permet de restructurer des données en transformant des colonnes en lignes. Elle sert fréquemment au début d’un traitement, pour transformer des données mal structurées en une table facile à traiter. Cette fonction prend quatre arguments principaux :\n\nle data.frame (ou le tibble) auquel elle est appliquée ;\n\ncols : un vecteur contenant le nom des colonnes dont les valeurs vont être transposées ;\n\nnames_to : le nom de la nouvelle colonne qui va contenir les noms des colonnes transposées ;\n\nvalues_to : le nom de la nouvelle colonne qui va contenir les valeurs des colonnes transposées.\n\nPour illustrer l’usage de cette fonction, nous allons utiliser les données du répertoire Filosofi 2016 agrégées au niveau des EPCI (table filosofi_epci_2016), et disponibles dans le package doremifasol. On convertit cette table en tibble et on conserve uniquement certaines variables grâce à la fonction select.\n\nfilosofi_epci_2016_tbl &lt;- as_tibble(filosofi_epci_2016) %&gt;% \n  select(CODGEO, TP6016, TP60AGE116, TP60AGE216,\n         TP60AGE316, TP60AGE416, TP60AGE516, TP60AGE616)\nfilosofi_epci_2016_tbl\n\n# A tibble: 1,244 × 8\n  CODGEO    TP6016 TP60AGE116 TP60AGE216 TP60AGE316 TP60AGE416 TP60AGE516\n  &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 200000172    8.8       12.5        9.4        9.7        7.9        6.6\n2 200000438    8          9.5        6.5        9.1        9          6  \n3 200000545   23.7       38.2       30.8       32         20.5       14.9\n4 200000628   20.1       28.1       22.4       25.5       18.2       17.1\n5 200000800   11.4       NA         NA         15.9       12         NA  \n6 200000925    6.5       NA          6          6.7        7.6        5.5\n# ℹ 1,238 more rows\n# ℹ 1 more variable: TP60AGE616 &lt;dbl&gt;\n\n\nNous allons restructurer cette table pour obtenir une nouvelle table, avec une observation par EPCI et par tranche d’âge. Voici le code qui permet d’obtenir cette table : on transpose les valeurs des colonnes dont le nom commence par “TP” (cols = starts_with(\"TP\")), le nom des colonnes transposées sera indiquée dans la nouvelle colonne “tranche_age” (names_to = \"tranche_age\") et les valeurs des colonnes transposées seront indiquées dans la colonne “taux_pauvrete” (values_to = \"taux_pauvrete\").\n\ndonnees_pauvrete_long &lt;- filosofi_epci_2016_tbl %&gt;% \n  pivot_longer(cols = starts_with(\"TP\"), \n               names_to = \"tranche_age\", \n               values_to = \"taux_pauvrete\")\ndonnees_pauvrete_long\n\n# A tibble: 8,708 × 3\n   CODGEO    tranche_age taux_pauvrete\n   &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt;\n 1 200000172 TP6016                8.8\n 2 200000172 TP60AGE116           12.5\n 3 200000172 TP60AGE216            9.4\n 4 200000172 TP60AGE316            9.7\n 5 200000172 TP60AGE416            7.9\n 6 200000172 TP60AGE516            6.6\n 7 200000172 TP60AGE616           NA  \n 8 200000438 TP6016                8  \n 9 200000438 TP60AGE116            9.5\n10 200000438 TP60AGE216            6.5\n# ℹ 8,698 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl est recommandé de travailler avec des données en format long plutôt qu’en format wide, notamment lorsque vous voulez faire des graphiques. En effet, le package de visualisation graphique ggplot2 est optimisé pour manipuler des données en format long (voir la fiche [Faire des graphiques avec ggplot2]). Ce conseil est particulièrement important si vous voulez représenter un graphique avec des groupes : il est préférable que les groupes soient empilés (format long) plutôt que juxtaposés (format wide), car le code est plus rapide et surtout plus facile à écrire.\n\n\n\n18.4.2.3 pivot_wider : transformer des lignes en colonnes\nLa fonction pivot_wider permet de restructurer des données en transformant des lignes en colonnes. Cette fonction prend quatre arguments principaux :\n\nle data.frame (ou le tibble) auquel elle est appliquée ;\n\nid_cols : un vecteur contenant le nom des colonnes qui définissent les observations de la table transposée ;\n\nnames_from : un vecteur contenant le nom de la (ou des) colonne(s) qui donne(nt) les noms des nouvelles colonnes ;\n\nvalues_from : un vecteur contenant le nom de la (ou des) colonne(s) dont les valeurs vont être transposées.\n\nPar ailleurs, l’option names_prefix permet de définir le préfixe du nom des nouvelles colonnes, ce qui est utile pour avoir des noms explicites.\n\nbpe_ens_2018_tbl %&gt;%\n    group_by(REG, TYPEQU) %&gt;%\n    summarise(NB_EQUIP_TOT = sum(NB_EQUIP)) %&gt;%\n    pivot_wider(id_cols =TYPEQU,  \n                names_from = REG, \n                values_from = NB_EQUIP_TOT, \n                names_prefix = \"nb_equip_reg\")\n\n# A tibble: 186 × 19\n  TYPEQU nb_equip_reg01 nb_equip_reg02 nb_equip_reg03 nb_equip_reg04\n  &lt;chr&gt;           &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 A101                2              2              1              7\n2 A104               20             21             16             28\n3 A105                1              1              1              1\n4 A106                2              1              2              2\n5 A107                2              1              1              4\n6 A108                2              1              1              2\n# ℹ 180 more rows\n# ℹ 14 more variables: nb_equip_reg06 &lt;dbl&gt;, nb_equip_reg11 &lt;dbl&gt;,\n#   nb_equip_reg24 &lt;dbl&gt;, nb_equip_reg27 &lt;dbl&gt;, nb_equip_reg28 &lt;dbl&gt;,\n#   nb_equip_reg32 &lt;dbl&gt;, nb_equip_reg44 &lt;dbl&gt;, nb_equip_reg52 &lt;dbl&gt;,\n#   nb_equip_reg53 &lt;dbl&gt;, nb_equip_reg75 &lt;dbl&gt;, nb_equip_reg76 &lt;dbl&gt;,\n#   nb_equip_reg84 &lt;dbl&gt;, nb_equip_reg93 &lt;dbl&gt;, nb_equip_reg94 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl est conseillé de bien réfléchir avant de restructurer en format wide, et de ne le faire que lorsque cela paraît indispensable. En effet, s’il est tentant de restructurer les données sous format wide car ce format peut paraître plus intuitif, il est généralement plus simple et plus rigoureux de traiter les données en format long. Ceci dit, il existe des situations dans lesquelles il est indiqué de restructurer les données en format wide. Voici deux exemples :\n\nproduire un tableau synthétique de résultats, prêt à être diffusé, avec quelques colonnes donnant des indicateurs par catégorie (exemple : la table filosofi_epci_2016 du package doremifasolData, qui contient plusieurs colonnes contenant le taux de pauvreté pour différentes tranches d’âge) ;\nproduire une table avec une colonne par année, de façon à calculer facilement un taux d’évolution entre deux dates.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Manipuler des données avec le `tidyverse`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_tidyverse.html#programmer-des-fonctions-avec-le-tidyverse",
    "href": "03_Fiches_thematiques/Fiche_tidyverse.html#programmer-des-fonctions-avec-le-tidyverse",
    "title": "18  Manipuler des données avec le tidyverse",
    "section": "\n18.5 Programmer des fonctions avec le tidyverse\n",
    "text": "18.5 Programmer des fonctions avec le tidyverse\n\nLorsqu’une suite d’opérations est amenée à être effectuée plusieurs fois, une bonne pratique consiste à factoriser son code, notamment grâce à des fonctions définies par l’utilisateur. Le site internet pratiques.utilitr.org présente notamment quelques recommandations de cet ordre.\n\n18.5.1 Présentation de la difficulté à fonctionnaliser\nLa fonctionnalisation n’est pas simple lorsqu’on travaille avec le tidyverse, en raison de l’évaluation non standard que celui-ci exploite. En effet, le tidyverse est un paradigme de traitement des données qui cherche à faciliter au maximum son utilisation. Pour ce faire, comme vu précédemment, il n’est pas besoin d’écrire le nom des variables entre guillemets, ou de répéter le nom de la base que l’on exploite, ni même d’utiliser le $. Toutefois, cette facilité génère des difficultés lorsqu’on souhaite créer une fonction. Le code suivant montre deux traitements qui, en toute logique, devrait donner des résultats similaires.\n\nhead(bpe_ens_2018) %&gt;% select(REG) # fonctionne\n\nselectionner_colonne &lt;- function(donnees,colonne) {\n  retour &lt;- donnees %&gt;%\n    select(colonne)\n  \n  return(retour)\n}\n\n# Ce code ne fonctionne pas\nselectionner_colonne(\n  donnees=head(bpe_ens_2018),\n  colonne = REG)\n\nAlors que R parvient parfaitement à comprendre que le terme REG fait référence à un nom de colonne de la base dans le premier cas, il ne le comprend pas dans le second cas ! Ceci est dû aux fonctions du tidyverse qui permettent cette flexibilité, tandis que la fonction selectionner_colonne cherche un objet qui s’intitulerait REG, sans comprendre qu’il s’agit du nom de la colonne du jeu de données.\n\n\n\n\n\n\nNote\n\n\n\nL’évaluation non-standard est un concept complexe et très riche dans R. La tidy-evaluation en est une utilisation spécifique.\nDe nombreuses fonctionnalités élémentaires de R utilisent de telles évaluations. Par exemple, les commandes library(dplyr) ou library(\"dplyr\") fonctionnent toutes les deux indifféremment. Pour autant, l’objet dplyr de la première expression n’existe pas. R comprend implicitement que nous voulions parler du nom d’un package et charge le package \"dplyr\".\nL’évaluation, standard ou non-standard, repose donc sur l’idée que R doit savoir quand nous faisons référence au nom d’un objet ou à l’objet lui-même. Par exemple, quand on écrit head(bpe_ens_2018), R comprend que l’appel à bpe_ens_2018 fait référence à une table de données nommée “bpe_ens_2018”, pas au nom en question. La tidy-evaluation permet de faire des appels un peu plus complexes. Ainsi, lorsque nous écrivons select(REG), R comprend que quelques opérations préalables sont nécessaires :\n\n\npatienter : il ne faut pas chercher à comprendre immédiatement le sens de REG car aucun objet dans notre environnement ne s’appelle REG ;\n\ntransporter : R va reconstituer toute la chaine de traitement select(head(bpe_ens_2018), REG) avant de chercher à interpréter ;\n\nréinterpréter : R peut désormais chercher à identifier si une colonne de head(bpe_ens_2018) se nomme REG.\n\n\n\n\n18.5.2 Présentation de la solution\nPour réussir à créer des fonctions facilement compatibles avec le tidyverse, il faut faire appel à un opérateur spécifique, appelé curly-curly ou doublestache : { }. Il s’agit de l’équivalent tidyverse de la fonction get présentée dans le chapitre Manipuler ses données avec data.table. Son utilisation est particulièrement simple, car il suffit d’entourer la variable par ces accolades.\n\nselectionner_colonne &lt;- function(donnees,colonne) {\n  retour &lt;- donnees %&gt;%\n    select({{colonne}})\n  \n  return(retour)\n}\n\n# Cette fois, le code fonctionne!\nselectionner_colonne(donnees=head(bpe_ens_2018),\n               colonne = REG)\n\n  REG\n1  84\n2  84\n3  84\n4  84\n5  84\n6  84\n\n\nLe doublestache est une nouvelle notation du package rlang(version 0.4.0), équivalent à l’opération !!enquo(). L’idée générale étant que enquo() permet de transformer un input en symbole à interpréter, tandis que l’opérateur bang-bang !! détermine quand il faut l’interpréter. Parfois, certaines opérations un peu complexes, qui dépassent le cadre de cette fiche nécessitent le recours à ces opérateurs particuliers.\n\n\n\n\n\n\nNote\n\n\n\nIl peut arriver que vous trouviez sur internet une page recommandant l’usage de l’expression eval(parse()). Il est fortement conseillé de ne jamais l’utiliser pour les raisons suivantes :\n\nelle est susceptible de créer des problèmes de sécurité. Tout le contenu à l’intérieur du parse est interprété par R comme du code à lancer, quel que soit le contenu. Il est donc particulièrement sensible aux problèmes d’injection de code ;\nsa performance est très mauvaise ;\nl’identification des bug est particulièrement délicate, avec des messages d’erreurs peu opportuns et compliqués à comprendre.\n\n\n\nLorsqu’on souhaite définir une nouvelle colonne dans le tidyverse, une légère modification est nécessaire, avec l’utilisation de l’opérateur :=, comme dans l’exemple suivant.\n\nselectionner_et_renommer &lt;- function(donnees,colonne, nouveau_nom = \"nouveau_nom\") {\n  retour &lt;- donnees %&gt;%\n    select({{colonne}}) %&gt;%\n    rename({{nouveau_nom}}:={{colonne}})\n  \n  return(retour)\n}\n\n# Ce code fonctionne!\nselectionner_et_renommer(\n  donnees=head(bpe_ens_2018),\n  colonne = REG,\n  nouveau_nom = \"region\")\n\n  region\n1     84\n2     84\n3     84\n4     84\n5     84\n6     84\n\n\nNotez bien la présence des := dans l’opération de renommage. Il est également prudent de proposer une valeur par défault à la variable dans la fonction. Ainsi, si l’utilisateur oublie d’indiquer un nouveau nom, la colonne sera renommée en nouveau_nom.\n\n\n\n\n\n\nNote\n\n\n\nL’opérateur := du tidyverse ne remplit pas les mêmes fonctions que l’opérateur := présenté dans la fiche Manipuler ses données avec data.table:\n\nDans data.table, l’opérateur sert à créer une nouvelle variable en limitant le coût en mémoire de cette opération ;\nDans le tidyverse, l’opérateur sert à faire comprendre à dplyr que la partie à gauche est un nom de colonne à ré-interpréter sans les guillemets.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPour fonctionnaliser un code utilisant le tidyverse, il est recommandé :\n\nd’utiliser au maximum l’opérateur doublestache {{ qui présente l’intérêt d’être très lisible et très simple d’utilisation ;\nlorsque l’évaluation doit avoir lieu “à gauche” d’un opérateur =, il faut habituellement le remplacer par := ;\nd’éviter l’utilisation de eval(parse()).\n\n\n\nSi les exemples précédents se basent tous sur dplyr, il est à remarquer que les solutions présentées s’appliquent à tous les packages du tidyverse, y compris ggplot2 (c.f. Faire des graphiques avec ggplot2).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Manipuler des données avec le `tidyverse`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_tidyverse.html#RessourcesTidyverse",
    "href": "03_Fiches_thematiques/Fiche_tidyverse.html#RessourcesTidyverse",
    "title": "18  Manipuler des données avec le tidyverse",
    "section": "\n18.6 Pour en savoir plus",
    "text": "18.6 Pour en savoir plus\n\nla documentation du package dplyr (en anglais) ;\nla documentation du package tidyr (en anglais) ;\nLe chapitre Data transformation du livre R for data science (en anglais), librement accessible en ligne ;\nParties dplyr et tidyr (en français) de l’introduction à R et au tidyverse de Julien Barnier ;\n\nVignette introductive sur dplyr (en anglais) ;\n\nVignette sur les jointures avec dplyr (en anglais) ;\n\nVignette sur la réorganisation de données avec tidyr (en anglais) ;\n\nPapier d’Hadley Whickam sur la définition du concept de tidy data (en anglais) ;\nUn aide-mémoire (cheatsheet) sur le tidyverse en français ;\nl’ article de lancement de rlang 0.4.0 (en anglais).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Manipuler des données avec le `tidyverse`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_datatable.html",
    "href": "03_Fiches_thematiques/Fiche_datatable.html",
    "title": "19  Manipuler des données avec data.table",
    "section": "",
    "text": "19.1 Tâches concernées et recommandations\nL’utilisateur souhaite manipuler des données structurées sous forme de data.frame (sélectionner des variables, sélectionner des observations, créer des variables, joindre des tables).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manipuler des données avec `data.table`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_datatable.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_datatable.html#tâches-concernées-et-recommandations",
    "title": "19  Manipuler des données avec data.table",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nPour des tables de données de taille petite et moyenne (inférieure à 1 Go ou moins d’un million d’observations), il est recommandé d’utiliser les packages tibble, dplyr et tidyr qui sont présentés dans la fiche Manipuler des données avec le tidyverse;\nPour des tables de données de grande taille (plus de 1 Go ou plus d’un million d’observations), il est recommandé d’utiliser soit le package data.table qui fait l’objet de la présente fiche, soit les packages arrow et duckdb présentés dans les fiches Manipuler des données avec arrow et Manipuler des données avec duckdb.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCertains exemples de cette fiche utilisent les données disponibles dans le package doremifasolData ; vous ne pourrez reproduire ces exemples que si ce package est installé sur la machine sur laquelle vous travaillez. Si vous ne savez pas si ce package est déjà installé, consultez la fiche Comment utiliser la documentation utilitR.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manipuler des données avec `data.table`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_datatable.html#présentation-de-data.table",
    "href": "03_Fiches_thematiques/Fiche_datatable.html#présentation-de-data.table",
    "title": "19  Manipuler des données avec data.table",
    "section": "\n19.2 Présentation de data.table\n",
    "text": "19.2 Présentation de data.table\n\nNe pas oublier de charger le package avec library(data.table).\n\n19.2.1 Principes structurants\nLe package data.table propose une version améliorée du data.frame de base : le data.table. La principale différence visible est que la visualisation d’un objet data.table est meilleure que celle d’un data.frame standard : le data.table indique automatiquement le type des variables (sous le nom de variable), et donne le nombre total d’observations de la table.\n\ndt &lt;- data.table(x = c(\"A\", \"B\", \"C\"),\n                 y = 1:12,\n                 z = 3:6)\ndt\n\n         x     y     z\n    &lt;char&gt; &lt;int&gt; &lt;int&gt;\n 1:      A     1     3\n 2:      B     2     4\n 3:      C     3     5\n 4:      A     4     6\n 5:      B     5     3\n 6:      C     6     4\n 7:      A     7     5\n 8:      B     8     6\n 9:      C     9     3\n10:      A    10     4\n11:      B    11     5\n12:      C    12     6\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl est possible de modifier les options globales de data.table pour avoir un affichage plus informatif avec les classes de chaque colonne et l’éventuelle clé de la base (voir plus bas) :\n\noptions(\n  \"datatable.print.keys\" = TRUE,\n  \"datatable.print.class\" = TRUE\n  )\ndt\n\n         x     y     z\n    &lt;char&gt; &lt;int&gt; &lt;int&gt;\n 1:      A     1     3\n 2:      B     2     4\n 3:      C     3     5\n 4:      A     4     6\n 5:      B     5     3\n 6:      C     6     4\n 7:      A     7     5\n 8:      B     8     6\n 9:      C     9     3\n10:      A    10     4\n11:      B    11     5\n12:      C    12     6\n\noptions(\n  \"datatable.print.keys\" = FALSE,\n  \"datatable.print.class\" = FALSE\n  )\n\n\n\nLa fonction fondamentale de data.table est l’opérateur [...] (crochets). Lorsqu’on les applique à un objet data.frame de base, les crochets df[...] servent uniquement à sélectionner des lignes ou des colonnes. Dans un data.table, les crochets dt[...] permettent de faire beaucoup plus de choses (quasiment tout, en pratique). En fait, les instructions à l’intérieur des crochets peuvent être envisagées comme des requêtes SQL mises en forme différemment.\nLa forme générale de l’opérateur [...] est la suivante : DT[i, j, by]. Cette grammaire peut se lire comme ceci : “on part du data.table DT, on sélectionne certaines lignes avec i, puis on calcule j pour chaque groupe défini par by. Si on fait un parallèle avec SQL, i correspond au WHERE, j au SELECT et by au GROUP BY. La fonction [...] présente deux grands avantages :\n\nIl n’est pas nécessaire d’utiliser le préfixe DT$ pour se référer aux variables à l’intérieur de [...] ;\nLe code est très concis, ce qui aide à le rendre lisible.\n\n\n\n\n\n\n\nNote\n\n\n\nCette syntaxe compacte est aussi un des atouts fondamentaux de data.table pour sa rapidité : data.table ne manipule que les colonnes mentionnées dans l’opérateur [...], ce qui réduit le temps de traitement des données.\n\n\nVoici un exemple simple. A partir des données générées ci-dessus, on veut calculer la moyenne de y par groupe défini par x, uniquement sur les observations pour lesquelles x est supérieur à 3. Voici comment on peut réaliser cette opération avec Base R, dplyr et data.table. Vous pouvez juger vous-même de la concision du code.\n\n\n\n\n\nBase R\n\n\n\naggregate(\n  dt[dt[[\"x\"]] &gt; 3]$y,\n  by = list(dt[dt[[\"x\"]] &gt; 3]$z),\n  FUN = sum)\n\n\n\n\n\ndplyr\n\n\n\ndt %&gt;%\n  dplyr::filter(x &gt; 3) %&gt;%\n  dplyr::group_by(z) %&gt;%\n  dplyr::summarise(sum(y))\n\n\n\n\n\ndata.table\n\n\n\ndt[x &gt; 3, sum(y), by = z]\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nL’utilisation du package data.table peut paraître plus déroutante pour les débutants que l’utilisation de dplyr. Toutefois, l’apprentissage de data.table est particulièrement recommandé si vous avez l’intention d’utiliser R avec des données volumineuses car data.table est beaucoup plus rapide et puissant que dplyr. Des remarques et conseils sont présents dans cette fiche pour vous aider à vous familiariser avec la syntaxe de data.table.\n\n\n\n19.2.2 Quelles fonctions peut-on utiliser avec un data.table ?\nLes data.tables sont simplement des data.frames particuliers, donc on peut normalement leur appliquer toutes les méthodes valables pour les data.frames. En particulier, on peut utiliser avec data.table toutes les fonctions des packages habituellement associés à dplyr : stringr pour le maniement de chaînes de caractères, lubridate pour les colonnes temporelles, forcats pour les colonnes de type factor, etc. Toutefois, il est utile de vérifier que le package data.table ne propose pas déjà une fonction adaptée. Par exemple, plutôt que d’utiliser la fonction str_split_fixed() du package stringr pour séparer une colonne en fonction d’un caractère, on utilisera tstrsplit() de data.table.\n\n19.2.3 Enchaîner les opérations en data.table\n\n\n19.2.3.1 Le principe est simple…\nIl est facile d’enchaîner des opérations avec data.table : il suffit d’accoler les opérateurs []. Votre code data.table prendra alors la forme suivante : dt[opération 1][opération 2][opération 3][...]. Voici un exemple simple, dans lequel on calcule la moyenne d’une variable par groupe, puis on trie la table.\n\n# En chaînant\nans &lt;- dt[ , .(moyenne = mean(y, na.rm = TRUE)), by = x][order(moyenne)]\nans\n\n   x moyenne\n1: A     5.5\n2: B     6.5\n3: C     7.5\n\n\n\n19.2.3.2 … mais il faut que le code reste lisible…\nLe problème avec l’enchaînement d’opérations multiples est qu’on aboutit rapidement à des lignes de codes extrêmement longues. C’est pourquoi il est préférable de revenir régulièrement à la ligne, de façon à garder un code qui reste lisible. Il y a évidemment plusieurs façons d’organiser le code. La seule obligation est que le crochet qui commence une nouvelle opération doit être accolé au crochet qui termine l’opération précédente (...][...). Voici deux organisations possibles, à vous de choisir celle qui vous paraît la plus claire et la plus adaptée à votre travail.\nLa première organisation enchaîne toutes les opérations en une seule fois :\n\nresultat &lt;- \n  dt[i = ...,\n     j = ...,\n     by = ...\n     ][i = ...,\n       j = ...,\n       by = ...\n       ]\n\nLa seconde organisation sépare les opérations en utilisant une table intermédiaire nommée resultat :\n\nresultat &lt;- dt[i = ...,\n               j = ...,\n               by = ...\n               ]\nresultat &lt;- resultat[i = ...,\n                     j = ...,\n                     by = ...\n                     ]\n\nComme indiqué précédemment, i, j et by ne sont pas forcément présents dans toutes les étapes. Voici ce que cette organisation du code donne sur un exemple légèrement plus complexe que le précédent :\n\ndt[ , total := y + z]\nresultat &lt;- dt[ ,\n                .(moyenne = mean(total, na.rm = TRUE)),\n                by = x\n                ][order(moyenne)]\nresultat\n\n   x moyenne\n1: A      10\n2: B      11\n3: C      12\n\n\n\n19.2.3.3 … car on peut facilement faire des erreurs\nL’enchaînement des opérations en data.table est puissant, mais peut aboutir à des résultats non désirés si on ne fait pas attention. Les exemples de ce paragraphe utilisent la fonction := ; si vous ne la connaissez pas encore, il est fortement conseillé de lire la section La fonction d’assignation par référence (ou :=) avant de poursuivre la lecture.\nVoici deux exemples d’opérations enchaînées en data.table dont les codes sont très similaires et qui aboutissent à des résultats très différents. Le premier exemple ne conserve qu’une partie de la table dt puis crée une variable, tandis que le second crée une variable avec une valeur non manquante pour une partie de la table uniquement.\n\n\n\n\n\n\n\nExemple 1\n\n\nExemple 2\n\n\n\n\n\nCode\n\n\n\ndt[y &gt; 3][ , newvar := 1]\n\n\n\n\ndt[y &gt; 3, newvar := 1]\n\n\n\n\n\n\nSignification\n\n\n\n\nPartir de dt, conserver uniquement les observations pour lesquelles x &gt; 3, et créer une nouvelle variable newvar qui vaut 1 partout\n\n\n\n\nPartir de dt, créer une nouvelle variable newvar qui vaut 1 pour les observations pour lesquelles x &gt; 3 et NA ailleurs\n\n\n\n\n\n\n\n\n19.3 Manipuler des tables de données avec data.table\n\nNous allons illustrer les fonctions de manipulation de données de data.table avec les jeux de données du package doremifasolData.\n\nlibrary(doremifasolData)\n\n\n19.3.1 Mettre des données dans un data.table\n\nIl y a principalement deux méthodes pour mettre des données sous forme d’un data.table :\n\nla fonction fread() importe un fichier plat comme les .csv (voir la fiche Importer des fichiers plats (.csv, .tsv, .txt) ;\nLes fonctions setDT() et as.data.table() convertissent un data.frame en data.table.\n\nDans la suite de cette section, on va illustrer les opérations de base en data.table avec la base permanente des équipements (table bpe_ens_2018), qu’on transforme en data.table.\n\n# Charger la base permanente des équipements\nbpe_ens_2018 &lt;- doremifasolData::bpe_ens_2018\n# Convertir ce data.frame en data.table\nbpe_ens_2018_dt &lt;- as.data.table(bpe_ens_2018)\n\n\n19.3.2 Manipuler une seule table avec data.table\n\n\n19.3.2.1 Sélectionner des lignes\nOn peut sélectionner des lignes dans un data.table avec dt[i]. Voici un exemple de code qui sélectionne les magasins de chaussures (TYPEQU == \"B304\") dans le premier arrondissement de Paris (DEPCOM == \"75101\") dans la table bpe_ens_2018_dt :\n\nselection &lt;- bpe_ens_2018_dt[DEPCOM == \"75101\" & TYPEQU == \"B304\"]\n\n\n\n\n\n\n\nNote\n\n\n\nVoici une remarque très importante sur le fonctionnement de data.table : lorsqu’on souhaite conserver toutes les lignes d’un data.table, il faut laisser vide l’emplacement pour i, sans oublier la virgule. Par exemple, pour connaître le nombre de lignes de iris_dt, on écrit : iris_dt[ , .N]. Notez bien l’emplacement vide et la virgule après [.\n\n\n\n19.3.2.2 Sélectionner des colonnes\nOn peut sélectionner des colonnes dans un data.table et renvoyer un data.table de plusieurs façons.\n\nLa première consiste à indiquer les colonnes à conserver sous forme de liste. La notation .() est un alias pour list() qui est pratique et concis dans un code data.table. Le code suivant sélectionne le code commune, le type d’équipement et le nombre d’équipement dans la base permanente des équipements, de deux façons équivalentes :\n\n\nbpe_ens_2018_dt[ , list(DEPCOM, TYPEQU, NB_EQUIP)]\nbpe_ens_2018_dt[ , .(DEPCOM, TYPEQU, NB_EQUIP)]\n\n\nLa seconde méthode consiste à utiliser un mot-clé de data.table, .SD qui signifie Subset of Data. On indique les colonnes qui seront aliasées par .SD avec la dimension .SDcols.\n\n\nbpe_ens_2018_dt[ , .SD, .SDcols = c(\"DEPCOM\", \"TYPEQU\", \"NB_EQUIP\")]\n\n\n\n\n\n\n\nNote\n\n\n\nLa seconde méthode peut vous sembler inutilement complexe. C’est vrai dans l’exemple donné ci-dessus, mais les fonctions .SD et .SDcols s’avèrent très puissantes dans un grand nombre de situations (notamment quand on veut programmer des fonctions qui font appel à data.table).\n\n\n\n19.3.2.3 Trier un data.table\n\nOn peut trier un data.table avec la fonction order(). Le code suivant trie la BPE selon le code commune et le type d’équipement.\n\nbpe_ens_2018_dt[order(DEPCOM, TYPEQU)]\n\n         REG DEP DEPCOM DCIRIS   AN TYPEQU NB_EQUIP\n      1:  84  01  01001  01001 2018   A401        2\n      2:  84  01  01001  01001 2018   A404        4\n      3:  84  01  01001  01001 2018   A504        1\n      4:  84  01  01001  01001 2018   A507        1\n     ---                                           \n1035561:  06 976  97617  97617 2018   F113        4\n1035562:  06 976  97617  97617 2018   F114        1\n1035563:  06 976  97617  97617 2018   F120        1\n1035564:  06 976  97617  97617 2018   F121        3\n\n\nIl suffit d’ajouter un signe - devant une variable pour trier par ordre décroissant. Le code suivant trie la BPE par code commune croissant et type d’équipement décroissant.\n\nbpe_ens_2018_dt[order(DEPCOM, -TYPEQU)]\n\n\n19.3.2.4 Calculer des statistiques\nLa méthode pour sélectionner des colonnes est également valable pour calculer des statistiques, car data.table accepte les expressions dans j. Le code suivant calcule le nombre total d’équipements dans la BPE, sum(NB_EQUIP, na.rm = TRUE) :\n\nbpe_ens_2018_dt[ , .(sum(NB_EQUIP, na.rm = TRUE))]\n\n        V1\n1: 2504782\n\n\nIl est possible de calculer plusieurs statistiques à la fois, et de donner des noms aux variables ; il suffit de séparer les formules par une virgule. Le code suivant calcule le nombre total d’équipements dans la BPE sum(NB_EQUIP, na.rm = TRUE), et le nombre total de boulangeries sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE).\n\nbpe_ens_2018_dt[ , \n                 .(NB_EQUIP_TOT   = sum(NB_EQUIP, na.rm = TRUE),\n                   NB_BOULANG_TOT = sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE))]\n\n   NB_EQUIP_TOT NB_BOULANG_TOT\n1:      2504782          48568\n\n\nOn peut évidemment combiner i et j pour calculer des statistiques sur un sous-ensemble d’observations. Dans l’exemple suivant, on sélectionne les boulangeries avec i, (TYPEQU == \"B203\"), et on calcule le nombre total d’équipements avec j, sum(NB_EQUIP, na.rm = TRUE).\n\nbpe_ens_2018_dt[TYPEQU == \"B203\", .(NB_BOULANG_TOT = sum(NB_EQUIP, na.rm = TRUE))]\n\n   NB_BOULANG_TOT\n1:          48568\n\n\n\n19.3.2.5 Les fonctions statistiques utiles de data.table\n\nVous pouvez utiliser toutes les fonctions statistiques de R avec data.table. Le package data.table propose par ailleurs des fonctions optimisées qui peuvent vous être utiles. En voici quelques-unes :\n\n\n\n\n\n\n\nFonction\nOpération\nExemple\n\n\n\n.N\nNombre d’observations\ndt[ , .N, by = 'group_var']\n\n\nuniqueN()\nNombre de valeurs uniques de la variable x\n\ndt[ , uniqueN(x), by = 'group_var']\n\n\nnafill\nRemplit les valeurs manquantes d’une variable numérique, par exemple par 123 (pour plus d’options, voir l’aide ?nafill)\ndt[ , nafill(y, fill = 123)]\n\n\n%chin%\nChaîne de caractères dans la liste\ndt[x %chin% c(\"a\", \"b\")]\n\n\n%between%\nValeur entre deux nombres\ndt[x %between% c(5,13)]\n\n\n%like%\nReconnaissance d’une chaîne de caractères (expression régulière)\ndt[departement %like% \"^Haute\"]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa fonction .N permet de créer facilement des compteurs avec la syntaxe 1:.N ou seq(.N). Par exemple dt[ , compteur := seq(.N), by = 'x'] permet de créer une variable compteur qui vaut de 1 à N pour chaque groupe d’observations défini par x.\n\n\n\n19.3.2.6 Opérations par groupe\nToutes les opérations précédentes peuvent être réalisées par groupe. Il suffit d’ajouter le nom des variables de groupe dans by (c’est l’équivalent du group_by() du package dplyr). Lorsqu’il y a plusieurs variables de groupe, on peut écrire l’argument by de deux façons :\n\nsoit by = c(\"var1\", \"var2\", \"var3\") (attention aux guillemets) ;\nsoit by = .(var1, var2, var3) (attention à la notation .()).\n\nLe code suivant groupe les données de la BPE par département, by = .(DEP), puis calcule le nombre total d’équipements, sum(NB_EQUIP, na.rm = TRUE) et le nombre total de boulangeries, sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE).\n\nbpe_ens_2018_dt[ , \n                 .(NB_EQUIP_TOT = sum(NB_EQUIP, na.rm = TRUE),\n                   NB_BOULANG_TOT = sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE)), \n                 by = .(DEP)]\n\n     DEP NB_EQUIP_TOT NB_BOULANG_TOT\n  1:  01        21394            401\n  2:  02        15534            339\n  3:  03        12216            299\n  4:  04         8901            185\n ---                                \n 98: 972        19068            370\n 99: 973         7852             98\n100: 974        30767            646\n101: 976         7353            101\n\n\n\n\n\n\n\n\nNote\n\n\n\nL’argument by fonctionne également avec l’opérateur :=. Vous pouvez en apprendre davantage sur l’usage de cet opérateur dans la partie La fonction d’assignation par référence (ou :=).\n\n\n\n19.3.3 Joindre des tables avec data.table\n\nPour joindre des données, data.table propose une fonction merge() plus rapide que la fonction de base. La syntaxe générale est z &lt;- merge(x, y, [options]). Voici une liste des principales options (les autres options sont consultables avec ?data.table::merge) :\n\n\n\n\n\n\nOption\nSignification\n\n\n\nby = var_jointure\nJoindre sur la variable var_jointure (présente dans x et dans y)\n\n\nby.x = \"identx\", by.y = \"identy\"\nJoindre sur la condition identx == identy\n\n\n\nall.x = TRUE\n\nLeft join (garder toutes les lignes de x)\n\n\nall.y = TRUE\n\nRight join (garder toutes les lignes de y)\n\n\nall = TRUE\n\nFull join (garder toutes les lignes de x et de y)\n\n\n\nEnfin, il est possible de réaliser des jointures plus sophistiquées avec data.table. Ces méthodes sont présentées dans la vignette sur le sujet.\n\n19.3.4 Indexer une table avec data.table\n\nL’indexation est une fonctionnalité très puissante pour accélérer les opérations sur les lignes (filtres, jointures, etc.) en data.table. Pour indexer une table il faut déclarer les variables faisant office de clé (appelées key). C’est possible de la manière suivante : setkey(dt, a) ou setkeyv(dt, \"a\"). Le data.table sera réordonné en fonction de cette variable et l’algorithme de recherche sur les lignes sera ainsi beaucoup plus efficace. Lorsqu’il y a plusieurs variables-clé, on écrit setkey(dt, a, b) ou setkeyv(dt, c(\"a\",\"b\")).\nPour savoir si un data.table est déjà indexé, on peut exécuter la commande key(dt) qui renvoie le nom des clés s’il y en a, et NULL sinon.\n\n\n\n\n\n\nTip\n\n\n\nL’exécution de la fonction data.table::setkey() peut prendre un peu de temps (parfois quelques minutes sur une table de plus de 10 millions de lignes), car data.table trie toute la table en fonction des variables-clé. Toutefois, c’est une étape vraiment utile car elle accélère considérablement les opérations ultérieures sur les lignes. Il est vivement recommandé de l’utiliser si une ou plusieurs variables vont régulièrement servir à filtrer ou combiner des données. Pour aller plus loin, voir cette vignette.\n\n\n\n19.3.5 Réorganiser les données en data.table\n\nLe package data.table permet de réorganiser facilement une table de données avec les fonctions dcast() et melt(). La fonction melt() réorganise les données dans un format long. La fonction dcast() réorganise les données dans un format wide.\n\n\n\n\n\n\nmelt()\ndcast()\n\n\n\nRéorganiser les données dans un format long\n\nRéorganise les données dans un format wide\n\n\n\n\n\n\n\n\n\n19.3.5.1 melt : transformer des colonnes en lignes\nLa fonction melt() réorganise les donnée dans un format long. Elle prend les arguments suivants :\n\n\ndata : les données ;\n\nid.vars : les variables qui identifient les lignes de table d’arrivée ; elles restent inchangées lors de l’utilisation de melt() ;\n\nmeasure.vars : les variables qui sont transposées ;\n\nvariable.name : le nom de la nouvelle colonne qui contient le nom des variables transposées ;\n\nvalue.name : le nom de la nouvelle colonne qui contient la valeur des variables transposées.\n\nPour illustrer l’usage de cette fonction, nous allons utiliser les données du répertoire Filosofi 2016 agrégées au niveau des EPCI (table filosofi_epci_2016), et disponibles dans le package doremifasolData. On convertit cette table en data.table et on conserve uniquement certaines variables.\n\n# Charger la table de Filosofi\nfilosofi_epci_2016 &lt;- doremifasolData::filosofi_epci_2016\n# Convertir la table en data.table\nfilosofi_epci_2016_dt &lt;- as.data.table(filosofi_epci_2016)\n# Sélectionner des colonnes\nfilosofi_epci_2016_dt &lt;- \n  filosofi_epci_2016_dt[, .(CODGEO, TP6016, TP60AGE116, TP60AGE216, \n                            TP60AGE316, TP60AGE416, TP60AGE516, TP60AGE616)]\n\nNous allons restructurer cette table pour obtenir une nouvelle table, avec une observation par EPCI et par tranche d’âge. Voici le code qui permet d’obtenir cette table : on indique dans measure.vars le nom des colonnes qui seront transposées, le nom des colonnes transposées sera indiqué dans la nouvelle colonne “tranche_age” (variable.name = \"tranche_age\") et les valeurs des colonnes transposées seront stockées dans la colonne “taux_pauvrete” (value.name = \"taux_pauvrete\").\n\ndonnees_pauvrete_long &lt;- \n  melt(data = filosofi_epci_2016_dt, \n       id.vars = c(\"CODGEO\"), \n       measure.vars = c(\"TP6016\", \"TP60AGE116\", \"TP60AGE216\", \n                        \"TP60AGE316\", \"TP60AGE416\", \"TP60AGE516\", \"TP60AGE616\"),\n       variable.name = \"tranche_age\",\n       value.name    = \"taux_pauvrete\"\n  )\ndonnees_pauvrete_long\n\n         CODGEO tranche_age taux_pauvrete\n   1: 200000172      TP6016           8.8\n   2: 200000438      TP6016           8.0\n   3: 200000545      TP6016          23.7\n   4: 200000628      TP6016          20.1\n  ---                                    \n8705: 249740085  TP60AGE616          41.5\n8706: 249740093  TP60AGE616          43.4\n8707: 249740101  TP60AGE616          39.8\n8708: 249740119  TP60AGE616          31.7\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl est recommandé de travailler avec des données en format long plutôt qu’en format wide, notamment lorsque vous voulez faire des graphiques. En effet, le package de visualisation graphique ggplot2 est optimisé pour manipuler des données en format long (voir la fiche [Faire des graphiques avec ggplot2]). Ce conseil est particulièrement important si vous voulez représenter un graphique avec des groupes : il est préférable que les groupes soient empilés (format long) plutôt que juxtaposés (format wide), car le code est plus rapide et facile à écrire.\n\n\n\n19.3.5.2 dcast : transformer des lignes en colonnes\nLa fonction dcast() réorganise les donnée dans un format large. Elle prend les arguments suivants :\n\n\ndata : les données ;\n\nformula : une formule de la forme var_ligne ~ var_colonne qui définit la structure de la nouvelle table ;\n\ns’il y a plusieurs variables, la formule prend la forme var1 + var2 ~ var3 ;\n\ndcast() conserve une ligne par valeur de la partie gauche, et crée (au moins) une colonne par valeur de la partie droite ;\n\n\n\nfun.aggregate : une liste contenant la ou les fonction(s) utilisées pour agréger les données le cas échéant ; exemple : list(mean, sum, sd) ;\n\nvalue.var : un vecteur contenant le nom de la ou des colonne(s) dont les valeurs vont être transposées ; exemple : c(\"var1\", \"var2\").\n\nDans l’exemple qui suit, on réorganise la table bpe_ens_2018_dt de façon à obtenir une table qui contient une ligne par type d’équipement et une colonne par région (TYPEQU ~ REG). Ces colonnes vont contenir la somme (fun.aggregate = sum) du nombre d’équipements (value.var = \"NB_EQUIP\").\n\nbpe_ens_2018_wide &lt;- dcast(bpe_ens_2018_dt, \n                           TYPEQU ~ REG, \n                           value.var = \"NB_EQUIP\", \n                           fun.aggregate = sum)\nhead(bpe_ens_2018_wide)\n\n   TYPEQU 01 02 03 04 06  11  24  27  28  32  44  52  53  75  76  84  93 94\n1:   A101  2  2  1  7  0 191  28  23  54 127  80  15  20  66  55  69  34  3\n2:   A104 20 21 16 28  5  91 153 230 183 214 319 173 157 407 406 423 179 39\n3:   A105  1  1  1  1  1   2   2   2   2   2   4   1   1   5   3   4   1  1\n4:   A106  2  1  2  2  1  10   7  12  10  17  17   8   8  19  19  21  11  2\n5:   A107  2  1  1  4  1  60   9  19  15  26  30  11  12  28  26  34  23  2\n6:   A108  2  1  1  2  1  19   9  13  13  25  21   8  10  21  20  28  14  2\n\n\nIl est possible d’utiliser dcast() avec plusieurs variables à transposer et plusieurs fonctions pour transposer. Dans l’exemple qui suit, on obtient une ligne par type d’équipement, et une colonne par région et par fonction d’agrégation (mean et sum).\n\nbpe_ens_2018_wide2 &lt;- dcast(bpe_ens_2018_dt, \n                            TYPEQU ~ REG, \n                            value.var = \"NB_EQUIP\", \n                            fun.aggregate = list(sum, mean))\nbpe_ens_2018_wide2\n\n     TYPEQU NB_EQUIP_sum_01 NB_EQUIP_sum_02 NB_EQUIP_sum_03 NB_EQUIP_sum_04\n  1:   A101               2               2               1               7\n  2:   A104              20              21              16              28\n  3:   A105               1               1               1               1\n  4:   A106               2               1               2               2\n ---                                                                       \n183:   G101             105              79              48             136\n184:   G102              49              49              29             112\n185:   G103               0               0               0               0\n186:   G104             110             104              46              98\n     NB_EQUIP_sum_06 NB_EQUIP_sum_11 NB_EQUIP_sum_24 NB_EQUIP_sum_27\n  1:               0             191              28              23\n  2:               5              91             153             230\n  3:               1               2               2               2\n  4:               1              10               7              12\n ---                                                                \n183:              20            3351             213             256\n184:              11            2478             670             890\n185:               0              96             238             330\n186:               6            2993             293             339\n     NB_EQUIP_sum_28 NB_EQUIP_sum_32 NB_EQUIP_sum_44 NB_EQUIP_sum_52\n  1:              54             127              80              15\n  2:             183             214             319             173\n  3:               2               2               4               1\n  4:              10              17              17               8\n ---                                                                \n183:             272             495             593             376\n184:             845             698            1318             762\n185:             378             521             368             646\n186:             401             354             533             330\n     NB_EQUIP_sum_53 NB_EQUIP_sum_75 NB_EQUIP_sum_76 NB_EQUIP_sum_84\n  1:              20              66              55              69\n  2:             157             407             406             423\n  3:               1               5               3               4\n  4:               8              19              19              21\n ---                                                                \n183:             345             720             786            1166\n184:             943            1908            1982            2797\n185:             751            1408            1437            1265\n186:             374             926             932            1099\n     NB_EQUIP_sum_93 NB_EQUIP_sum_94 NB_EQUIP_mean_01 NB_EQUIP_mean_02\n  1:              34               3         1.000000         1.000000\n  2:             179              39         1.000000         1.000000\n  3:               1               1         1.000000         1.000000\n  4:              11               2         1.000000         1.000000\n ---                                                                  \n183:            1016             120         2.282609         1.975000\n184:            2111             438         2.450000         2.130435\n185:             718             187              NaN              NaN\n186:             876             182         1.718750         1.575758\n     NB_EQUIP_mean_03 NB_EQUIP_mean_04 NB_EQUIP_mean_06 NB_EQUIP_mean_11\n  1:         1.000000         1.000000              NaN         1.091429\n  2:         1.000000         1.000000         1.000000         1.000000\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.714286         1.837838         5.000000         2.080074\n184:         1.318182         2.036364         1.833333         2.250681\n185:              NaN              NaN              NaN         1.103448\n186:         1.533333         1.400000         1.500000         1.780488\n     NB_EQUIP_mean_24 NB_EQUIP_mean_27 NB_EQUIP_mean_28 NB_EQUIP_mean_32\n  1:         1.037037         1.000000         1.018868         1.058333\n  2:         1.000000         1.004367         1.000000         1.014218\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.601504         1.422222         1.511111         1.633663\n184:         1.763158         1.666667         1.978923         1.681928\n185:         1.048458         1.103679         1.330986         1.527859\n186:         1.140078         1.232727         1.297735         1.156863\n     NB_EQUIP_mean_44 NB_EQUIP_mean_52 NB_EQUIP_mean_53 NB_EQUIP_mean_75\n  1:         1.025641         1.000000         1.000000         1.157895\n  2:         1.009494         1.005814         1.000000         1.007426\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.694286         1.748837         1.674757         1.578947\n184:         1.734211         1.836145         2.063457         1.927273\n185:         1.153605         2.044304         1.891688         1.733990\n186:         1.230947         1.274131         1.307692         1.293296\n     NB_EQUIP_mean_76 NB_EQUIP_mean_84 NB_EQUIP_mean_93 NB_EQUIP_mean_94\n  1:         1.145833         1.029851         1.000000         1.000000\n  2:         1.015000         1.011962         1.028736         1.083333\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.526214         1.707174         1.785589         2.000000\n184:         2.045408         2.109351         2.507126         3.369231\n185:         1.600223         1.408686         1.681499         2.101124\n186:         1.226316         1.345165         1.364486         1.857143\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nLa fonction dcast() crée une colonne par valeur des variables utilisées dans la partie droite de la formule. Il faut donc faire attention à ce que ces variables aient un nombre limité de valeurs, pour ne pas obtenir une table extrêmement large. On peut éventuellement discrétiser les variables continues, ou regrouper les modalités avant d’utiliser dcast().\n\nOn peut obtenir des noms de colonnes peu significatifs lorsqu’on utilise dcast() avec une fonction d’agrégation. Il est conseillé de modifier légèrement la partie droite de la formule pour obtenir des noms plus significatifs. Voici un exemple où on ajoute le préfixe resultat_region :\n\nbpe_ens_2018_wide2 &lt;- dcast(bpe_ens_2018_dt, \n                         TYPEQU ~ paste0(\"resultat_region\",REG), \n                         value.var = \"NB_EQUIP\", \n                         fun.aggregate = sum)\nhead(bpe_ens_2018_wide2)\n\n   TYPEQU resultat_region01 resultat_region02 resultat_region03\n1:   A101                 2                 2                 1\n2:   A104                20                21                16\n3:   A105                 1                 1                 1\n4:   A106                 2                 1                 2\n5:   A107                 2                 1                 1\n6:   A108                 2                 1                 1\n   resultat_region04 resultat_region06 resultat_region11 resultat_region24\n1:                 7                 0               191                28\n2:                28                 5                91               153\n3:                 1                 1                 2                 2\n4:                 2                 1                10                 7\n5:                 4                 1                60                 9\n6:                 2                 1                19                 9\n   resultat_region27 resultat_region28 resultat_region32 resultat_region44\n1:                23                54               127                80\n2:               230               183               214               319\n3:                 2                 2                 2                 4\n4:                12                10                17                17\n5:                19                15                26                30\n6:                13                13                25                21\n   resultat_region52 resultat_region53 resultat_region75 resultat_region76\n1:                15                20                66                55\n2:               173               157               407               406\n3:                 1                 1                 5                 3\n4:                 8                 8                19                19\n5:                11                12                28                26\n6:                 8                10                21                20\n   resultat_region84 resultat_region93 resultat_region94\n1:                69                34                 3\n2:               423               179                39\n3:                 4                 1                 1\n4:                21                11                 2\n5:                34                23                 2\n6:                28                14                 2\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl est conseillé de bien réfléchir avant de restructurer des données en format wide, et de ne le faire que lorsque cela paraît indispensable. En effet, s’il est tentant de restructurer les données sous format wide car ce format peut paraître plus intuitif, il est généralement plus simple et plus rigoureux de traiter les données en format long. Ceci dit, il existe des situations dans lesquelles il est indiqué de restructurer les données en format wide. Voici deux exemples :\n\nproduire un tableau synthétique de résultats, prêt à être diffusé, avec quelques colonnes donnant des indicateurs par catégorie (exemple : la table filosofi_epci_2016 du package doremifasolData) ;\nproduire une table avec une colonne par année, de façon à calculer facilement un taux d’évolution entre deux dates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n19.4 La fonction d’assignation par référence (ou :=)\nJusqu’à présent, nous avons manipulé un data.table existant, mais nous ne lui avons pas ajouté de nouvelles colonnes. Pour ce faire, nous allons utiliser la fonction := qui s’appelle “assignation par référence” et qui peut également s’appeler comme une fonction `:=`(), prenant ses arguments entre parenthèses. Voici comment on crée une nouvelle colonne dans bpe_ens_2018_dt :\n\nbpe_ens_2018_dt[ , nouvelle_colonne :=  NB_EQUIP * 10]\nhead(bpe_ens_2018_dt)\n\n   REG DEP DEPCOM DCIRIS   AN TYPEQU NB_EQUIP nouvelle_colonne\n1:  84  01  01001  01001 2018   A401        2               20\n2:  84  01  01001  01001 2018   A404        4               40\n3:  84  01  01001  01001 2018   A504        1               10\n4:  84  01  01001  01001 2018   A507        1               10\n5:  84  01  01001  01001 2018   B203        1               10\n6:  84  01  01001  01001 2018   C104        1               10\n\n\n\n19.4.1 La spécificité de data.table : la modification par référence\nA première vue, on peut penser que la fonction := est l’équivalent de la fonction dplyr::mutate() dans la grammaire data.table. C’est vrai dans la mesure où elle permet de faire des choses similaires, mais il faut garder en tête que son fonctionnement est complètement différent de celui de dplyr::mutate(). En effet, la grande spécificité de data.table par rapport à dplyr est que l’utilisation de la fonction := modifie directement la table de données, car data.table fonctionne sur le principe de la modification par référence (voir ce lien pour plus de détails). Cela signifie en pratique qu’il ne faut pas réassigner l’objet lorsqu’on modifie une de ses colonnes. C’est ce comportement qui permet à data.table d’être très rapide et très économe en mémoire vive, rendant son usage approprié pour des données volumineuses.\nPour créer une colonne, on écrit donc directement dt[ , nouvelle_colonne := une_formule] et non dt &lt;- dt[ , nouvelle_colonne := une_formule]. Voici un exemple qui compare dplyr et data.table :\n\n\n\n\n\n\n\n\n\n\nPackage\n\n\nCode\n\n\nCommentaire\n\n\n\n\n\ndplyr\n\n\n\n\nbpe_ens_2018 &lt;- \n  bpe_ens_2018 %&gt;%\n  dplyr::mutate(nouvelle_colonne =  NB_EQUIP * 10)\n\n\n\nIl faut utiliser une assignation (&lt;-) pour modifier la table.\n\n\n\n\n\ndata.table\n\n\n\n\nbpe_ens_2018_dt[ , nouvelle_colonne :=  NB_EQUIP * 10]\n\n\n\nIl ne faut pas d’assignation pour modifier la table, qui est modifiée par référence.\n\n\n\n\n19.4.2 Les usages de :=\n\nOn peut se servir de la fonction := de multiples façons, et avec plusieurs notations.\n\n19.4.2.1 Créer plusieurs variables à la fois\nVoici comment créer plusieurs variables à la fois avec :=, en utilisant une notation vectorielle :\n\nbpe_ens_2018_dt[ , c(\"nouvelle_colonne1\", \"nouvelle_colonne2\") :=  \n                   list(NB_EQUIP * 2, NB_EQUIP + 3)]\n\nOn peut faire exactement la même chose en utilisant la notation `:=`(). Voici le même exemple écrit avec `:=`().\n\nbpe_ens_2018_dt[ , `:=`(nouvelle_colonne1 = NB_EQUIP * 2,\n                        nouvelle_colonne2 = NB_EQUIP + 3)]\n\n\n\n\n\n\n\nNote\n\n\n\nSi vous utilisez la notation`:=`(), alors il faut utiliser uniquement = à l’intérieur des parenthèses pour créer ou modifier des variables, et non :=. Par exemple,\n\ndt[ , `:=`(var1 = \"Hello\", var2 = \"world\")]\n\n\n\n\n19.4.2.2 Supprimer une colonne\nOn peut facilement supprimer une colonne en lui assignant la valeur NULL (c’est hyper rapide !). Voici un exemple :\n\nbpe_ens_2018_dt[ , NB_EQUIP := NULL]\n\n\n19.4.2.3 Faire un remplacement conditionnel\nLa fonction := peut être utilisée pour modifier une colonne pour certaines lignes seulement, en fonction d’une condition logique. C’est beaucoup plus efficace qu’un terme dplyr::if_else() ou dplyr::case_when(). Imaginons qu’on veuille créer une colonne EQUIP_HORS_CHAUSS égale au nombre d’équipements (NB_EQUIP) sauf pour les lignes correspondantes à des magasins de chaussures (TYPEQU == \"B304\") où elle vaut NA. Dans ce cas, le code dplyr serait :\n\nbpe_ens_2018 %&gt;% \n  dplyr::mutate(NB_EQUIP_HORS_CHAUSS = dplyr::case_when(\n    TYPEQU == \"B304\" ~ NA_real_,\n    TRUE ~ NB_EQUIP)\n  )\n\nDeux alternatives existent en data.table nécessitant toutes deux beaucoup moins de mémoire vive :\n\nbpe_ens_2018_dt[ , NB_EQUIP_HORS_CHAUSS := NB_EQUIP\n                 ][TYPEQU == \"B304\", NB_EQUIP_HORS_CHAUSS := NA_real_]\n\nou bien en utilisant la fonction data.table::fcase dont le fonctionnement ressemble à celui de dplyr::case_when :\n\nbpe_ens_2018_dt[ , NB_EQUIP_HORS_CHAUSS := data.table::fcase(TYPEQU == \"B304\", NA_real_,\n                                                   TYPEQU != \"B304\", NB_EQUIP)\n                                                   ]\n\n\n19.4.3 Attention en utilisant :=\n\nL’utilisation de la fonction := est déroutante lorsqu’on découvre data.table. Voici trois remarques qui vous feront gagner du temps :\n\n\nVous pouvez faire appel à d’autres fonctions à l’intérieur de la fonction :=. Par exemple, si on veut mettre la variable name en minuscules, on peut utiliser la fonction tolower(). On écrit alors :\n\ndt[ , name_minuscule := tolower(name)]\n\n\n\nLorsque l’on crée plusieurs variables avec la fonction :=, elles sont créées en même temps. On ne peut donc pas faire appel dans une formule à une variable qu’on crée dans le même appel à la fonction :=. Par exemple, le code suivant ne fonctionne pas :\n\nbpe_ens_2018_dt[ , `:=`(nouvelle_colonne1 =  NB_EQUIP * 2,\n                        nouvelle_colonne2 =  nouvelle_colonne1 + 3)]\n\nEn effet, au moment où la fonction := est exécutée, la colonne nouvelle_colonne1 n’existe pas encore, donc la formule nouvelle_colonne2 = nouvelle_colonne1 + 3 n’a pas encore de sens. Si vous créez des variables en chaîne, il faut décomposer l’opération en plusieurs étapes enchaînées. Voici le code qui permet de créer les deux colonnes à la suite :\n\nbpe_ens_2018_dt[ , nouvelle_colonne1 :=  NB_EQUIP * 2\n                ][ , nouvelle_colonne2 :=  nouvelle_colonne1 + 3]\n\n\n\nUn mauvais usage de la fonction := peut vous amener à écraser par erreur vos données. En effet, si vous exécuter par erreur la commande dt[ , ma_variable_importante := 0], vous écrasez la variable ma_variable_importante. Vous devez alors recharger vos données… Il faut donc bien réfléchir à ce que vous voulez faire avant de remplacer ou modifier une variable existante avec la fonction :=. Si vous modifiez un data.table dans une fonction, un filet de sécurité consiste à d’abord copier le data.table initial et ainsi faire les modifications sur le nouvel objet, de la manière suivante :\n\n  dt_copy &lt;- data.table::copy(dt)\n  dt_copy[, ma_variable_importante := \"Nouvelle valeur\"]\n\n\n\n19.5 Programmer des fonctions avec data.table\n\nUne des forces de data.table est qu’il est relativement simple d’utiliser ce package dans des fonctions. Pour illustrer l’usage des fonctions, nous allons utiliser la table filosofi_com_2016 disponible dans le package doremifasolData. Cette table donne des informations sur les revenus des ménages au niveau communal. Nous créons une variable donnant le numéro du département (departement) en extrayant les deux premiers caractères du code commune (CODGEO) avec la fonction str_sub du package stringr (vous pouvez consulter la fiche [Manipuler des données textuelles pour en apprendre davantage sur stringr]). Enfin, nous utilisons la fonction .SD pour sélectionner uniquement quelques variables.\n\n# Charger la table de données et la transformer en data.table\nfilosofi_com_2016_dt &lt;- as.data.table(doremifasolData::filosofi_com_2016)\n# Créer une variable donnant le département\nfilosofi_com_2016_dt[, departement := stringr::str_sub(CODGEO, start = 1L, end = 2L)]\n# Supprimer les départements d'outre-mer\nfilosofi_com_2016_dt &lt;- filosofi_com_2016_dt[departement  != \"97\"]\n# Alléger la table en ne conservant que quelques variables\nfilosofi_com_2016_dt &lt;- \n  filosofi_com_2016_dt[, .SD, .SDcols = c(\"departement\", \"CODGEO\", \"NBMENFISC16\",\n                                          \"NBPERSMENFISC16\")]\n\n\n19.5.1 Utiliser .SD et lapply\n\nLe mot clé .SD (Subset of Data) permet d’appliquer la même opération sur plusieurs colonnes. Les colonnes auxquelles l’opération s’applique sont contrôlées par l’argument .SDcols (par défaut, toutes les colonnes sont traitées). Le mot clé .SD est régulièrement utilisé en conjonction avec la fonction lapply. Cette syntaxe, très puissante, permet également d’avoir des codes assez compacts, ce qui les rend plus lisible.\nUn usage classique de ce duo lapply+.SD consiste à écrire des fonctions de statistiques descriptives. Par exemple, imaginons qu’on souhaite calculer la moyenne, l’écart-type et les quantiles (P25, P50 et P75) de nombreuses colonnes. On peut alors définir la fonction suivante :\n\nmes_statistiques &lt;- \n  function(x) return(c(mean(x, na.rm = TRUE), \n                       sd(x, na.rm = TRUE), \n                       quantile(x, probs = c(.25,.5,.75), na.rm = TRUE)))\n\nVoici comment on peut appliquer cette fonction aux colonnes NBMENFISC16 (nombre de ménages fiscaux) et NBPERSMENFISC16 (nombre de personnes dans les ménages fiscaux) de la table filosofi_com_2016_dt :\n\ndata_agregee &lt;- \n  filosofi_com_2016_dt[ ,\n                        lapply(.SD, mes_statistiques), \n                        .SDcols = c(\"NBMENFISC16\", \"NBPERSMENFISC16\")]\ndata_agregee[, 'stat' := c(\"moyenne\",\"écart-type\",\"P25\",\"P50\",\"P75\")]\ndata_agregee\n\n   NBMENFISC16 NBPERSMENFISC16       stat\n1:    916.3269         2097.18    moyenne\n2:   7382.4628        15245.60 écart-type\n3:    105.0000          250.50        P25\n4:    218.0000          527.00        P50\n5:    526.0000         1278.25        P75\n\n\nIl est également très simple d’effectuer des calculs par groupe avec la méthode lapply+.SD. On peut par facilement adapter le code précédent pour calculer des statistiques descriptives par département (variable departement).\n\ndata_agregee &lt;- \n  filosofi_com_2016_dt[ ,\n                        lapply(.SD, mes_statistiques), \n                        by = departement,\n                        .SDcols = c(\"NBMENFISC16\", \"NBPERSMENFISC16\")]\ndata_agregee[, 'stat' := c(\"moyenne\",\"écart-type\",\"P25\",\"P50\",\"P75\"), by = departement]\ndata_agregee\n\n\n19.5.2 Définir des fonctions modifiant un data.table\n\nIl est très facile d’écrire avec data.table des fonctions génériques faisant appel à des noms de variables en arguments. Pour déclarer à data.table qu’un nom fait référence à une colonne, la manière la plus simple est d’utiliser la fonction get. Dans l’exemple suivant, on définit la fonction creation_var qui crée dans la table data une nouvelle variable (dont le nom est l’argument nouveau_nom) égale à une autre variable incrémentée (dont le nom est l’argument nom_variable) de 1. L’utilisation de la fonction get permet d’indiquer à data.table que la chaîne de caractères nom_variable désigne une colonne de la table data.\n\ncreation_var &lt;- function(data, nom_variable, nouveau_nom){\n  data[, c(nouveau_nom) := get(nom_variable) + 1]\n}\nhead(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\"), 2)\n\n   departement CODGEO NBMENFISC16 NBPERSMENFISC16 nouvelle_variable\n1:          01  01001         313           795.5               314\n2:          01  01002         101           248.0               102\n\n\nc(nouveau_nom) permet de s’assurer que data.table crée une nouvelle colonne dont le nom est défini en argument (et qui ne s’appelle donc pas nouveau_nom).\n\n\n\n\n\n\nNote\n\n\n\nLa version 1.14.1 de data.table (encore en développement) apporte une syntaxe améliorée dans ce cas et considère l’utilisation de get comme désuète. Le [...] admet un nouvel argument env à qui on donne tous les remplacements que l’on souhaite. L’exemple devient :\ncreation_var &lt;- function(data, nom_variable, nouveau_nom){\n  data[, nouveau_nom := nom_variable + 1, \n    env = list(nouveau_nom = nouveau_nom, nom_variable = nom_variable)]\n}\nhead(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\"), 2)\nL’avantage c’est qu’on peut effectuer de tels remplacements dans i (dimension ligne) et qu’on peut même remplacer des fonctions :\ncreation_var &lt;- function(data, nom_variable, nouveau_nom, fonction){\n  data[, nouveau_nom := fonction(nom_variable), \n    env = list(nouveau_nom = nouveau_nom, nom_variable = nom_variable, fonction= fonction)]\n  head(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\",\n                  fonction = \"sqrt\"), 2)\n}\n\n\n\n\n\n\n\n\nTip\n\n\n\nLorsqu’on définit des fonctions pour effectuer des traitements génériques, une précaution est nécessaire pour ne pas modifier les données en entrée de la fonction si l’opérateur := est utilisée. Il est recommandé dans ce cas de créer une copie du dataframe en entrée (data.table::copy(df)) et d’effectuer les traitements sur cette copie.\n\n\n\n19.6 Pour en savoir plus\n\nla documentation officielle de data.table (en anglais) ;\nles vignettes de data.table :\n\n\nIntroduction à data.table (en anglais) ;\n\nModification par référence (en anglais) ;\n\nla foire aux questions de data.table (en anglais) ;\n\n\nune cheatsheet sur data.table (en anglais) ;\nune introduction à l’utilisation de l’opérateur [...] (en français) ;\nCe cours complet sur data.table (en français).\nCette présentation des fonctionnalités du package (en français) ;\nCe post sur les fonctions utilisant data.table.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manipuler des données avec `data.table`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_datatable.html#manipuler-des-tables-de-données-avec-data.table",
    "href": "03_Fiches_thematiques/Fiche_datatable.html#manipuler-des-tables-de-données-avec-data.table",
    "title": "19  Manipuler des données avec data.table",
    "section": "\n19.3 Manipuler des tables de données avec data.table\n",
    "text": "19.3 Manipuler des tables de données avec data.table\n\nNous allons illustrer les fonctions de manipulation de données de data.table avec les jeux de données du package doremifasolData.\n\nlibrary(doremifasolData)\n\n\n19.3.1 Mettre des données dans un data.table\n\nIl y a principalement deux méthodes pour mettre des données sous forme d’un data.table :\n\nla fonction fread() importe un fichier plat comme les .csv (voir la fiche Importer des fichiers plats (.csv, .tsv, .txt) ;\nLes fonctions setDT() et as.data.table() convertissent un data.frame en data.table.\n\nDans la suite de cette section, on va illustrer les opérations de base en data.table avec la base permanente des équipements (table bpe_ens_2018), qu’on transforme en data.table.\n\n# Charger la base permanente des équipements\nbpe_ens_2018 &lt;- doremifasolData::bpe_ens_2018\n# Convertir ce data.frame en data.table\nbpe_ens_2018_dt &lt;- as.data.table(bpe_ens_2018)\n\n\n19.3.2 Manipuler une seule table avec data.table\n\n\n19.3.2.1 Sélectionner des lignes\nOn peut sélectionner des lignes dans un data.table avec dt[i]. Voici un exemple de code qui sélectionne les magasins de chaussures (TYPEQU == \"B304\") dans le premier arrondissement de Paris (DEPCOM == \"75101\") dans la table bpe_ens_2018_dt :\n\nselection &lt;- bpe_ens_2018_dt[DEPCOM == \"75101\" & TYPEQU == \"B304\"]\n\n\n\n\n\n\n\nNote\n\n\n\nVoici une remarque très importante sur le fonctionnement de data.table : lorsqu’on souhaite conserver toutes les lignes d’un data.table, il faut laisser vide l’emplacement pour i, sans oublier la virgule. Par exemple, pour connaître le nombre de lignes de iris_dt, on écrit : iris_dt[ , .N]. Notez bien l’emplacement vide et la virgule après [.\n\n\n\n19.3.2.2 Sélectionner des colonnes\nOn peut sélectionner des colonnes dans un data.table et renvoyer un data.table de plusieurs façons.\n\nLa première consiste à indiquer les colonnes à conserver sous forme de liste. La notation .() est un alias pour list() qui est pratique et concis dans un code data.table. Le code suivant sélectionne le code commune, le type d’équipement et le nombre d’équipement dans la base permanente des équipements, de deux façons équivalentes :\n\n\nbpe_ens_2018_dt[ , list(DEPCOM, TYPEQU, NB_EQUIP)]\nbpe_ens_2018_dt[ , .(DEPCOM, TYPEQU, NB_EQUIP)]\n\n\nLa seconde méthode consiste à utiliser un mot-clé de data.table, .SD qui signifie Subset of Data. On indique les colonnes qui seront aliasées par .SD avec la dimension .SDcols.\n\n\nbpe_ens_2018_dt[ , .SD, .SDcols = c(\"DEPCOM\", \"TYPEQU\", \"NB_EQUIP\")]\n\n\n\n\n\n\n\nNote\n\n\n\nLa seconde méthode peut vous sembler inutilement complexe. C’est vrai dans l’exemple donné ci-dessus, mais les fonctions .SD et .SDcols s’avèrent très puissantes dans un grand nombre de situations (notamment quand on veut programmer des fonctions qui font appel à data.table).\n\n\n\n19.3.2.3 Trier un data.table\n\nOn peut trier un data.table avec la fonction order(). Le code suivant trie la BPE selon le code commune et le type d’équipement.\n\nbpe_ens_2018_dt[order(DEPCOM, TYPEQU)]\n\n         REG DEP DEPCOM DCIRIS   AN TYPEQU NB_EQUIP\n      1:  84  01  01001  01001 2018   A401        2\n      2:  84  01  01001  01001 2018   A404        4\n      3:  84  01  01001  01001 2018   A504        1\n      4:  84  01  01001  01001 2018   A507        1\n     ---                                           \n1035561:  06 976  97617  97617 2018   F113        4\n1035562:  06 976  97617  97617 2018   F114        1\n1035563:  06 976  97617  97617 2018   F120        1\n1035564:  06 976  97617  97617 2018   F121        3\n\n\nIl suffit d’ajouter un signe - devant une variable pour trier par ordre décroissant. Le code suivant trie la BPE par code commune croissant et type d’équipement décroissant.\n\nbpe_ens_2018_dt[order(DEPCOM, -TYPEQU)]\n\n\n19.3.2.4 Calculer des statistiques\nLa méthode pour sélectionner des colonnes est également valable pour calculer des statistiques, car data.table accepte les expressions dans j. Le code suivant calcule le nombre total d’équipements dans la BPE, sum(NB_EQUIP, na.rm = TRUE) :\n\nbpe_ens_2018_dt[ , .(sum(NB_EQUIP, na.rm = TRUE))]\n\n        V1\n1: 2504782\n\n\nIl est possible de calculer plusieurs statistiques à la fois, et de donner des noms aux variables ; il suffit de séparer les formules par une virgule. Le code suivant calcule le nombre total d’équipements dans la BPE sum(NB_EQUIP, na.rm = TRUE), et le nombre total de boulangeries sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE).\n\nbpe_ens_2018_dt[ , \n                 .(NB_EQUIP_TOT   = sum(NB_EQUIP, na.rm = TRUE),\n                   NB_BOULANG_TOT = sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE))]\n\n   NB_EQUIP_TOT NB_BOULANG_TOT\n1:      2504782          48568\n\n\nOn peut évidemment combiner i et j pour calculer des statistiques sur un sous-ensemble d’observations. Dans l’exemple suivant, on sélectionne les boulangeries avec i, (TYPEQU == \"B203\"), et on calcule le nombre total d’équipements avec j, sum(NB_EQUIP, na.rm = TRUE).\n\nbpe_ens_2018_dt[TYPEQU == \"B203\", .(NB_BOULANG_TOT = sum(NB_EQUIP, na.rm = TRUE))]\n\n   NB_BOULANG_TOT\n1:          48568\n\n\n\n19.3.2.5 Les fonctions statistiques utiles de data.table\n\nVous pouvez utiliser toutes les fonctions statistiques de R avec data.table. Le package data.table propose par ailleurs des fonctions optimisées qui peuvent vous être utiles. En voici quelques-unes :\n\n\n\n\n\n\n\nFonction\nOpération\nExemple\n\n\n\n.N\nNombre d’observations\ndt[ , .N, by = 'group_var']\n\n\nuniqueN()\nNombre de valeurs uniques de la variable x\n\ndt[ , uniqueN(x), by = 'group_var']\n\n\nnafill\nRemplit les valeurs manquantes d’une variable numérique, par exemple par 123 (pour plus d’options, voir l’aide ?nafill)\ndt[ , nafill(y, fill = 123)]\n\n\n%chin%\nChaîne de caractères dans la liste\ndt[x %chin% c(\"a\", \"b\")]\n\n\n%between%\nValeur entre deux nombres\ndt[x %between% c(5,13)]\n\n\n%like%\nReconnaissance d’une chaîne de caractères (expression régulière)\ndt[departement %like% \"^Haute\"]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa fonction .N permet de créer facilement des compteurs avec la syntaxe 1:.N ou seq(.N). Par exemple dt[ , compteur := seq(.N), by = 'x'] permet de créer une variable compteur qui vaut de 1 à N pour chaque groupe d’observations défini par x.\n\n\n\n19.3.2.6 Opérations par groupe\nToutes les opérations précédentes peuvent être réalisées par groupe. Il suffit d’ajouter le nom des variables de groupe dans by (c’est l’équivalent du group_by() du package dplyr). Lorsqu’il y a plusieurs variables de groupe, on peut écrire l’argument by de deux façons :\n\nsoit by = c(\"var1\", \"var2\", \"var3\") (attention aux guillemets) ;\nsoit by = .(var1, var2, var3) (attention à la notation .()).\n\nLe code suivant groupe les données de la BPE par département, by = .(DEP), puis calcule le nombre total d’équipements, sum(NB_EQUIP, na.rm = TRUE) et le nombre total de boulangeries, sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE).\n\nbpe_ens_2018_dt[ , \n                 .(NB_EQUIP_TOT = sum(NB_EQUIP, na.rm = TRUE),\n                   NB_BOULANG_TOT = sum(NB_EQUIP * (TYPEQU == \"B203\"), na.rm = TRUE)), \n                 by = .(DEP)]\n\n     DEP NB_EQUIP_TOT NB_BOULANG_TOT\n  1:  01        21394            401\n  2:  02        15534            339\n  3:  03        12216            299\n  4:  04         8901            185\n ---                                \n 98: 972        19068            370\n 99: 973         7852             98\n100: 974        30767            646\n101: 976         7353            101\n\n\n\n\n\n\n\n\nNote\n\n\n\nL’argument by fonctionne également avec l’opérateur :=. Vous pouvez en apprendre davantage sur l’usage de cet opérateur dans la partie La fonction d’assignation par référence (ou :=).\n\n\n\n19.3.3 Joindre des tables avec data.table\n\nPour joindre des données, data.table propose une fonction merge() plus rapide que la fonction de base. La syntaxe générale est z &lt;- merge(x, y, [options]). Voici une liste des principales options (les autres options sont consultables avec ?data.table::merge) :\n\n\n\n\n\n\nOption\nSignification\n\n\n\nby = var_jointure\nJoindre sur la variable var_jointure (présente dans x et dans y)\n\n\nby.x = \"identx\", by.y = \"identy\"\nJoindre sur la condition identx == identy\n\n\n\nall.x = TRUE\n\nLeft join (garder toutes les lignes de x)\n\n\nall.y = TRUE\n\nRight join (garder toutes les lignes de y)\n\n\nall = TRUE\n\nFull join (garder toutes les lignes de x et de y)\n\n\n\nEnfin, il est possible de réaliser des jointures plus sophistiquées avec data.table. Ces méthodes sont présentées dans la vignette sur le sujet.\n\n19.3.4 Indexer une table avec data.table\n\nL’indexation est une fonctionnalité très puissante pour accélérer les opérations sur les lignes (filtres, jointures, etc.) en data.table. Pour indexer une table il faut déclarer les variables faisant office de clé (appelées key). C’est possible de la manière suivante : setkey(dt, a) ou setkeyv(dt, \"a\"). Le data.table sera réordonné en fonction de cette variable et l’algorithme de recherche sur les lignes sera ainsi beaucoup plus efficace. Lorsqu’il y a plusieurs variables-clé, on écrit setkey(dt, a, b) ou setkeyv(dt, c(\"a\",\"b\")).\nPour savoir si un data.table est déjà indexé, on peut exécuter la commande key(dt) qui renvoie le nom des clés s’il y en a, et NULL sinon.\n\n\n\n\n\n\nTip\n\n\n\nL’exécution de la fonction data.table::setkey() peut prendre un peu de temps (parfois quelques minutes sur une table de plus de 10 millions de lignes), car data.table trie toute la table en fonction des variables-clé. Toutefois, c’est une étape vraiment utile car elle accélère considérablement les opérations ultérieures sur les lignes. Il est vivement recommandé de l’utiliser si une ou plusieurs variables vont régulièrement servir à filtrer ou combiner des données. Pour aller plus loin, voir cette vignette.\n\n\n\n19.3.5 Réorganiser les données en data.table\n\nLe package data.table permet de réorganiser facilement une table de données avec les fonctions dcast() et melt(). La fonction melt() réorganise les données dans un format long. La fonction dcast() réorganise les données dans un format wide.\n\n\n\n\n\n\nmelt()\ndcast()\n\n\n\nRéorganiser les données dans un format long\n\nRéorganise les données dans un format wide\n\n\n\n\n\n\n\n\n\n19.3.5.1 melt : transformer des colonnes en lignes\nLa fonction melt() réorganise les donnée dans un format long. Elle prend les arguments suivants :\n\n\ndata : les données ;\n\nid.vars : les variables qui identifient les lignes de table d’arrivée ; elles restent inchangées lors de l’utilisation de melt() ;\n\nmeasure.vars : les variables qui sont transposées ;\n\nvariable.name : le nom de la nouvelle colonne qui contient le nom des variables transposées ;\n\nvalue.name : le nom de la nouvelle colonne qui contient la valeur des variables transposées.\n\nPour illustrer l’usage de cette fonction, nous allons utiliser les données du répertoire Filosofi 2016 agrégées au niveau des EPCI (table filosofi_epci_2016), et disponibles dans le package doremifasolData. On convertit cette table en data.table et on conserve uniquement certaines variables.\n\n# Charger la table de Filosofi\nfilosofi_epci_2016 &lt;- doremifasolData::filosofi_epci_2016\n# Convertir la table en data.table\nfilosofi_epci_2016_dt &lt;- as.data.table(filosofi_epci_2016)\n# Sélectionner des colonnes\nfilosofi_epci_2016_dt &lt;- \n  filosofi_epci_2016_dt[, .(CODGEO, TP6016, TP60AGE116, TP60AGE216, \n                            TP60AGE316, TP60AGE416, TP60AGE516, TP60AGE616)]\n\nNous allons restructurer cette table pour obtenir une nouvelle table, avec une observation par EPCI et par tranche d’âge. Voici le code qui permet d’obtenir cette table : on indique dans measure.vars le nom des colonnes qui seront transposées, le nom des colonnes transposées sera indiqué dans la nouvelle colonne “tranche_age” (variable.name = \"tranche_age\") et les valeurs des colonnes transposées seront stockées dans la colonne “taux_pauvrete” (value.name = \"taux_pauvrete\").\n\ndonnees_pauvrete_long &lt;- \n  melt(data = filosofi_epci_2016_dt, \n       id.vars = c(\"CODGEO\"), \n       measure.vars = c(\"TP6016\", \"TP60AGE116\", \"TP60AGE216\", \n                        \"TP60AGE316\", \"TP60AGE416\", \"TP60AGE516\", \"TP60AGE616\"),\n       variable.name = \"tranche_age\",\n       value.name    = \"taux_pauvrete\"\n  )\ndonnees_pauvrete_long\n\n         CODGEO tranche_age taux_pauvrete\n   1: 200000172      TP6016           8.8\n   2: 200000438      TP6016           8.0\n   3: 200000545      TP6016          23.7\n   4: 200000628      TP6016          20.1\n  ---                                    \n8705: 249740085  TP60AGE616          41.5\n8706: 249740093  TP60AGE616          43.4\n8707: 249740101  TP60AGE616          39.8\n8708: 249740119  TP60AGE616          31.7\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl est recommandé de travailler avec des données en format long plutôt qu’en format wide, notamment lorsque vous voulez faire des graphiques. En effet, le package de visualisation graphique ggplot2 est optimisé pour manipuler des données en format long (voir la fiche [Faire des graphiques avec ggplot2]). Ce conseil est particulièrement important si vous voulez représenter un graphique avec des groupes : il est préférable que les groupes soient empilés (format long) plutôt que juxtaposés (format wide), car le code est plus rapide et facile à écrire.\n\n\n\n19.3.5.2 dcast : transformer des lignes en colonnes\nLa fonction dcast() réorganise les donnée dans un format large. Elle prend les arguments suivants :\n\n\ndata : les données ;\n\nformula : une formule de la forme var_ligne ~ var_colonne qui définit la structure de la nouvelle table ;\n\ns’il y a plusieurs variables, la formule prend la forme var1 + var2 ~ var3 ;\n\ndcast() conserve une ligne par valeur de la partie gauche, et crée (au moins) une colonne par valeur de la partie droite ;\n\n\n\nfun.aggregate : une liste contenant la ou les fonction(s) utilisées pour agréger les données le cas échéant ; exemple : list(mean, sum, sd) ;\n\nvalue.var : un vecteur contenant le nom de la ou des colonne(s) dont les valeurs vont être transposées ; exemple : c(\"var1\", \"var2\").\n\nDans l’exemple qui suit, on réorganise la table bpe_ens_2018_dt de façon à obtenir une table qui contient une ligne par type d’équipement et une colonne par région (TYPEQU ~ REG). Ces colonnes vont contenir la somme (fun.aggregate = sum) du nombre d’équipements (value.var = \"NB_EQUIP\").\n\nbpe_ens_2018_wide &lt;- dcast(bpe_ens_2018_dt, \n                           TYPEQU ~ REG, \n                           value.var = \"NB_EQUIP\", \n                           fun.aggregate = sum)\nhead(bpe_ens_2018_wide)\n\n   TYPEQU 01 02 03 04 06  11  24  27  28  32  44  52  53  75  76  84  93 94\n1:   A101  2  2  1  7  0 191  28  23  54 127  80  15  20  66  55  69  34  3\n2:   A104 20 21 16 28  5  91 153 230 183 214 319 173 157 407 406 423 179 39\n3:   A105  1  1  1  1  1   2   2   2   2   2   4   1   1   5   3   4   1  1\n4:   A106  2  1  2  2  1  10   7  12  10  17  17   8   8  19  19  21  11  2\n5:   A107  2  1  1  4  1  60   9  19  15  26  30  11  12  28  26  34  23  2\n6:   A108  2  1  1  2  1  19   9  13  13  25  21   8  10  21  20  28  14  2\n\n\nIl est possible d’utiliser dcast() avec plusieurs variables à transposer et plusieurs fonctions pour transposer. Dans l’exemple qui suit, on obtient une ligne par type d’équipement, et une colonne par région et par fonction d’agrégation (mean et sum).\n\nbpe_ens_2018_wide2 &lt;- dcast(bpe_ens_2018_dt, \n                            TYPEQU ~ REG, \n                            value.var = \"NB_EQUIP\", \n                            fun.aggregate = list(sum, mean))\nbpe_ens_2018_wide2\n\n     TYPEQU NB_EQUIP_sum_01 NB_EQUIP_sum_02 NB_EQUIP_sum_03 NB_EQUIP_sum_04\n  1:   A101               2               2               1               7\n  2:   A104              20              21              16              28\n  3:   A105               1               1               1               1\n  4:   A106               2               1               2               2\n ---                                                                       \n183:   G101             105              79              48             136\n184:   G102              49              49              29             112\n185:   G103               0               0               0               0\n186:   G104             110             104              46              98\n     NB_EQUIP_sum_06 NB_EQUIP_sum_11 NB_EQUIP_sum_24 NB_EQUIP_sum_27\n  1:               0             191              28              23\n  2:               5              91             153             230\n  3:               1               2               2               2\n  4:               1              10               7              12\n ---                                                                \n183:              20            3351             213             256\n184:              11            2478             670             890\n185:               0              96             238             330\n186:               6            2993             293             339\n     NB_EQUIP_sum_28 NB_EQUIP_sum_32 NB_EQUIP_sum_44 NB_EQUIP_sum_52\n  1:              54             127              80              15\n  2:             183             214             319             173\n  3:               2               2               4               1\n  4:              10              17              17               8\n ---                                                                \n183:             272             495             593             376\n184:             845             698            1318             762\n185:             378             521             368             646\n186:             401             354             533             330\n     NB_EQUIP_sum_53 NB_EQUIP_sum_75 NB_EQUIP_sum_76 NB_EQUIP_sum_84\n  1:              20              66              55              69\n  2:             157             407             406             423\n  3:               1               5               3               4\n  4:               8              19              19              21\n ---                                                                \n183:             345             720             786            1166\n184:             943            1908            1982            2797\n185:             751            1408            1437            1265\n186:             374             926             932            1099\n     NB_EQUIP_sum_93 NB_EQUIP_sum_94 NB_EQUIP_mean_01 NB_EQUIP_mean_02\n  1:              34               3         1.000000         1.000000\n  2:             179              39         1.000000         1.000000\n  3:               1               1         1.000000         1.000000\n  4:              11               2         1.000000         1.000000\n ---                                                                  \n183:            1016             120         2.282609         1.975000\n184:            2111             438         2.450000         2.130435\n185:             718             187              NaN              NaN\n186:             876             182         1.718750         1.575758\n     NB_EQUIP_mean_03 NB_EQUIP_mean_04 NB_EQUIP_mean_06 NB_EQUIP_mean_11\n  1:         1.000000         1.000000              NaN         1.091429\n  2:         1.000000         1.000000         1.000000         1.000000\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.714286         1.837838         5.000000         2.080074\n184:         1.318182         2.036364         1.833333         2.250681\n185:              NaN              NaN              NaN         1.103448\n186:         1.533333         1.400000         1.500000         1.780488\n     NB_EQUIP_mean_24 NB_EQUIP_mean_27 NB_EQUIP_mean_28 NB_EQUIP_mean_32\n  1:         1.037037         1.000000         1.018868         1.058333\n  2:         1.000000         1.004367         1.000000         1.014218\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.601504         1.422222         1.511111         1.633663\n184:         1.763158         1.666667         1.978923         1.681928\n185:         1.048458         1.103679         1.330986         1.527859\n186:         1.140078         1.232727         1.297735         1.156863\n     NB_EQUIP_mean_44 NB_EQUIP_mean_52 NB_EQUIP_mean_53 NB_EQUIP_mean_75\n  1:         1.025641         1.000000         1.000000         1.157895\n  2:         1.009494         1.005814         1.000000         1.007426\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.694286         1.748837         1.674757         1.578947\n184:         1.734211         1.836145         2.063457         1.927273\n185:         1.153605         2.044304         1.891688         1.733990\n186:         1.230947         1.274131         1.307692         1.293296\n     NB_EQUIP_mean_76 NB_EQUIP_mean_84 NB_EQUIP_mean_93 NB_EQUIP_mean_94\n  1:         1.145833         1.029851         1.000000         1.000000\n  2:         1.015000         1.011962         1.028736         1.083333\n  3:         1.000000         1.000000         1.000000         1.000000\n  4:         1.000000         1.000000         1.000000         1.000000\n ---                                                                    \n183:         1.526214         1.707174         1.785589         2.000000\n184:         2.045408         2.109351         2.507126         3.369231\n185:         1.600223         1.408686         1.681499         2.101124\n186:         1.226316         1.345165         1.364486         1.857143\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nLa fonction dcast() crée une colonne par valeur des variables utilisées dans la partie droite de la formule. Il faut donc faire attention à ce que ces variables aient un nombre limité de valeurs, pour ne pas obtenir une table extrêmement large. On peut éventuellement discrétiser les variables continues, ou regrouper les modalités avant d’utiliser dcast().\n\nOn peut obtenir des noms de colonnes peu significatifs lorsqu’on utilise dcast() avec une fonction d’agrégation. Il est conseillé de modifier légèrement la partie droite de la formule pour obtenir des noms plus significatifs. Voici un exemple où on ajoute le préfixe resultat_region :\n\nbpe_ens_2018_wide2 &lt;- dcast(bpe_ens_2018_dt, \n                         TYPEQU ~ paste0(\"resultat_region\",REG), \n                         value.var = \"NB_EQUIP\", \n                         fun.aggregate = sum)\nhead(bpe_ens_2018_wide2)\n\n   TYPEQU resultat_region01 resultat_region02 resultat_region03\n1:   A101                 2                 2                 1\n2:   A104                20                21                16\n3:   A105                 1                 1                 1\n4:   A106                 2                 1                 2\n5:   A107                 2                 1                 1\n6:   A108                 2                 1                 1\n   resultat_region04 resultat_region06 resultat_region11 resultat_region24\n1:                 7                 0               191                28\n2:                28                 5                91               153\n3:                 1                 1                 2                 2\n4:                 2                 1                10                 7\n5:                 4                 1                60                 9\n6:                 2                 1                19                 9\n   resultat_region27 resultat_region28 resultat_region32 resultat_region44\n1:                23                54               127                80\n2:               230               183               214               319\n3:                 2                 2                 2                 4\n4:                12                10                17                17\n5:                19                15                26                30\n6:                13                13                25                21\n   resultat_region52 resultat_region53 resultat_region75 resultat_region76\n1:                15                20                66                55\n2:               173               157               407               406\n3:                 1                 1                 5                 3\n4:                 8                 8                19                19\n5:                11                12                28                26\n6:                 8                10                21                20\n   resultat_region84 resultat_region93 resultat_region94\n1:                69                34                 3\n2:               423               179                39\n3:                 4                 1                 1\n4:                21                11                 2\n5:                34                23                 2\n6:                28                14                 2\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl est conseillé de bien réfléchir avant de restructurer des données en format wide, et de ne le faire que lorsque cela paraît indispensable. En effet, s’il est tentant de restructurer les données sous format wide car ce format peut paraître plus intuitif, il est généralement plus simple et plus rigoureux de traiter les données en format long. Ceci dit, il existe des situations dans lesquelles il est indiqué de restructurer les données en format wide. Voici deux exemples :\n\nproduire un tableau synthétique de résultats, prêt à être diffusé, avec quelques colonnes donnant des indicateurs par catégorie (exemple : la table filosofi_epci_2016 du package doremifasolData) ;\nproduire une table avec une colonne par année, de façon à calculer facilement un taux d’évolution entre deux dates.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manipuler des données avec `data.table`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_datatable.html#assignation-reference",
    "href": "03_Fiches_thematiques/Fiche_datatable.html#assignation-reference",
    "title": "19  Manipuler des données avec data.table",
    "section": "\n19.4 La fonction d’assignation par référence (ou :=)",
    "text": "19.4 La fonction d’assignation par référence (ou :=)\nJusqu’à présent, nous avons manipulé un data.table existant, mais nous ne lui avons pas ajouté de nouvelles colonnes. Pour ce faire, nous allons utiliser la fonction := qui s’appelle “assignation par référence” et qui peut également s’appeler comme une fonction `:=`(), prenant ses arguments entre parenthèses. Voici comment on crée une nouvelle colonne dans bpe_ens_2018_dt :\n\nbpe_ens_2018_dt[ , nouvelle_colonne :=  NB_EQUIP * 10]\nhead(bpe_ens_2018_dt)\n\n   REG DEP DEPCOM DCIRIS   AN TYPEQU NB_EQUIP nouvelle_colonne\n1:  84  01  01001  01001 2018   A401        2               20\n2:  84  01  01001  01001 2018   A404        4               40\n3:  84  01  01001  01001 2018   A504        1               10\n4:  84  01  01001  01001 2018   A507        1               10\n5:  84  01  01001  01001 2018   B203        1               10\n6:  84  01  01001  01001 2018   C104        1               10\n\n\n\n19.4.1 La spécificité de data.table : la modification par référence\nA première vue, on peut penser que la fonction := est l’équivalent de la fonction dplyr::mutate() dans la grammaire data.table. C’est vrai dans la mesure où elle permet de faire des choses similaires, mais il faut garder en tête que son fonctionnement est complètement différent de celui de dplyr::mutate(). En effet, la grande spécificité de data.table par rapport à dplyr est que l’utilisation de la fonction := modifie directement la table de données, car data.table fonctionne sur le principe de la modification par référence (voir ce lien pour plus de détails). Cela signifie en pratique qu’il ne faut pas réassigner l’objet lorsqu’on modifie une de ses colonnes. C’est ce comportement qui permet à data.table d’être très rapide et très économe en mémoire vive, rendant son usage approprié pour des données volumineuses.\nPour créer une colonne, on écrit donc directement dt[ , nouvelle_colonne := une_formule] et non dt &lt;- dt[ , nouvelle_colonne := une_formule]. Voici un exemple qui compare dplyr et data.table :\n\n\n\n\n\n\n\n\n\n\nPackage\n\n\nCode\n\n\nCommentaire\n\n\n\n\n\ndplyr\n\n\n\n\nbpe_ens_2018 &lt;- \n  bpe_ens_2018 %&gt;%\n  dplyr::mutate(nouvelle_colonne =  NB_EQUIP * 10)\n\n\n\nIl faut utiliser une assignation (&lt;-) pour modifier la table.\n\n\n\n\n\ndata.table\n\n\n\n\nbpe_ens_2018_dt[ , nouvelle_colonne :=  NB_EQUIP * 10]\n\n\n\nIl ne faut pas d’assignation pour modifier la table, qui est modifiée par référence.\n\n\n\n\n19.4.2 Les usages de :=\n\nOn peut se servir de la fonction := de multiples façons, et avec plusieurs notations.\n\n19.4.2.1 Créer plusieurs variables à la fois\nVoici comment créer plusieurs variables à la fois avec :=, en utilisant une notation vectorielle :\n\nbpe_ens_2018_dt[ , c(\"nouvelle_colonne1\", \"nouvelle_colonne2\") :=  \n                   list(NB_EQUIP * 2, NB_EQUIP + 3)]\n\nOn peut faire exactement la même chose en utilisant la notation `:=`(). Voici le même exemple écrit avec `:=`().\n\nbpe_ens_2018_dt[ , `:=`(nouvelle_colonne1 = NB_EQUIP * 2,\n                        nouvelle_colonne2 = NB_EQUIP + 3)]\n\n\n\n\n\n\n\nNote\n\n\n\nSi vous utilisez la notation`:=`(), alors il faut utiliser uniquement = à l’intérieur des parenthèses pour créer ou modifier des variables, et non :=. Par exemple,\n\ndt[ , `:=`(var1 = \"Hello\", var2 = \"world\")]\n\n\n\n\n19.4.2.2 Supprimer une colonne\nOn peut facilement supprimer une colonne en lui assignant la valeur NULL (c’est hyper rapide !). Voici un exemple :\n\nbpe_ens_2018_dt[ , NB_EQUIP := NULL]\n\n\n19.4.2.3 Faire un remplacement conditionnel\nLa fonction := peut être utilisée pour modifier une colonne pour certaines lignes seulement, en fonction d’une condition logique. C’est beaucoup plus efficace qu’un terme dplyr::if_else() ou dplyr::case_when(). Imaginons qu’on veuille créer une colonne EQUIP_HORS_CHAUSS égale au nombre d’équipements (NB_EQUIP) sauf pour les lignes correspondantes à des magasins de chaussures (TYPEQU == \"B304\") où elle vaut NA. Dans ce cas, le code dplyr serait :\n\nbpe_ens_2018 %&gt;% \n  dplyr::mutate(NB_EQUIP_HORS_CHAUSS = dplyr::case_when(\n    TYPEQU == \"B304\" ~ NA_real_,\n    TRUE ~ NB_EQUIP)\n  )\n\nDeux alternatives existent en data.table nécessitant toutes deux beaucoup moins de mémoire vive :\n\nbpe_ens_2018_dt[ , NB_EQUIP_HORS_CHAUSS := NB_EQUIP\n                 ][TYPEQU == \"B304\", NB_EQUIP_HORS_CHAUSS := NA_real_]\n\nou bien en utilisant la fonction data.table::fcase dont le fonctionnement ressemble à celui de dplyr::case_when :\n\nbpe_ens_2018_dt[ , NB_EQUIP_HORS_CHAUSS := data.table::fcase(TYPEQU == \"B304\", NA_real_,\n                                                   TYPEQU != \"B304\", NB_EQUIP)\n                                                   ]\n\n\n19.4.3 Attention en utilisant :=\n\nL’utilisation de la fonction := est déroutante lorsqu’on découvre data.table. Voici trois remarques qui vous feront gagner du temps :\n\n\nVous pouvez faire appel à d’autres fonctions à l’intérieur de la fonction :=. Par exemple, si on veut mettre la variable name en minuscules, on peut utiliser la fonction tolower(). On écrit alors :\n\ndt[ , name_minuscule := tolower(name)]\n\n\n\nLorsque l’on crée plusieurs variables avec la fonction :=, elles sont créées en même temps. On ne peut donc pas faire appel dans une formule à une variable qu’on crée dans le même appel à la fonction :=. Par exemple, le code suivant ne fonctionne pas :\n\nbpe_ens_2018_dt[ , `:=`(nouvelle_colonne1 =  NB_EQUIP * 2,\n                        nouvelle_colonne2 =  nouvelle_colonne1 + 3)]\n\nEn effet, au moment où la fonction := est exécutée, la colonne nouvelle_colonne1 n’existe pas encore, donc la formule nouvelle_colonne2 = nouvelle_colonne1 + 3 n’a pas encore de sens. Si vous créez des variables en chaîne, il faut décomposer l’opération en plusieurs étapes enchaînées. Voici le code qui permet de créer les deux colonnes à la suite :\n\nbpe_ens_2018_dt[ , nouvelle_colonne1 :=  NB_EQUIP * 2\n                ][ , nouvelle_colonne2 :=  nouvelle_colonne1 + 3]\n\n\n\nUn mauvais usage de la fonction := peut vous amener à écraser par erreur vos données. En effet, si vous exécuter par erreur la commande dt[ , ma_variable_importante := 0], vous écrasez la variable ma_variable_importante. Vous devez alors recharger vos données… Il faut donc bien réfléchir à ce que vous voulez faire avant de remplacer ou modifier une variable existante avec la fonction :=. Si vous modifiez un data.table dans une fonction, un filet de sécurité consiste à d’abord copier le data.table initial et ainsi faire les modifications sur le nouvel objet, de la manière suivante :\n\n  dt_copy &lt;- data.table::copy(dt)\n  dt_copy[, ma_variable_importante := \"Nouvelle valeur\"]\n\n\n\n19.5 Programmer des fonctions avec data.table\n\nUne des forces de data.table est qu’il est relativement simple d’utiliser ce package dans des fonctions. Pour illustrer l’usage des fonctions, nous allons utiliser la table filosofi_com_2016 disponible dans le package doremifasolData. Cette table donne des informations sur les revenus des ménages au niveau communal. Nous créons une variable donnant le numéro du département (departement) en extrayant les deux premiers caractères du code commune (CODGEO) avec la fonction str_sub du package stringr (vous pouvez consulter la fiche [Manipuler des données textuelles pour en apprendre davantage sur stringr]). Enfin, nous utilisons la fonction .SD pour sélectionner uniquement quelques variables.\n\n# Charger la table de données et la transformer en data.table\nfilosofi_com_2016_dt &lt;- as.data.table(doremifasolData::filosofi_com_2016)\n# Créer une variable donnant le département\nfilosofi_com_2016_dt[, departement := stringr::str_sub(CODGEO, start = 1L, end = 2L)]\n# Supprimer les départements d'outre-mer\nfilosofi_com_2016_dt &lt;- filosofi_com_2016_dt[departement  != \"97\"]\n# Alléger la table en ne conservant que quelques variables\nfilosofi_com_2016_dt &lt;- \n  filosofi_com_2016_dt[, .SD, .SDcols = c(\"departement\", \"CODGEO\", \"NBMENFISC16\",\n                                          \"NBPERSMENFISC16\")]\n\n\n19.5.1 Utiliser .SD et lapply\n\nLe mot clé .SD (Subset of Data) permet d’appliquer la même opération sur plusieurs colonnes. Les colonnes auxquelles l’opération s’applique sont contrôlées par l’argument .SDcols (par défaut, toutes les colonnes sont traitées). Le mot clé .SD est régulièrement utilisé en conjonction avec la fonction lapply. Cette syntaxe, très puissante, permet également d’avoir des codes assez compacts, ce qui les rend plus lisible.\nUn usage classique de ce duo lapply+.SD consiste à écrire des fonctions de statistiques descriptives. Par exemple, imaginons qu’on souhaite calculer la moyenne, l’écart-type et les quantiles (P25, P50 et P75) de nombreuses colonnes. On peut alors définir la fonction suivante :\n\nmes_statistiques &lt;- \n  function(x) return(c(mean(x, na.rm = TRUE), \n                       sd(x, na.rm = TRUE), \n                       quantile(x, probs = c(.25,.5,.75), na.rm = TRUE)))\n\nVoici comment on peut appliquer cette fonction aux colonnes NBMENFISC16 (nombre de ménages fiscaux) et NBPERSMENFISC16 (nombre de personnes dans les ménages fiscaux) de la table filosofi_com_2016_dt :\n\ndata_agregee &lt;- \n  filosofi_com_2016_dt[ ,\n                        lapply(.SD, mes_statistiques), \n                        .SDcols = c(\"NBMENFISC16\", \"NBPERSMENFISC16\")]\ndata_agregee[, 'stat' := c(\"moyenne\",\"écart-type\",\"P25\",\"P50\",\"P75\")]\ndata_agregee\n\n   NBMENFISC16 NBPERSMENFISC16       stat\n1:    916.3269         2097.18    moyenne\n2:   7382.4628        15245.60 écart-type\n3:    105.0000          250.50        P25\n4:    218.0000          527.00        P50\n5:    526.0000         1278.25        P75\n\n\nIl est également très simple d’effectuer des calculs par groupe avec la méthode lapply+.SD. On peut par facilement adapter le code précédent pour calculer des statistiques descriptives par département (variable departement).\n\ndata_agregee &lt;- \n  filosofi_com_2016_dt[ ,\n                        lapply(.SD, mes_statistiques), \n                        by = departement,\n                        .SDcols = c(\"NBMENFISC16\", \"NBPERSMENFISC16\")]\ndata_agregee[, 'stat' := c(\"moyenne\",\"écart-type\",\"P25\",\"P50\",\"P75\"), by = departement]\ndata_agregee\n\n\n19.5.2 Définir des fonctions modifiant un data.table\n\nIl est très facile d’écrire avec data.table des fonctions génériques faisant appel à des noms de variables en arguments. Pour déclarer à data.table qu’un nom fait référence à une colonne, la manière la plus simple est d’utiliser la fonction get. Dans l’exemple suivant, on définit la fonction creation_var qui crée dans la table data une nouvelle variable (dont le nom est l’argument nouveau_nom) égale à une autre variable incrémentée (dont le nom est l’argument nom_variable) de 1. L’utilisation de la fonction get permet d’indiquer à data.table que la chaîne de caractères nom_variable désigne une colonne de la table data.\n\ncreation_var &lt;- function(data, nom_variable, nouveau_nom){\n  data[, c(nouveau_nom) := get(nom_variable) + 1]\n}\nhead(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\"), 2)\n\n   departement CODGEO NBMENFISC16 NBPERSMENFISC16 nouvelle_variable\n1:          01  01001         313           795.5               314\n2:          01  01002         101           248.0               102\n\n\nc(nouveau_nom) permet de s’assurer que data.table crée une nouvelle colonne dont le nom est défini en argument (et qui ne s’appelle donc pas nouveau_nom).\n\n\n\n\n\n\nNote\n\n\n\nLa version 1.14.1 de data.table (encore en développement) apporte une syntaxe améliorée dans ce cas et considère l’utilisation de get comme désuète. Le [...] admet un nouvel argument env à qui on donne tous les remplacements que l’on souhaite. L’exemple devient :\ncreation_var &lt;- function(data, nom_variable, nouveau_nom){\n  data[, nouveau_nom := nom_variable + 1, \n    env = list(nouveau_nom = nouveau_nom, nom_variable = nom_variable)]\n}\nhead(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\"), 2)\nL’avantage c’est qu’on peut effectuer de tels remplacements dans i (dimension ligne) et qu’on peut même remplacer des fonctions :\ncreation_var &lt;- function(data, nom_variable, nouveau_nom, fonction){\n  data[, nouveau_nom := fonction(nom_variable), \n    env = list(nouveau_nom = nouveau_nom, nom_variable = nom_variable, fonction= fonction)]\n  head(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\",\n                  fonction = \"sqrt\"), 2)\n}\n\n\n\n\n\n\n\n\nTip\n\n\n\nLorsqu’on définit des fonctions pour effectuer des traitements génériques, une précaution est nécessaire pour ne pas modifier les données en entrée de la fonction si l’opérateur := est utilisée. Il est recommandé dans ce cas de créer une copie du dataframe en entrée (data.table::copy(df)) et d’effectuer les traitements sur cette copie.\n\n\n\n19.6 Pour en savoir plus\n\nla documentation officielle de data.table (en anglais) ;\nles vignettes de data.table :\n\n\nIntroduction à data.table (en anglais) ;\n\nModification par référence (en anglais) ;\n\nla foire aux questions de data.table (en anglais) ;\n\n\nune cheatsheet sur data.table (en anglais) ;\nune introduction à l’utilisation de l’opérateur [...] (en français) ;\nCe cours complet sur data.table (en français).\nCette présentation des fonctionnalités du package (en français) ;\nCe post sur les fonctions utilisant data.table.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manipuler des données avec `data.table`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_datatable.html#programmer-des-fonctions-avec-data.table",
    "href": "03_Fiches_thematiques/Fiche_datatable.html#programmer-des-fonctions-avec-data.table",
    "title": "19  Manipuler des données avec data.table",
    "section": "\n19.5 Programmer des fonctions avec data.table\n",
    "text": "19.5 Programmer des fonctions avec data.table\n\nUne des forces de data.table est qu’il est relativement simple d’utiliser ce package dans des fonctions. Pour illustrer l’usage des fonctions, nous allons utiliser la table filosofi_com_2016 disponible dans le package doremifasolData. Cette table donne des informations sur les revenus des ménages au niveau communal. Nous créons une variable donnant le numéro du département (departement) en extrayant les deux premiers caractères du code commune (CODGEO) avec la fonction str_sub du package stringr (vous pouvez consulter la fiche [Manipuler des données textuelles pour en apprendre davantage sur stringr]). Enfin, nous utilisons la fonction .SD pour sélectionner uniquement quelques variables.\n\n# Charger la table de données et la transformer en data.table\nfilosofi_com_2016_dt &lt;- as.data.table(doremifasolData::filosofi_com_2016)\n# Créer une variable donnant le département\nfilosofi_com_2016_dt[, departement := stringr::str_sub(CODGEO, start = 1L, end = 2L)]\n# Supprimer les départements d'outre-mer\nfilosofi_com_2016_dt &lt;- filosofi_com_2016_dt[departement  != \"97\"]\n# Alléger la table en ne conservant que quelques variables\nfilosofi_com_2016_dt &lt;- \n  filosofi_com_2016_dt[, .SD, .SDcols = c(\"departement\", \"CODGEO\", \"NBMENFISC16\",\n                                          \"NBPERSMENFISC16\")]\n\n\n19.5.1 Utiliser .SD et lapply\n\nLe mot clé .SD (Subset of Data) permet d’appliquer la même opération sur plusieurs colonnes. Les colonnes auxquelles l’opération s’applique sont contrôlées par l’argument .SDcols (par défaut, toutes les colonnes sont traitées). Le mot clé .SD est régulièrement utilisé en conjonction avec la fonction lapply. Cette syntaxe, très puissante, permet également d’avoir des codes assez compacts, ce qui les rend plus lisible.\nUn usage classique de ce duo lapply+.SD consiste à écrire des fonctions de statistiques descriptives. Par exemple, imaginons qu’on souhaite calculer la moyenne, l’écart-type et les quantiles (P25, P50 et P75) de nombreuses colonnes. On peut alors définir la fonction suivante :\n\nmes_statistiques &lt;- \n  function(x) return(c(mean(x, na.rm = TRUE), \n                       sd(x, na.rm = TRUE), \n                       quantile(x, probs = c(.25,.5,.75), na.rm = TRUE)))\n\nVoici comment on peut appliquer cette fonction aux colonnes NBMENFISC16 (nombre de ménages fiscaux) et NBPERSMENFISC16 (nombre de personnes dans les ménages fiscaux) de la table filosofi_com_2016_dt :\n\ndata_agregee &lt;- \n  filosofi_com_2016_dt[ ,\n                        lapply(.SD, mes_statistiques), \n                        .SDcols = c(\"NBMENFISC16\", \"NBPERSMENFISC16\")]\ndata_agregee[, 'stat' := c(\"moyenne\",\"écart-type\",\"P25\",\"P50\",\"P75\")]\ndata_agregee\n\n   NBMENFISC16 NBPERSMENFISC16       stat\n1:    916.3269         2097.18    moyenne\n2:   7382.4628        15245.60 écart-type\n3:    105.0000          250.50        P25\n4:    218.0000          527.00        P50\n5:    526.0000         1278.25        P75\n\n\nIl est également très simple d’effectuer des calculs par groupe avec la méthode lapply+.SD. On peut par facilement adapter le code précédent pour calculer des statistiques descriptives par département (variable departement).\n\ndata_agregee &lt;- \n  filosofi_com_2016_dt[ ,\n                        lapply(.SD, mes_statistiques), \n                        by = departement,\n                        .SDcols = c(\"NBMENFISC16\", \"NBPERSMENFISC16\")]\ndata_agregee[, 'stat' := c(\"moyenne\",\"écart-type\",\"P25\",\"P50\",\"P75\"), by = departement]\ndata_agregee\n\n\n19.5.2 Définir des fonctions modifiant un data.table\n\nIl est très facile d’écrire avec data.table des fonctions génériques faisant appel à des noms de variables en arguments. Pour déclarer à data.table qu’un nom fait référence à une colonne, la manière la plus simple est d’utiliser la fonction get. Dans l’exemple suivant, on définit la fonction creation_var qui crée dans la table data une nouvelle variable (dont le nom est l’argument nouveau_nom) égale à une autre variable incrémentée (dont le nom est l’argument nom_variable) de 1. L’utilisation de la fonction get permet d’indiquer à data.table que la chaîne de caractères nom_variable désigne une colonne de la table data.\n\ncreation_var &lt;- function(data, nom_variable, nouveau_nom){\n  data[, c(nouveau_nom) := get(nom_variable) + 1]\n}\nhead(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\"), 2)\n\n   departement CODGEO NBMENFISC16 NBPERSMENFISC16 nouvelle_variable\n1:          01  01001         313           795.5               314\n2:          01  01002         101           248.0               102\n\n\nc(nouveau_nom) permet de s’assurer que data.table crée une nouvelle colonne dont le nom est défini en argument (et qui ne s’appelle donc pas nouveau_nom).\n\n\n\n\n\n\nNote\n\n\n\nLa version 1.14.1 de data.table (encore en développement) apporte une syntaxe améliorée dans ce cas et considère l’utilisation de get comme désuète. Le [...] admet un nouvel argument env à qui on donne tous les remplacements que l’on souhaite. L’exemple devient :\ncreation_var &lt;- function(data, nom_variable, nouveau_nom){\n  data[, nouveau_nom := nom_variable + 1, \n    env = list(nouveau_nom = nouveau_nom, nom_variable = nom_variable)]\n}\nhead(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\"), 2)\nL’avantage c’est qu’on peut effectuer de tels remplacements dans i (dimension ligne) et qu’on peut même remplacer des fonctions :\ncreation_var &lt;- function(data, nom_variable, nouveau_nom, fonction){\n  data[, nouveau_nom := fonction(nom_variable), \n    env = list(nouveau_nom = nouveau_nom, nom_variable = nom_variable, fonction= fonction)]\n  head(creation_var(filosofi_com_2016_dt, \n                  nom_variable = \"NBMENFISC16\",\n                  nouveau_nom  = \"nouvelle_variable\",\n                  fonction = \"sqrt\"), 2)\n}\n\n\n\n\n\n\n\n\nTip\n\n\n\nLorsqu’on définit des fonctions pour effectuer des traitements génériques, une précaution est nécessaire pour ne pas modifier les données en entrée de la fonction si l’opérateur := est utilisée. Il est recommandé dans ce cas de créer une copie du dataframe en entrée (data.table::copy(df)) et d’effectuer les traitements sur cette copie.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manipuler des données avec `data.table`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_datatable.html#Ressourcesdatatable",
    "href": "03_Fiches_thematiques/Fiche_datatable.html#Ressourcesdatatable",
    "title": "19  Manipuler des données avec data.table",
    "section": "\n19.6 Pour en savoir plus",
    "text": "19.6 Pour en savoir plus\n\nla documentation officielle de data.table (en anglais) ;\nles vignettes de data.table :\n\n\nIntroduction à data.table (en anglais) ;\n\nModification par référence (en anglais) ;\n\nla foire aux questions de data.table (en anglais) ;\n\n\nune cheatsheet sur data.table (en anglais) ;\nune introduction à l’utilisation de l’opérateur [...] (en français) ;\nCe cours complet sur data.table (en français).\nCette présentation des fonctionnalités du package (en français) ;\nCe post sur les fonctions utilisant data.table.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Manipuler des données avec `data.table`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html",
    "href": "03_Fiches_thematiques/Fiche_arrow.html",
    "title": "20  Manipuler des données avec arrow",
    "section": "",
    "text": "20.1 Tâches concernées et recommandations\nL’utilisateur souhaite manipuler des données structurées sous forme de data.frame par le biais de l’écosystème Arrow (sélectionner des variables, sélectionner des observations, créer des variables, joindre des tables).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_arrow.html#tâches-concernées-et-recommandations",
    "title": "20  Manipuler des données avec arrow",
    "section": "",
    "text": "Tâches concernées et recommandations\n\n\n\n\nPour des tables de données de taille petite et moyenne (inférieure à 1 Go ou moins d’un million d’observations), il est recommandé d’utiliser les packages tibble, dplyr et tidyr qui sont présentés dans la fiche Manipuler des données avec le tidyverse;\nPour des tables de données de grande taille (plus de 1 Go en CSV, plus de 200 Mo en Parquet, ou plus d’un million d’observations), il est recommandé d’utiliser soit les packages arrow (qui fait l’objet de la présente fiche) et #duckdb (voir la fiche Manipuler des données avec duckdb), soit le package data.table qui fait l’objet de la fiche Manipuler des données avec data.table.\nIl est essentiel de travailler avec la dernière version d’arrow, de duckdb et de R car les packages arrow et duckdb sont en cours de développement. Par ailleurs, les recommandations d’utilitR peuvent évoluer en fonction du développement de ces packages.\nSi les données traitées sont très volumineuses (plus de 5 Go en CSV, plus de 1 Go en Parquet ou plus de 5 millions d’observations), il est essentiel de manipuler uniquement des objets Arrow Table, plutôt que des tibbles. Cela implique notamment d’utiliser la fonction compute() plutôt que collect() dans les traitements intermédiaires.\nPour les personnes qui découvrent arrow, il est recommandé de partir de l’exemple de script de la Section 20.6 pour se familiariser avec l’usage d'arrow.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nApprendre à utiliser arrow n’est pas difficile, car la syntaxe utilisée est quasiment identique à celle du tidyverse. Toutefois, une bonne compréhension du fonctionnement de R et de arrow est nécessaire pour bien utiliser arrow sur des données volumineuses. Voici quelques conseils pour bien démarrer:\n\nIl est indispensable de lire les fiches Importer des fichiers Parquet et Manipuler des données avec le tidyverse avant de lire la présente fiche.\nIl est complètement normal de rencontrer des erreurs difficiles à comprendre lorsqu’on commence à utiliser arrow, il ne faut donc pas se décourager.\nIl ne faut pas hésiter à demander de l’aide à des collègues, ou à poser des questions sur les salons Tchap adaptés (le salon Langage R par exemple).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html#présentation-du-package-arrow-et-du-projet-associé",
    "href": "03_Fiches_thematiques/Fiche_arrow.html#présentation-du-package-arrow-et-du-projet-associé",
    "title": "20  Manipuler des données avec arrow",
    "section": "\n20.2 Présentation du package arrow et du projet associé",
    "text": "20.2 Présentation du package arrow et du projet associé\n\n20.2.1 Qu’est-ce qu’arrow?\nApache Arrow est un projet open-source qui propose deux choses:\n\nune représentation standardisée des données tabulaires en mémoire vive appelée Apache Arrow Columnar Format, qui est à la fois efficace (les traitements sont rapides), interopérable (différents langages de programmation peuvent accéder aux mêmes données, sans conversion des données dans un autre format) et indépendante du langage de programmation utilisé.\nune implémentation de ce standard en C++, qui prend la forme d’une librairie C++ nommée libarrow. Il existe d’autres implémentations dans d’autres langages; l’implémentation en Rust est par exemple utilisée par le projet Polars, une librairie alternative à dplyr (R) ou Pandas (Python) pour le traitement de données.\n\nUn point important à retenir est donc que arrow n’est pas un outil spécifique à R, et il faut bien distinguer le projet arrow (et la librairie C++ libarrow) du package R arrow. Ce package propose simplement une interface qui permet d’utiliser la librairie libarrow avec R, et il existe d’autres interfaces pour se servir de libarrow avec d’autres langages: en Python, en Java, en Javascript, en Julia, etc.\n\n20.2.2 Spécificités de arrow\n\nLe projet arrow présente cinq spécificités:\n\n\nReprésentation des données en mémoire : arrow organise les données en colonnes plutôt qu’en lignes (on parle de columnar format). Concrètement, cela veut dire que dans la RAM toutes les valeurs de la première colonne sont stockées de façon contiguë, puis les valeurs de la deuxième colonne, etc. Cette structuration des données rend les traitements très efficaces: si l’on veut par exemple calculer la moyenne d’une variable, il est possible d’accéder directement au bloc de mémoire vive qui contient l’intégralité de cette colonne (indépendamment des autres colonnes de la table de données), d’où un traitement très rapide.\n\n\nIllustration de ce principe\n\n\nIllustration de ce principe du stockage orienté colonnes (droite) par rapport au csv\n\n\nUtilisation avec Parquet: arrow est souvent utilisé pour manipuler des données stockées en format Parquet. Parquet est un format de stockage orienté colonne conçu pour être très rapide en lecture (voir la fiche Importer des fichiers Parquet pour plus de détails). arrow est optimisé pour travailler sur des fichiers Parquet, notamment lorsqu’ils contiennent des données très volumineuses.\nTraitement de données volumineuses: arrow est conçu pour traiter des données sans avoir besoin de les charger complètement dans la mémoire vive. Cela signifie qu’arrow est capable de traiter des données plus volumineuses que la mémoire vive (RAM) dont on dispose. C’est un avantage majeur en comparaison aux autres approches possibles en R (data.table et dplyr par exemple).\nInteropérabilité: arrow est conçu pour être interopérable entre plusieurs langages de programmation tels que R, Python, Java, C++, etc. Cela signifie que les données peuvent être échangées entre ces langages sans avoir besoin de convertir les données, d’où des gains importants de temps et de performance.\nLazy Evaluation: arrow prend en charge la lazy evaluation (évaluation différée) dans certains contextes. Cela signifie que les traitements ne sont effectivement exécutés que lorsqu’ils sont nécessaires, ce qui peut améliorer les performances en évitant le calcul de résultats intermédiaires non utilisés. La Section 21.5.2 présente en détail cette notion.\n\n20.2.3 A quoi sert le package arrow?\nDu point de vue d’un statisticien utilisant R, le package arrow permet de faire trois choses:\n\nImporter des données (exemples: fichiers CSV, fichiers Parquet, stockage S3) et les organiser en mémoire vive dans un objet Arrow Table;\nManipuler des données organisées dans un Arrow Table avec la syntaxe dplyr, ou avec le langage SQL (grâce à duckdb);\nÉcrire des données en format Parquet.\n\n20.2.4 Quels sont les avantages d’arrow?\nEn pratique, le package arrow présente trois avantages:\n\n\nPerformances élevées: arrow est très efficace et très rapide pour la manipulation de données tabulaires (nettement plus performant que dplyr par exemple);\n\nUsage réduit des ressources: arrow est conçu pour ne charger en mémoire que le minimum de données. Cela permet de réduire considérablement les besoins en mémoire, même lorsque les données sont volumineuses;\n\nFacilité d’apprentissage grâce aux approches dplyr et SQL: arrow peut être utilisé avec les verbes de dplyr (select, mutate, etc.) et/ou avec le langage SQL grâce à duckdb. Par conséquent, il n’est pas nécessaire d’apprendre une nouvelle syntaxe pour utiliser arrow, on peut s’appuyer sur la ou les approches que l’on maîtrise déjà. En revanche, il est à noter que le package data.table n’est pas directement compatible avec arrow (il faut convertir les objets Arrow Table en data.table, opération longue lorsque les données sont volumineuses).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html#que-faut-il-savoir-pour-utiliser-arrow",
    "href": "03_Fiches_thematiques/Fiche_arrow.html#que-faut-il-savoir-pour-utiliser-arrow",
    "title": "20  Manipuler des données avec arrow",
    "section": "\n20.3 Que faut-il savoir pour utiliser arrow?",
    "text": "20.3 Que faut-il savoir pour utiliser arrow?\nLe package arrow présente quatre caractéristiques importantes:\n\nune structure de données spécifique: le Arrow Table;\nune utilisation via la syntaxe dplyr;\nun moteur d’exécution spécifique: acero;\nun mode de fonctionnement particulier: l’évaluation différée.\n\n\n20.3.1 Charger et paramétrer le package arrow\n\nPour utiliser arrow, il faut commencer par charger le package. Comme arrow s’utilise presque toujours avec dplyr en pratique, il est préférable de prendre l’habitude de charger les deux packages ensemble. Par ailleurs, il est utile de définir systématiquement deux réglages qui sont importants pour les performances d’arrow: autoriser arrow à utiliser plusieurs processeurs en parallèle, et définir le nombre de processeurs qu’arrow peut utiliser.\n\nlibrary(arrow)\nlibrary(dplyr)\n\n# Autoriser arrow à utiliser plusieurs processeurs en parallèle\noptions(arrow.use_threads = TRUE)\n# Définir le nombre de processeurs qu'arrow peut utiliser\narrow::set_cpu_count(parallel::detectCores() %/% 4)\n\n\n20.3.2 Le data.frame version arrow: le Arrow Table\n\nLe package arrow structure les données non pas dans un data.frame classique, mais dans un objet spécifique à arrow: le Arrow Table. Dans un objet Arrow Table, les données sont organisées en colonnes plutôt qu’en lignes, conformément aux spécifications d’arrow (voir la présentation d’arrow). Pour convertir un data.frame ou un tibble en Arrow Table, il suffit d’utiliser la fonction as_arrow_table().\nPar rapport à un data.frame standard ou à un tibble, le Arrow Table se distingue immédiatement sur trois points. Pour illustrer ces différences, on utilise la base permanente des équipements 2018 (table bpe_ens_2018), transformée en tibble d’une part, et en Arrow Table d’autre part.\n\n# Charger les données et les convertir en tibble\nbpe_ens_2018_tbl   &lt;- doremifasolData::bpe_ens_2018 |&gt; as_tibble()\n\n# Charger les données et les convertir en Arrow Table\nbpe_ens_2018_arrow &lt;- doremifasolData::bpe_ens_2018 |&gt; as_arrow_table()\n\nPremière différence: alors que les data.frames et les tibbles apparaissent dans la rubrique Data de l’environnement RStudio (cadre rouge dans la capture d’écran), les objets Arrow Table apparaissent dans la rubrique Values (cadre blanc).\n\nDeuxième différence: alors que l’affichage dans la console d’un data.frame ou tibble permet de visualiser les premières lignes, la même opération sur un Arrow Table affiche uniquement des métadonnées (nombre de lignes et de colonnes, nom et type des colonnes).\n\n# Affichage d'un tibble\nbpe_ens_2018_tbl\n\n# A tibble: 1,035,564 × 7\n   REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 84    01    01001  01001   2018 A401          2\n 2 84    01    01001  01001   2018 A404          4\n 3 84    01    01001  01001   2018 A504          1\n 4 84    01    01001  01001   2018 A507          1\n 5 84    01    01001  01001   2018 B203          1\n 6 84    01    01001  01001   2018 C104          1\n 7 84    01    01001  01001   2018 D233          1\n 8 84    01    01001  01001   2018 F102          1\n 9 84    01    01001  01001   2018 F111          1\n10 84    01    01001  01001   2018 F113          1\n# ℹ 1,035,554 more rows\n\n\n\n# Affichage d'un Arrow Table\nbpe_ens_2018_arrow\n\nTable\n1035564 rows x 7 columns\n$REG &lt;string&gt;\n$DEP &lt;string&gt;\n$DEPCOM &lt;string&gt;\n$DCIRIS &lt;string&gt;\n$AN &lt;double&gt;\n$TYPEQU &lt;string&gt;\n$NB_EQUIP &lt;double&gt;\n\nSee $metadata for additional Schema metadata\n\n\nTroisième différence: alors qu’il est possible d’afficher le contenu d’un data.frame ou d’un tibble en cliquant sur son nom dans la rubrique Data de l’environnement ou en utilisant la fonction View(), il n’est pas possible d’afficher directement le contenu d’un Arrow Table. Pour afficher le contenu d’un Arrow Table, il faut d’abord convertir le Arrow Table en tibble avec la fonction collect().\n\n\n\n\n\n\nTip\n\n\n\nIl arrive fréquemment que l’on souhaite jeter un coup d’oeil au contenu d’un Arrow Table. Toutefois, convertir directement un Arrow Table très volumineux en tibble peut poser de sérieux problèmes: temps de conversion, consommation importante de RAM, voire plantage de R si le Arrow Table est vraiment très gros. Il est donc fortement conseillé de prendre un petit extrait du Arrow Table concerné et de convertir uniquement cet extrait en tibble.\n\nExemple de code qui visualise un échantillon d’une table Arrow# Extraire les 1000 premières lignes du Arrow Table et les convertir en tibble\nextrait_bpe &lt;- bpe_ens_2018_arrow |&gt; slice_head(n = 1000) |&gt; collect()\nView(extrait_bpe)\n\n\n\n\n\n20.3.3 Manipuler des Arrow Table avec la syntaxe dplyr\n\nLe package R arrow a été écrit de façon à ce qu’un Arrow Table puisse être manipulé avec les fonctions de dplyr (select, filter, mutate, left_join, etc.), comme si cette table était un data.frame ou un tibble standard.\nIl est également possible d’utiliser sur un Arrow Table un certain nombre de fonctions des packages du tidyverse (comme stringr et lubridate). Cela s’avère très commode en pratique, car lorsqu’on sait utiliser dplyr et le tidyverse, on peut commencer à utiliser arrow sans avoir à apprendre une nouvelle syntaxe de manipulation de données. Il y a néanmoins des subtilités à connaître, détaillées dans la suite de cette fiche.\nDans l’exemple suivant, on calcule le nombre d’équipements par région, à partir d’un tibble et à partir d’un Arrow table. La seule différence apparente entre les deux traitement est la présence de la fonction collect() à la fin des instructions; cette fonction indique que l’on souhaite que le résultat du traitement soit stocké sous la forme d’un tibble. La raison d’être de ce collect() est expliquée plus loin, dans le paragraphe sur l’évaluation différée.\n\n\nManipulation d’un tibble\n\nbpe_ens_2018_tbl |&gt;\n  group_by(REG) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  )\n\n\n\n\nManipulation d’un Arrow Table\n\nbpe_ens_2018_arrow |&gt;\n  group_by(REG) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) |&gt;\n  collect()\n\n\n\n\n20.3.4 Le moteur d’exécution d’arrow: acero\n\nIl y a une différence fondamentale entre manipuler un data.frame ou un tibble et manipuler un Arrow Table. Pour bien la comprendre, il faut d’abord comprendre la distinction entre syntaxe de manipulation des données et moteur d’exécution:\n\nLa syntaxe de manipulation des données sert à décrire les manipulations de données qu’on veut faire (calculer des moyennes, faire des jointures…), indépendamment de la façon dont ces calculs sont effectivement réalisés;\nle moteur d’exécution fait référence à la façon dont les opérations sur les données sont effectivement réalisées en mémoire, indépendamment de la façon dont elles ont été décrites par l’utilisateur.\n\nLa grande différence entre manipuler un tibble et manipuler un Arrow Table tient au moteur d’exécution: si on manipule un tibble avec la syntaxe de dplyr, alors c’est le moteur d’exécution de dplyr qui fait les calculs; si on manipule un Arrow Table avec la syntaxe de dplyr, alors c’est le moteur d’exécution d’arrow (nommé acero) qui fait les calculs. C’est justement parce que le moteur d’exécution d’arrow est beaucoup plus efficace que celui de dplyr qu’arrow est beaucoup plus rapide.\n\nCette différence de moteurs d’exécution a une conséquence technique importante: une fois que l’utilisateur a défini des instructions avec la syntaxe dplyr, il est nécessaire que celles-ci soient converties pour que le moteur acero (écrit en C++ et non en R) puisse les exécuter. De façon générale, arrow fait cette conversion de façon automatique et invisible, car le package arrow contient la traduction C++ de plusieurs centaines de fonctions du tidyverse. Par exemple, le package arrow contient la traduction C++ de la fonction filter() de dplyr, ce qui fait que les instructions filter() écrites en syntaxe tidyverse sont converties de façon automatique et invisible en des instructions C++ équivalentes. La liste des fonctions du tidyverse supportées par acero est disponible sur cette page. Il arrive toutefois qu’on veuille utiliser une fonction non supportée par acero. Cette situation est décrite dans le paragraphe “Comment utiliser une fonction non supportée par acero”.\n\n20.3.5 L’évaluation différée avec arrow (lazy evaluation)\nUne caractéristique importante d’arrow est qu’il pratique l’évaluation différée (lazy evaluation): les calculs ne sont effectivement réalisés que lorsqu’ils sont nécessaires. En pratique, cela signifie qu’arrow se contente de mémoriser les instructions, sans faire aucun calcul tant que l’utilisateur ne le demande pas explicitement. Il existe deux fonctions pour déclencher l’évaluation d’un traitement arrow: collect() et compute(). Il n’y a qu’une seule différence entre collect() et compute(), mais elle est importante: collect() renvoie le résultat du traitement sous la forme d’un tibble, tandis que compute() le renvoie sous la forme d’un Arrow Table.\nL’évaluation différée permet d’améliorer les performances en évitant le calcul de résultats intermédiaires inutiles, et en optimisant les requêtes pour utiliser le minimum de données et le minimum de ressources. L’exemple suivant illustre l’intérêt de l’évaluation différée dans un cas simple.\n\n# Étape 1: compter les équipements\neq_dep &lt;- bpe_ens_2018_arrow |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  )\n\n# Étape 2: filtrer sur le département\nresultats &lt;- eq_dep |&gt; \n  filter(DEP == \"59\") |&gt; \n  collect()\n\nDans cet exemple, on procède à un traitement en deux temps: on compte les équipements par département, puis on filtre sur le département. Il est important de souligner que la première étape ne réalise aucun calcul par elle-même, car elle ne comprend ni collect() ni compute(). L’objet equipements_par_departement n’est pas une table et ne contient pas de données, il contient simplement une requête (query) décrivant les opérations à mener sur la table bpe_ens_2018_arrow.\nOn pourrait penser que, lorsqu’on exécute l’ensemble de ce traitement, arrow se contente d’exécuter les instructions les unes après les autres: compter les équipements par département, puis conserver uniquement le département 59. Mais en réalité arrow fait beaucoup mieux que cela: arrow analyse la requête avant de l’exécuter, et optimise le traitement pour minimiser le travail. Dans le cas présent, arrow repère que la requête ne porte en fait que sur le département 59, et commence donc par filtrer les données sur le département avant de compter les équipements, de façon à ne conserver que le minimum de données nécessaires et à ne réaliser que le minimum de calculs. Ce type d’optimisation s’avère très utile quand les données à traiter sont très volumineuses.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html#comment-bien-utiliser-arrow",
    "href": "03_Fiches_thematiques/Fiche_arrow.html#comment-bien-utiliser-arrow",
    "title": "20  Manipuler des données avec arrow",
    "section": "\n20.4 Comment bien utiliser arrow?",
    "text": "20.4 Comment bien utiliser arrow?\nAu premier abord, on peut avoir l’impression qu’arrow s’utilise exactement comme dplyr (c’est d’ailleurs fait exprès!). Il y a toutefois quelques différences qui peuvent avoir un impact considérable sur les performances des traitements. Cette partie détaille quatre recommandations à suivre pour bien utiliser arrow:\n\nUtiliser correctement l’évaluation différée;\nUtiliser compute() plutôt que collect();\nUtiliser open_dataset() plutôt que read_parquet();\nSurveiller la consommation de RAM de R.\n\n\n20.4.1 Savoir bien utiliser l’évaluation différée\nLa Section 21.5.2 a présenté la notion d’évaluation différée et son intérêt pour optimiser les performances. Toutefois, l’évaluation différée n’est pas toujours facile à utiliser, et présente des limites qu’il faut bien comprendre. Cette section décrit plus en détail le fonctionnement de l’évaluation différée et ses limites. Pour illustrer ce fonctionnement, on commence par exporter la base permanente des équipements sous la forme d’un dataset Arrow partitionné. La fiche Importer des fichiers Parquet décrit en détail ce qu’est un fichier Parquet partitionné et comment le manipuler.\n\n# Sauvegarder la BPE 2018 sous la forme d'un dataset Arrow partitionné\nwrite_dataset(\n  bpe_ens_2018_arrow,\n  \"bpe2018/\",\n  partitioning = \"REG\",\n  hive_style = TRUE\n)\n\n\n20.4.1.1 Comment fonctionne l’évaluation différée?\nCe paragraphe s’adresse aux lecteurs qui souhaitent comprendre plus en détail le fonctionnement de l’évaluation différée. Les lecteurs pressés peuvent passer directement au paragraphe suivant, sur les limites de l’évaluation différée.\nLe traitement suivant est un exemple simple d’utilisation de l’évaluation différée. Ce traitement comprend trois étapes: se connecter aux données avec open_dataset(), puis calculer le nombre d’équipements par département, et enfin sélectionner le département 59.\n\n# Étape 1: se connecter au fichier Paruet Partitionné\nds_bpe2018 &lt;- open_dataset(\n  \"bpe2018/\",\n  partitioning = schema(\"REG\" = utf8()),\n  hive_style = TRUE\n)\n\n# Étape 2: compter les équipements\neq_dep &lt;- ds_bpe2018 |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  )\n\n# Étape 3: filtrer sur le département\nresultats &lt;- eq_dep |&gt; \n  filter(DEP == \"59\")\n\nVoici quelques commentaires pour comprendre ce traitement:\n\nLe code ci-dessus n’effectue aucun calcul, car il ne comprend ni collect() ni compute(). Il faut exécuter resultats |&gt; collect() ou resultats |&gt; compute() pour que les calculs soient effectivement réalisés.\nLes objets ds_bpe2018, eq_dep et resultats ne sont pas des tables R standards contenant des données: ce sont des requêtes (de classe arrow_dplyr_query), qui décrivent des opérations à mener sur des données. C’est justement en utilisant collect() ou compute() qu’on demande à arrow d’exécuter ces requêtes avec le moteur acero.\nIl est possible d’afficher le contenu des requêtes avec la fonction show_exec_plan().\n\n\nLa première requête est très courte: elle ne contient que la description des données contenues dans le fichier Parquet partitionné.\n\n# Imprimer la première requête\nshow_exec_plan(ds_bpe2018)\n\nExecPlan with 3 nodes:\n2:SinkNode{}\n  1:ProjectNode{projection=[DEP, DEPCOM, DCIRIS, AN, TYPEQU, NB_EQUIP, REG]}\n    0:SourceNode{}\n\n\n\n\nLa deuxième requête est un peu plus longue, et si on regarde en détail, on constate deux choses. Premièrement, elle contient la première requête, mais elle n’a conservé que les variables utilisées dans le traitement (NB_EQUIP et DEP). C’est un exemple d’optimisation faite par arrow: le moteur acero a compris automatiquement qu’il suffisait de charger seulement deux variables pour réaliser le traitement. Deuxièmement, on retrouve tous les éléments du traitement (notamment le group_by et la somme), mais le traitement décrit en syntaxe tidyverse a été traduit automatiquement en fonctions internes d’arrow (la fonction sum est par exemple remplacée par hash_sum).\n\n# Imprimer la deuxième requête, qui contient la première\nshow_exec_plan(eq_dep)\n\nExecPlan with 4 nodes:\n3:SinkNode{}\n  2:GroupByNode{keys=[\"DEP\"], aggregates=[\n    hash_sum(NB_EQUIP_TOT, {skip_nulls=false, min_count=0}),\n  ]}\n    1:ProjectNode{projection=[\"NB_EQUIP_TOT\": NB_EQUIP, DEP]}\n      0:SourceNode{}\n\n\n\n\nEnfin, la troisième requête est encore plus longue, et contient les deux premières. Autrement dit, elle contient l’intégralité du traitement, donc on réalise l’intégralité du traitement lorsqu’on exécute cette requête avec resultats |&gt; collect().\n\n# Imprimer la troisième requête, qui contient les deux premières\nshow_exec_plan(resultats)\n\nExecPlan with 5 nodes:\n4:SinkNode{}\n  3:FilterNode{filter=(DEP == \"59\")}\n    2:GroupByNode{keys=[\"DEP\"], aggregates=[\n        hash_sum(NB_EQUIP_TOT, {skip_nulls=false, min_count=0}),\n    ]}\n      1:ProjectNode{projection=[\"NB_EQUIP_TOT\": NB_EQUIP, DEP]}\n        0:SourceNode{}\n\n\n\n\n\n\n20.4.1.2 Quelles sont les limites de l’évaluation différée?\nL’évaluation différée optimise les performances en minimisant la quantité de données chargées en RAM et la quantité de calculs effectivement réalisés. Avec cette vision en tête, on pourrait penser que la meilleure façon d’utiliser arrow est d’écrire un traitement entier en mode lazy (autrement dit, sans aucun compute() ni aucun collect() dans les étapes intermédiaires), et faire un unique compute() ou collect() tout à la fin du traitement, pour que toutes les opérations soient optimisées en une seule étape. Un traitement idéal ressemblerait alors à ceci:\n\n# Se connecter aux données\ndata1 &lt;- open_dataset(\"data1.parquet\")\ndata2 &lt;- open_dataset(\"data2.parquet\")\n\n# Une première étape de traitement\ntable_intermediaire1 &lt;- data1 |&gt;\n  select(...) |&gt;\n  filter(...) |&gt;\n  mutate(...)\n\n# Une deuxième étape de traitement\ntable_intermediaire2 &lt;- data2 |&gt;\n  select(...) |&gt;\n  filter(...) |&gt;\n  mutate(...)\n\n# Et encore beaucoup d'autres étapes de traitement\n# avec beaucoup d'instructions...\n\n# La dernière étape du traitement\nresultats &lt;- table_intermediaire8 |&gt;\n  left_join(\n    table_intermediaire9, \n    by = \"identifiant\"\n  ) |&gt;\n  compute()\n  \nwrite_parquet(resultats, \"resultats.parquet\")\n\nLa réalité n’est malheureusement pas si simple, car l’évaluation différée a des limites. En effet, au moment de produire le résultat final de l’exemple précédent, la fonction compute() donne l’instruction au moteur acero d’analyser puis d’exécuter l’intégralité du traitement en une seule fois (le paragraphe précédent donne un exemple détaillé). Or, le moteur acero est certes puissant, mais il a ses limites et ne peut pas exécuter en une seule fois des traitements vraiment trop complexes. Par exemple, acero rencontre des difficultés lorsqu’on enchaîne de multiples jointures de tables volumineuses.\nCes limites de l’évaluation différée peuvent provoquer des bugs violents. Lorsque le moteur acero échoue à exécuter une requête trop complexe, les conséquences sont brutales: R n’imprime aucun message d’erreur, la session R plante et il faut simplement redémarrer R et tout recommencer. Il est donc nécessaire de bien structurer le traitement pour profiter des avantages de l’évaluation différée sans en toucher les limites.\n\n20.4.1.3 Décomposer le traitement en étapes cohérentes, puis le tester\nLa solution évidente pour ne pas toucher les limites de l’évaluation différée consiste à décomposer le traitement en étapes, et à exécuter chaque étape séparément, en mettant un compute(). De cette façon, acero va réaliser séquentiellement plusieurs traitements un peu complexes, plutôt qu’échouer à réaliser un seul traitement très complexe en une seule fois.\nLa vraie difficulté consiste à savoir quelle est la bonne longueur de ces étapes intermédiaires: s’il faut éviter de faire de très longues étapes (sinon l’évaluation différée plante), il faut également éviter d’exécuter une à une les étapes du traitement (sinon on perd les avantages de l’évaluation différée). Il n’y a pas de solution miracle, et seule la pratique permet de déterminer ce qui est raisonnable. Voici toutefois quelques conseils de bon sens:\n\n\nUn point de départ raisonnable peut consister à définir des étapes de traitement qui ne dépassent pas 30 ou 40 lignes de code. Une étape de traitement de 200 lignes aura toutes les chances de poser des problèmes, d’autant qu’elle ne sera probablement pas très lisible.\n\nIl est préférable que le séquencement des étapes soit cohérent avec l’objet du traitement. Par exemple, si l’ensemble du traitement consiste à retraiter séparément deux tables, puis à les joindre, on peut imaginer trois étapes qui s’achèvent chacune par un compute(): le retraitement de la première table, le retraitement de la seconde table, et la jointure.\n\nPlus les données sont volumineuses, plus il faut être prudent avant de définir de longues étapes de traitement.\n\nPlus les opérations unitaires sont complexes, plus les étapes doivent être courtes. Par exemple, si les opérations sont des filter() et des select(), il est possible d’en enchaîner un certain nombre en une seule étape de traitement sans aucun problème, car ces opérations sont simples. Inversement, une étape de traitement ne doit pas comprendre plus de trois ou quatre jointures (car les jointures sont des opérations complexes), en particulier si les tables sont volumineuses.\n\n20.4.2 Lire des fichiers Parquet avec open_dataset() plutôt que read_parquet()\n\nIl est recommandé d’utiliser systématiquement la fonction open_dataset() plutôt que la fonction read_parquet() pour accéder à des données stockées en format Parquet. En effet, la fonction open_dataset() présente deux avantages:\n\n\nConsommation mémoire inférieure: la fonction open_dataset() crée une connexion au fichier Parquet, mais elle n’importe pas les données contenues dans le fichier tant que l’utilisateur ne le demande pas avec compute() ou collect(), et elle est optimisée pour importer uniquement les données nécessaires au traitement. Inversement, la fonction read_parquet() importe immédiatement dans R toutes les données du fichier Parquet, y compris des données qui ne servent pas à la suite du traitement.\n\nUsage plus général: la fonction open_dataset() peut se connecter à un fichier Parquet unique, mais aussi à des fichiers Parquet partitionnés, tandis que read_parquet() ne peut pas lire ces derniers.\n\n20.4.3 Utiliser des objets Arrow Table plutôt que des data.frames\n\nLorsqu’on manipule des données volumineuses, il est essentiel de manipuler uniquement des objets Arrow Table, plutôt que des data.frames (ou des tibbles). Cela implique deux recommandations:\n\nImporter les données directement dans des Arrow Table, ou à défaut convertir en Arrow Table avec la fonction as_arrow_table(). Par exemple, lorsqu’on importe un fichier Parquet avec la fonction read_parquet() ou un fichier csv avec la fonction read_csv_arrow(), il est recommandé d’utiliser l’option as_data_frame = FALSE pour que les données soient importées sous forme de Arrow Table.\n\nUtiliser systématiquement compute() plutôt que collect() dans les étapes de calcul intermédiaires. Cette recommandation est particulièrement importante.\nL’exemple suivant explique pourquoi il est préférable d’utiliser compute() dans les étapes intermédiaires:\n\n\n\n\nSituation à éviter\nLa première étape de traitement est déclenchée par collect(), la table intermédiaire res_etape1 est donc un tibble. C’est le moteur d’exécution de dplyr qui est utilisé pour manipuler res_etape1 lors de la seconde étape, ce qui dégrade fortement les performances sur données volumineuses.\n\n# Etape 1\nres_etape1 &lt;- bpe_ens_2018_tbl |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) |&gt;\n  collect()\n\n# Etape 2\nres_final &lt;- res_etape1 |&gt; \n  filter(DEP == \"59\") |&gt; \n  collect()\n\n# Sauvegarder les résultats\nwrite_parquet(res_final, \"resultats.parquet\")\n\n\n\n\nUsage recommandé\nLa première étape de traitement est déclenchée par compute(), la table intermédiaire res_etape1 est donc un Arrow Table. C’est le moteur d’exécution acero qui est utilisé pour manipuler res_etape1 lors de la seconde étape, ce qui assure de bonnes performances notamment sur données volumineuses.\n\n# Etape 1\nres_etape1 &lt;- bpe_ens_2018_tbl |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) |&gt;\n  compute()\n\n# Etape 2\nres_final &lt;- res_etape1 |&gt; \n  filter(DEP == \"59\") |&gt; \n  compute()\n\n# Sauvegarder les résultats\nwrite_parquet(res_final, \"resultats.parquet\")\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous ne savez plus si une table de données est un Arrow Table ou un tibble, il suffit d’exécuter class(le_nom_de_ma_table). Si la table est un Arrow Table, vous obtiendrez ceci: \"Table\"        \"ArrowTabular\" \"ArrowObject\"  \"R6\". Si elle est un tibble, vous obtiendrez \"tbl_df\"     \"tbl\"        \"data.frame\".\n\n\n\n20.4.4 Surveiller la consommation de RAM de R\n\nComme expliqué plus haut, les Arrow Table ne sont pas des objets R standards, mais des objets C++ qui peuvent être manipulés avec R via arrow. En pratique, cela signifie que R n’a qu’un contrôle partiel sur la RAM occupé par arrow, et ne parvient pas toujours à libérer la RAM qu’arrow a utilisée temporairement pour réaliser un traitement. En particulier, la fonction gc() ne permet pas de libérer la RAM qu’arrow a utilisée temporairement. Cette imperfection de la gestion de la RAM implique deux choses:\n\nSi on travaille sur des données volumineuses, il est important de surveiller fréquemment sa consommation de RAM pour s’assurer qu’elle n’est pas excessive (cf. fiche Superviser sa session R) ;\nSi la consommation de RAM devient très élevée, la seule solution semble être de redémarrer la session R. En pratique, redémarrer la session R ne fait pas perdre plus de quelques minutes, car grâce à arrow et Parquet le chargement des données est très rapide.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html#notions-avancées",
    "href": "03_Fiches_thematiques/Fiche_arrow.html#notions-avancées",
    "title": "20  Manipuler des données avec arrow",
    "section": "\n20.5 Notions avancées",
    "text": "20.5 Notions avancées\n\n20.5.1 Connaître les limites d’arrow\n\nLe projet arrow est relativement récent et en développement actif. Il n’est donc pas surprenant qu’il y ait parfois des bugs, et que certaines fonctions standards de R ne soient pas encore disponibles en arrow. Il est important de connaître les quelques limites d’arrow pour savoir comment les contourner. Voici quatre limites d’arrow à la date de rédaction de cette fiche (janvier 2024):\n\nles jointures de tables volumineuses: arrow ne parvient pas à joindre des tables de données très volumineuses; il est préférable d’utiliser duckdb pour ce type d’opération;\nles réorganisations de données (wide-to-long et long-to-wide): il n’existe pas à ce jour dans arrow de fonctions pour réorganiser une table de données (comme pivot_wider et pivot_longer du package tidyr).\n\nles fonctions fenêtre (window functions): arrow ne permet pas d’ajouter directement à une table des informations issues d’une agrégation par groupe de la même table. Par exemple, arrow ne peut pas ajouter directement à la base permanente des équipements une colonne égale au nombre total d’équipements du département:\n\n# Arrow ne peut pas exécuter ceci\ndata &lt;- bpe_ens_2018_arrow |&gt;\n  group_by(DEP) |&gt;\n  mutate(\n    NB_EQUIP_TOTAL_DEP  = sum(NB_EQUIP)\n  ) |&gt;\n  compute()\n\n\n\nles empilements de tables: il est facile d’empiler plusieurs tibbles avec dplyr grâce à la fonction bind_rows(): bind_rows(table1, table2, table3, table4). En revanche, il n’existe pas à ce jour de fonction similaire dans arrow. Les fonctions union et union_all permettent d’empiler seulement deux Arrow Table, donc pour empiler plusieurs Arrow Tables il faut appeler plusieurs fois ces fonctions. Par ailleurs, les deux Arrow Table doivent être parfaitement compatibles pour être empilés (il faut le même nombre de colonnes avec le même nom et le même type, ce qui n’est pas toujours le cas en pratique).\n\n# Comment empiler de multiples Arrow Tables\nresultats &lt;- table1 |&gt;\n  union(table2) |&gt;\n  union(table3) |&gt;\n  union(table4) |&gt;\n  compute()\n\n\n\n20.5.2 Surmonter le problème des fonctions non supportées par acero\n\nLorsqu’on manipule des données avec arrow, il arrive fréquemment qu’on écrive un traitement que le moteur d’exécution acero n’arrive pas à exécuter. En ce cas, R renonce à manipuler les données sous forme de Arrow Table avec le moteur acero, convertit les données en tibble et poursuit le traitement avec le moteur d’exécution de dplyr (comme un traitement dplyr standard). R signale systématiquement le recours à cette solution de repli par un message d’erreur qui se termine par pulling data into R.\nLe recours à cette solution de repli a pour conséquence de dégrader fortement les performances (car le moteur de dplyr est moins efficace qu’acero). Il est donc préférable d’essayer de réécrire la partie du traitement qui pose problème avec des fonctions supportées par acero. Cela est particulièrement recommandé si les données manipulées sont volumineuses ou si le traitement concerné doit être exécuté fréquemment.\n\n\n\n\n\n\nTip\n\n\n\nIl arrive qu’il soit impossible de trouver une solution entièrement supportée par acero, ou que la solution soit vraiment trop complexe à écrire. Ce n’est pas une catastrophe: en dernier recours, on peut tout à fait convertir temporairement les données en tibble (avec collect()) et exécuter le traitement qui pose problème avec dplyr. Le traitement sera simplement plus lent (voire beaucoup plus lent). En revanche, il est important de reconvertir ensuite les données en Arrow Table le plus vite possible, en utilisant la fonction as_arrow_table().\n\n\n\n20.5.2.1 Une solution simple existe-t-elle?\nDans la plupart des cas, il est possible de trouver une solution simple pour écrire un traitement que le moteur acero peut exécuter. Voici quelques pistes:\n\nVérifier qu’on utilise la dernière version d’arrow et mettre à jour le package si ce n’est pas le cas;\nÉtudier en détail le message d’erreur renvoyé par R pour bien comprendre d’où vient le problème;\nRegarder la liste des fonctions du tidyverse supportées par acero pour voir s’il est possible d’utiliser une fonction supportée par acero;\nFaire des tests pour pour voir si une réécriture mineure du traitement peut régler le problème.\n\nL’exemple qui suit montre que la solution peut être très simple, même lorsque l’erreur semble complexe. Dans cet exemple, on veut calculer le nombre de boulangeries (TYPEQU == \"B203\") et de poissonneries (TYPEQU == \"B206\") dans chaque département, en stockant les résultats dans un Arrow Table (avec compute()). Malheureusement, acero ne parvient pas à réaliser ce traitement, et R est contraint de convertir les données en tibble.\n\nresultats &lt;- bpe_ens_2018_arrow |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    nb_boulangeries  = sum(NB_EQUIP * (TYPEQU == \"B203\")),\n    nb_poissonneries = sum(NB_EQUIP * (TYPEQU == \"B206\"))\n  ) |&gt;\n  compute()\n\nLe message d’erreur renvoyé par R est la suivante: ! NotImplemented: Function 'multiply_checked' has no kernel matching input types (double, bool); pulling data into R. En lisant attentivement le message d’erreur et en le rapprochant du traitement, on finit par comprendre que l’erreur vient de l’opération sum(NB_EQUIP * (TYPEQU == \"B203\")): arrow ne parvient pas à faire la multiplication entre NB_EQUIP (un nombre réel) et (TYPEQU == \"B203\") (un booléen). La solution est très simple: il suffit de convertir (TYPEQU == \"B203\") en nombre entier avec la fonction as.integer() qui est supportée par acero. Le code suivant peut alors être entièrement exécuté par acero:\n\nresultats &lt;- bpe_ens_2018_arrow |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    nb_boulangeries  = sum(NB_EQUIP * as.integer(TYPEQU == \"B203\")),\n    nb_poissonneries = sum(NB_EQUIP * as.integer(TYPEQU == \"B206\"))\n  ) |&gt;\n  compute()\n\n\n20.5.2.2 Passer par duckdb\n\nIl arrive qu’il ne soit pas possible de résoudre le problème en réécrivant légèrement le traitement. Une autre solution peut consister à passer par duckdb, qui permet de manipuler directement des objets Arrow Table de façon simple et transparente. Dans l’exemple suivant, on veut ajouter à la base permanente des équipements une colonne égale au nombre total d’équipements du département. Cette opération ne peut pas être exécutée par arrow (voir le paragraphe Section 20.5.1), contrairement à duckdb. Voici comment faire avec duckdb:\n\nlibrary(duckdb)\n\ndata &lt;- bpe_ens_2018_arrow |&gt;\n  to_duckdb() |&gt;\n  group_by(DEP) |&gt;\n  mutate(\n    NB_EQUIP_TOTAL_DEP  = sum(NB_EQUIP)\n  ) |&gt;\n  to_arrow() |&gt;\n  compute()\n\nCet exemple appelle trois commentaires:\n\nLa fonction to_duckdb() sert à ce que duckdb puisse accéder à l’objet Arrow Table;\nSymétriquement, la fonction to_arrow() sert à remettre les données dans un objet Arrow Table;\nLes instructions figurant entre ces deux étapes (le group_by() puis le mutate()) sont exécutées par le moteur d’exécution de duckdb, de façon complètement transparente pour l’utilisateur.\n\n20.5.2.3 Définir soi-même des fonctions arrow (utilisation avancée)\nSi les pistes mentionnées précédemment ne fournissent pas de solution simple, il est possible d’aller plus loin et d’écrire ses propres fonctions arrow. Cette approche permet de faire beaucoup plus de choses mais elle nécessite de bien comprendre le fonctionnement d’arrow et les fonctions internes de la librairie libarrow. Il s’agit d’une utilisation avancée d’arrow qui dépasse le cadre de la documentation utilitR. Les lecteurs intéressés pourront consulter les deux ressources suivantes:\n\n\nun post de blog qui décrit en détail les liens entre libarrow et R (en anglais);\nla partie du Apache Arrow R Cookbook qui porte sur les Arrow functions.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html#sec-template-arrow",
    "href": "03_Fiches_thematiques/Fiche_arrow.html#sec-template-arrow",
    "title": "20  Manipuler des données avec arrow",
    "section": "\n20.6 Un exemple de traitement de données avec arrow\n",
    "text": "20.6 Un exemple de traitement de données avec arrow\n\nLe code ci-dessous propose un exemple de traitement de données avec arrow qui suit les recommandations et conseils de la présente fiche. Vous pouvez le copier-coller et vous en inspirer pour construire vos propres traitements!\n\nlibrary(arrow)\nlibrary(dplyr)\n\n# Autoriser arrow à utiliser plusieurs processeurs en parallèle\noptions(arrow.use_threads = TRUE)\n# Définir le nombre de processeurs qu'arrow peut utiliser - ici on prend la partie entière du nombre de processeurs disponibles divisé par 4\narrow::set_cpu_count(parallel::detectCores() %/% 4)\n\n##################\n### Se connecter aux données\n### Conseil: utiliser open_dataset() plutôt que read_parquet()\n##################\n\n# Cas 1: se connecter à un unique fichier Parquet\ndataset1 &lt;- open_dataset(\"mon/beau/dossier/dataset1.parquet\")\n\n# Cas 2: se connecter à un fichier Parquet partitionné par la variable DEP\ndataset2 &lt;- open_dataset(\n  \"mon/beau/dossier/dataset2/\",\n  partitioning = schema(\n    DEP = utf8()\n  )\n)\n\n##################\n### Faire les traitements\n### Conseils: \n### - Faire des étapes de traitement de 30-40 lignes, suivies d'un compute()\n### - Ne pas utiliser collect() dans les calculs intermédiaires sur des données volumineuses\n### - Faire attention à suivre la consommation de RAM\n##################\n\n# Une première étape de traitement portant sur le dataset1\ntable_intermediaire1 &lt;- dataset1 |&gt;\n  select(...) |&gt;\n  filter(...) |&gt;\n  mutate(...) |&gt;\n  compute()\n\n# Une première étape de traitement portant sur le dataset2\ntable_intermediaire2 &lt;- dataset2 |&gt;\n  select(...) |&gt;\n  filter(...) |&gt;\n  mutate(...) |&gt;\n  compute()\n\n# Et encore beaucoup d'autres étapes de traitement\n# avec beaucoup d'instructions...\n\n# La dernière étape du traitement\nresultat_final &lt;- table_intermediaire8 |&gt;\n  left_join(\n    table_intermediaire9, \n    by = \"identifiant\"\n  ) |&gt;\n  compute()\n\n\n##################\n### Visualiser un extrait d'une table intermédiaire\n### Vous pouvez utiliser collect() sur un extrait des données\n##################\n\nextrait_table2 &lt;- table_intermediaire2 |&gt; slice_head(n = 1000) |&gt; collect()\nView(extrait_table2)\n\n##################\n### Visualiser les résultats finaux sous forme de tibble\n### Vous pouvez utiliser collect() sur de petites données\n##################\n\nresultat_final_tbl &lt;- resultat_final |&gt; collect()\n\n\n##################\n### Exporter les résultats\n### Conseil: partitionner les fichiers Parquet si les données sont volumineuses\n##################\n\n# Cas 1: exporter les résultats sous la forme d'un unique fichier Parquet\nwrite_parquet(resultat_final, \"mon/dossier/de/sortie/resultat_final.parquet\")\n\n# Cas 2: exporter les résultats sous la forme d'un fichier Parquet par les variables DEP et annee\nwrite_dataset(\n  resultat_final, \n  \"mon/dossier/de/sortie/resultat_final/\",\n  format= \"parquet\",\n  hive_style = TRUE,\n  partitioning = c(\"DEP\", \"annee\")\n)",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_arrow.html#RessourcesArrow",
    "href": "03_Fiches_thematiques/Fiche_arrow.html#RessourcesArrow",
    "title": "20  Manipuler des données avec arrow",
    "section": "\n20.7 Pour en savoir plus",
    "text": "20.7 Pour en savoir plus\n\nla documentation officielle du package arrow (en anglais);\n\nun post de blog qui décrit en détail les liens entre libarrow et R (en anglais);\nla liste des fonctions du tidyverse supportées par acero.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Manipuler des données avec `arrow`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html",
    "title": "21  Manipuler des données avec duckdb",
    "section": "",
    "text": "21.1 Tâches concernées et recommandations\nL’utilisateur souhaite manipuler des données structurées sous forme de data.frame par le biais de l’écosystème duckdb (sélectionner des variables, sélectionner des observations, créer des variables, joindre des tables).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html#tâches-concernées-et-recommandations",
    "title": "21  Manipuler des données avec duckdb",
    "section": "",
    "text": "Important\n\n\n\nTâches concernées et recommandations\n\nPour des tables de données de taille petite et moyenne (inférieure à 1 Go ou moins d’un million d’observations), il est recommandé d’utiliser les packages tibble, dplyr et tidyr qui sont présentés dans la fiche Manipuler des données avec le tidyverse;\nPour des tables de données de grande taille (plus de 1 Go en CSV, plus de 200 Mo en Parquet, ou plus d’un million d’observations), il est recommandé d’utiliser soit les packages arrow (voir la fiche Manipuler des données avec arrow) et duckdb qui fait l’objet de la présente fiche, soit le package data.table qui fait l’objet de la fiche Manipuler des données avec data.table.\nIl est essentiel de travailler avec la dernière version d’arrow, de duckdb et de R car les packages arrow et duckdb sont en cours de développement. Par ailleurs, les recommandations d’utilitR peuvent évoluer en fonction du développement de ces packages.\nSi les données sont très volumineuses (plus de 5 Go en CSV, plus de 1 Go en Parquet ou plus de 5 millions d’observations), il est recommandé de manipuler les données avec duckdb (et avec arrow) plutôt qu’avec le tidyverse. Il peut arriver que le volume de données soit tellement important qu’il ne soit pas possible de les traiter avec duckdb et arrow; il faut s’orienter vers des infrastructures big data permettant le calcul distribué et utiliser des logiciels adaptés (Spark par exemple).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nApprendre à utiliser duckdb n’est pas difficile, car la syntaxe utilisée est quasiment identique à celle du tidyverse. Toutefois, une bonne compréhension du fonctionnement de R et de duckdb est nécessaire pour bien utiliser duckdb sur des données volumineuses. Voici quelques conseils pour bien démarrer:\n\nIl est indispensable de lire la fiche Manipuler des données avec le tidyverse avant de lire la présente fiche.\nIl est recommandé de lire les fiches Se connecter à une base de données et Manipuler des données avec arrow avant de lire la présente fiche.\nIl est complètement normal de rencontrer des erreurs difficiles à comprendre lorsqu’on commence à utiliser duckdb, il ne faut donc pas se décourager.\nIl ne faut pas hésiter à demander de l’aide à des collègues, ou à poser des questions sur les salons Tchap adaptés (le salon Langage R par exemple).",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html#présentation-du-package-duckdb-et-du-projet-associé",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html#présentation-du-package-duckdb-et-du-projet-associé",
    "title": "21  Manipuler des données avec duckdb",
    "section": "\n21.2 Présentation du package duckdb et du projet associé",
    "text": "21.2 Présentation du package duckdb et du projet associé\n\n21.2.1 Qu’est-ce que duckdb?\nDuckDB est un projet open-source (license MIT) qui propose un moteur SQL optimisé pour réaliser des travaux d’analyse statistique sur des bases de données :\n\nun moteur SQL rapide, capable d’utiliser des données au format parquet sans les charger complètement en mémoire,\nun dialecte SQL enrichi avec des fonctions qui facilitent l’analyse de données,\nune installation et une utilisation faciles,\nun moteur portable, utilisable sous Windows, MacOS, Linux, et interfacé avec de nombreux langages de programmation (R, Python, Javascript, etc.).\n\nUn point important à comprendre est que DuckDB n’est pas un outil spécifique à R: DuckDB est un système de gestion de base de données (SGBD), similaire par exemple à une base PostgreSQL. Cela a deux conséquencées:\n\nla base de données DuckDB a une existence propre sur le disque ou dans la mémoire, et on peut donc lui envoyer directement des requêtes SQL, sans passer par R.\nIl faut bien distinguer le projet DuckDB du package R duckdb. Ce package propose simplement une interface avec R parmi les autres interfaces existantes : Python, Java, Javascript, Julia, etc.\n\nToutefois, DuckDB est très facile à utiliser avec R, ce qui permet de bénéficier des optimisations inhérentes au langage SQL, à la fois en terme d’utilisation de la mémoire et de rapidité de calcul. C’est de plus un bon intermédiaire avant de passer à des infrastructures avancées telles que spark ou oracle.\n\n21.2.2 À quoi sert le package duckdb?\nDu point de vue d’un statisticien utilisant R, le package duckdb permet de faire trois choses:\n\nImporter des données (exemples: fichiers CSV, fichiers Parquet);\nManipuler des données avec la syntaxe dplyr, ou avec le langage SQL;\nÉcrire des données au format Parquet.\n\n21.2.3 Quels sont les avantages de duckdb?\n\n\nDisponibilité immédiate: on peut pré-visualiser les données ou le résultat d’un calcul sans l’exécuter totalement, sans attendre le chargement des données;\n\nPerformances élevées: duckdb est très rapide pour la manipulation de données tabulaires (nettement plus performant que dplyr par exemple);\n\nNe pas nécessairement charger les données en mémoire: duckdb permet également de travailler directement sur des fichiers du disque dur;\n\nOptimisations automatiques: duckdb sélectionne automatiquement les colonnes nécessaires, et ne lit que les lignes nécessaires. Cela permet d’accélérer les calculs et de réduire considérablement les besoins en mémoire, même lorsque les données sont volumineuses;\n\nFacilité d’apprentissage grâce aux approches dplyr et SQL: duckdb peut être utilisé avec les verbes de dplyr (select, mutate, etc.) et/ou avec le langage SQL. Par conséquent, il n’est pas nécessaire d’apprendre une nouvelle syntaxe pour utiliser duckdb, on peut s’appuyer sur la ou les approches que l’on maîtrise déjà.\n\n21.2.4 Quels sont les points d’attention à l’usage ?\n\n\nReprésentation des données en mémoire : duckdb est un moteur SQL. Les lignes n’ont pas d’ordre pré-défini, et le résultat d’un traitement peut être dans un ordre imprévisible.\n\nTraitement de données volumineuses: duckdb peut traiter de gros volumes de données, qu’elles soient en mémoire vive ou sur le disque dur. Lorsque les données sont en mémoire vive, les packages duckdb et arrow peuvent être utilisés conjointement de façon très efficace: cela veut dire concrètement que duckdb peut manipuler directement des données stockées dans un objet Arrow Table, sans avoir à convertir les données dans un autre format. Avec des données stockées sur le disque dur, duckdb est capable de faire les traitements sur des données plus volumineuses que la mémoire vive (RAM). C’est un avantage majeur en comparaison aux autres approches possibles en R (data.table et dplyr par exemple). Toutefois, il faut dans ce cas ajouter le temps de lecture des données au temps nécessaire pour le calcul.\n\nÉvaluation différée: duckdb construit des requêtes SQL, qui sont exécutées uniquement lorsque le résultat est explicitement demandée, après optimisation des étapes intermédiaires, et peuvent être exécutées partiellement. La Section 21.5.2 présente en détail cette notion.\n\nTraduction en SQL: duckdb traduit automatiquement les instructions dplyr en requêtes SQL (de la même façon qu’arrow traduit ces instructions en code C++). Il arrive toutefois que certaines fonctions de dplyr n’aient pas d’équivalent direct en duckdb et ne puissent être traduites automatiquement. Dans ce cas (qui est heureusement moins fréquent qu’avec arrow), il faut parfois utiliser une fonction SQL directement ou trouver une solution pour contourner le problème. La Section 21.5.3 donne quelques trucs et astuces dans ce cas.\n\nInteropérabilité: duckdb est conçu pour être interopérable entre plusieurs langages de programmation tels que R, Python, Java, C++, etc. Cela signifie que les données peuvent être échangées entre ces langages sans avoir besoin de convertir les données, d’où des gains importants de temps et de performance.\n\n21.2.5 Quand utiliser duckdb plutôt que arrow ?\nLes packages duckdb et arrow ont des cas d’usage très similaires (voir la fiche Manipuler des données avec arrow), mais on peut préférer l’un à l’autre selon les cas. On peut également les utiliser ensemble pour profiter de chacun de leurs avantages. Le tableau ci-dessous compare quelques cas d’usage de ces deux packages :\n\n\n\n\n\n\n\nJe souhaite…\narrow\nduckdb\n\n\n\nOptimiser mes traitements pour des données volumineuses\n✔️\n✔️\n\n\nTravailler sur un fichier .parquet ou .csv sans le charger entièrement en mémoire\n✔️\n✔️\n\n\nUtiliser la syntaxe dplyr pour traiter mes données\n✔️\n✔️\n\n\nUtiliser du langage SQL pour traiter mes données\n❌\n✔️\n\n\nJoindre des tables très volumineuses (plus de 4 Go)\n❌\n✔️\n\n\nUtiliser des fonctions fenêtres (voir Section 21.6)\n❌\n✔️\n\n\nUtiliser des fonctions statistiques qui n’existent pas dans arrow (voir Section 21.6)\n❌\n✔️\n\n\nÉcrire un fichier .parquet\n✔️\n✔️ *\n\n\n\n* pour écrire un fichier .parquet avec le package duckdb, il faut utiliser une instruction SQL (voir Section 21.5.4.2)",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html#installation-de-duckdb",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html#installation-de-duckdb",
    "title": "21  Manipuler des données avec duckdb",
    "section": "\n21.3 Installation de duckdb\n",
    "text": "21.3 Installation de duckdb\n\nIl suffit d’installer le package duckdb, qui contient à la fois DuckDB et une interface pour que R puisse s’y connecter.\n\ninstall.packages(\"duckdb\", repos=\"https://cloud.r-project.org\")",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html#utilisation-de-duckdb",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html#utilisation-de-duckdb",
    "title": "21  Manipuler des données avec duckdb",
    "section": "\n21.4 Utilisation de duckdb\n",
    "text": "21.4 Utilisation de duckdb\n\nDans cette section, on présente l’utilisation basique de duckdb. C’est très facile: il n’est pas nécessaire de connaître le langage SQL car il est possible d’utiliser duckdb avec la syntaxe dplyr.\n\n21.4.1 Charger le package duckdb\n\nPour utiliser duckdb, il faut commencer par charger le package. Cela charge automatiquement le package DBI, qui permet de se connecter aux bases de données. Il est utile de charger également le package dplyr afin de pouvoir requêter la base de données avec la syntaxe bien connue de dplyr.\n\nlibrary(duckdb)\nlibrary(dplyr)\n\nLe moteur duckdb fonctionnant “en dehors” de R, il détecte le nombre de processeurs et effectue les opérations en parallèle si possible.\n\n21.4.2 Connexion à une base de données\nduckdb est une base de données distante et s’utilise comme telle: il faut ouvrir une connexion, puis “charger” les données dans la base de données pour les manipuler.\nComme beaucoup d’autres bases de données (distantes ou locales), on ouvre une connexion au moteur duckdb avec une base de données en mémoire vive de la façon suivante :\n\nconn_ddb &lt;- DBI::dbConnect(drv = duckdb::duckdb())\n\nConcrètement, cette commande crée une nouvelle base de données duckdb dans la mémoire vive. Cette base de données ne contient aucune donnée lorsqu’elle est créée. L’objet conn_ddb apparaît dans l’onglet Data de l’environnement RStudio, mais la liste des tables n’y est pas directement accessible. Pour plus d’informations, se reporter à la documentation du package DBI.\nÀ la fin du traitement ou du programme, on ferme la connexion avec le code ci-dessous. L’option shutdown est importante : elle permet de fermer complètement la session duckdb et de libérer la mémoire utilisée. Si on n’utilise pas cette option, il arrive souvent que des connexions à moitié ouvertes continuent à consommer des ressources, et il faut alors relancer la session R.\n\nDBI::dbDisconnect(conn_ddb, shutdown = TRUE)\n\nPar défaut, duckdb utilisera tous les cœurs disponibles. Si vous travaillez sur un serveur mutualisé, il est conseillé de limiter le nombre de cœurs utilisés par duckdb afin de ne pas consommer toutes les ressources. Vous pouvez trouver plus d’information dans la section Configurer duckdb.\n\nconn_ddb &lt;- DBI::dbConnect(duckdb::duckdb(\n  config = list(threads = \"6\")\n))\n\nPour la suite, on supposera que la connexion à une base de données duckdb est ouverte.\n\n21.4.3 Chargement des données\nUne fois qu’on s’est connecté à une base de données duckDB, il faut charger des données dans cette base de données. Il y a deux façons de le faire:\n\nEn établissant un lien entre la base de données duckDB et les objets de la session R;\nEn indiquant à duckdb l’emplacement des données sur le disque dur.\n\n\n21.4.3.1 Chargement de données provenant de la session R\n\nLa fonction duckdb_register() permet de charger dans duckdb des données présentes dans la session R. Cette méthode a l’avantage de ne pas recopier les données: elle se contente d’établir un lien logique entre la base de données duckdb et un objet de la session R. Voici un exemple avec la Base permanente des équipements: grâce à la fonction duckdb::duckdb_register(), l’objet bpe_ens_2018 est référencé dans la base de données duckdb sous le nom bpe_ens_2018_duckdb.\n\n# Charger la Base permanente des équipements 2018 dans la session R\nbpe_ens_2018 &lt;- doremifasolData::bpe_ens_2018 |&gt; as_tibble()\n\n# Etablir le lien logique entre la base de données duckdb et la table de données\nconn_ddb %&gt;% duckdb::duckdb_register(\n  name = \"bpe_ens_2018_duckdb\", \n  df = bpe_ens_2018)\n\nLe code ci-dessous permet de vérifier que le chargement des données a bien fonctionné. La fonction tbl permet d’accéder à un objet de la base de données par le nom (de la table), ou par du code SQL (utilisation un peu plus avancée). Par défaut, duckdb affiche les 10 premières lignes du résultat, sans effectuer tout le calcul. C’est très pratique et très rapide !\n\nconn_ddb %&gt;% tbl(\"bpe_ens_2018_duckdb\")\n\n# Source:   table&lt;bpe_ens_2018_duckdb&gt; [?? x 7]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n   REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 84    01    01001  01001   2018 A401          2\n 2 84    01    01001  01001   2018 A404          4\n 3 84    01    01001  01001   2018 A504          1\n 4 84    01    01001  01001   2018 A507          1\n 5 84    01    01001  01001   2018 B203          1\n 6 84    01    01001  01001   2018 C104          1\n 7 84    01    01001  01001   2018 D233          1\n 8 84    01    01001  01001   2018 F102          1\n 9 84    01    01001  01001   2018 F111          1\n10 84    01    01001  01001   2018 F113          1\n# ℹ more rows\n\n\n\n21.4.3.2 Chargement de données stockées sur le disque dur\nPour l’exemple suivant, on sauvegarde les données bpe_ens_2018 au format Parquet.\n\nbpe_ens_2018 |&gt; arrow::write_dataset(\"bpe_ens_2018_dataset\")\n\nIl existe deux méthodes pour manipuler des données stockées en Parquet avec duckdb sans avoir à les charger en mémoire: soit utiliser la fonction dplyr::tbl() qui lit directement les fichiers Parquet avec duckdb, soit utiliser la fonction arrow::open_dataset() et créer un lien logique avec la fonction arrow::to_duckdb(). Si la deuxième méthode est plus simple, surtout quand vous connaissez déjà arrow, la première est systématiquement plus efficace et peut générer des gains de consommation mémoire et de temps de traitement conséquents. Il est donc conseillé de ne pas lire vos fichiers avec arrow::open_dataset si vos traitements sont lourds (il ne faut pas hésiter à faire des tests).\nLa première approche repose uniquement sur duckdb. Vous devez utilisez la fonction dplyr::tbl:\n\nconn_ddb %&gt;% tbl(\"read_parquet('bpe_ens_2018_dataset/**/*.parquet')\")\n\n# Source:   SQL [?? x 7]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n   REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 84    01    01001  01001   2018 A401          2\n 2 84    01    01001  01001   2018 A404          4\n 3 84    01    01001  01001   2018 A504          1\n 4 84    01    01001  01001   2018 A507          1\n 5 84    01    01001  01001   2018 B203          1\n 6 84    01    01001  01001   2018 C104          1\n 7 84    01    01001  01001   2018 D233          1\n 8 84    01    01001  01001   2018 F102          1\n 9 84    01    01001  01001   2018 F111          1\n10 84    01    01001  01001   2018 F113          1\n# ℹ more rows\n\n\nQuelques explications de cette commande:\n\nLa fonction read_parquet est une fonction interne à duckdb, elle ne doit surtout pas être confondue avec la fonction read_parquet() du package arrow. Remarque: duckdb propose aussi des fonctions pour lire d’autres formats comme csv, json…\n\n**/*.parquet est un motif qui indique que vous souhaitez lire, dans tous les sous-dossiers quelque soit le niveau (**), l’ensemble des fichiers parquets (*.parquet) qui s’y trouvent. C’est notamment utile pour lire des fichiers Parquet partitionnés. Quand vous n’avez pas besoin de passer d’arguments à read_parquet, vous pouvez l’omettre :\n\n\nconn_ddb %&gt;% tbl('bpe_ens_2018_dataset/**/*.parquet')\n\n# Source:   table&lt;\"bpe_ens_2018_dataset/**/*.parquet\"&gt; [?? x 7]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n   REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 84    01    01001  01001   2018 A401          2\n 2 84    01    01001  01001   2018 A404          4\n 3 84    01    01001  01001   2018 A504          1\n 4 84    01    01001  01001   2018 A507          1\n 5 84    01    01001  01001   2018 B203          1\n 6 84    01    01001  01001   2018 C104          1\n 7 84    01    01001  01001   2018 D233          1\n 8 84    01    01001  01001   2018 F102          1\n 9 84    01    01001  01001   2018 F111          1\n10 84    01    01001  01001   2018 F113          1\n# ℹ more rows\n\n\nLa seconde approche consiste à passer par arrow, puis à transmettre les données à duckdb. Cette méthode utilise un objet intermédiaire de type Arrow Dataset (voir la fiche Manipuler des données avec arrow):\n\n# Créer une connexion au dataset Parquet\nbpe_ens_2018_dataset &lt;- arrow::open_dataset(\"bpe_ens_2018_dataset\")\n\n# Etablir le lien entre la base de données duckdb et le dataset Parquet\nbpe_ens_2018_dataset %&gt;% arrow::to_duckdb(conn_ddb)\n\n# Source:   table&lt;arrow_001&gt; [?? x 7]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n   REG   DEP   DEPCOM DCIRIS    AN TYPEQU NB_EQUIP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 84    01    01001  01001   2018 A401          2\n 2 84    01    01001  01001   2018 A404          4\n 3 84    01    01001  01001   2018 A504          1\n 4 84    01    01001  01001   2018 A507          1\n 5 84    01    01001  01001   2018 B203          1\n 6 84    01    01001  01001   2018 C104          1\n 7 84    01    01001  01001   2018 D233          1\n 8 84    01    01001  01001   2018 F102          1\n 9 84    01    01001  01001   2018 F111          1\n10 84    01    01001  01001   2018 F113          1\n# ℹ more rows\n\n\nCes deux approches ont un point commun important: elles établissent une connexion aux données contenues dans le dataset Parquet, mais elles ne chargent pas les données en mémoire (ni dans la mémoire de R, ni dans celle de DuckDB).\nPour plus de commodité, on sauvegarde l’instruction précédente dans la variable bpe_ens_2018_dataset.\n\nbpe_ens_2018_dataset &lt;- conn_ddb %&gt;% \n  tbl('bpe_ens_2018_dataset/*.parquet')\n\n\n21.4.4 Manipulation des données avec la syntaxe dplyr\n\nLe package R duckdb a été écrit de façon à pouvoir manipuler les données avec la syntaxe de dplyr (select, filter, mutate, left_join, etc.). duckdb traduit le code R, y compris certaines fonctions de stringr et lubridate en requête SQL. Cela s’avère très commode en pratique, car lorsqu’on sait utiliser dplyr et le tidyverse, on peut commencer à utiliser duckdb sans avoir à apprendre une nouvelle syntaxe de manipulation de données.\nDans l’exemple suivant, on calcule le nombre d’équipements par région, à partir d’un tibble et à partir d’une table duckdb. La seule différence apparente entre les deux traitement est la présence de la fonction collect() à la fin des instructions; cette fonction indique que l’on souhaite obtenir le résultat du traitement sous la forme d’un tibble. La raison d’être de ce collect() est expliquée plus loin, dans le paragraphe sur l’évaluation différée. Les résultats sont identiques, à l’exception de l’ordre des lignes. En effet, un moteur SQL ne respecte pas l’ordre par défaut, il faut le demander explicitement avec arrange.\n\n\nManipulation d’un tibble\n\ndoremifasolData::bpe_ens_2018 |&gt;\n  group_by(REG) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  )\n\n# A tibble: 18 × 2\n   REG   NB_EQUIP_TOT\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 01           23939\n 2 02           19068\n 3 03            7852\n 4 04           30767\n 5 06            7353\n 6 11          469181\n 7 24           81379\n 8 27           96309\n 9 28          107186\n10 32          175859\n11 44          181130\n12 52          119689\n13 53          111291\n14 75          237008\n15 76          256813\n16 84          303775\n17 93          254405\n18 94           21778\n\n\n\n\n\nManipulation d’une table duckdb\n\nbpe_ens_2018_dataset |&gt;\n  group_by(REG) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) |&gt;\n  collect()\n\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n\n\n# A tibble: 18 × 2\n   REG   NB_EQUIP_TOT\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 84          303775\n 2 32          175859\n 3 02           19068\n 4 03            7852\n 5 75          237008\n 6 52          119689\n 7 01           23939\n 8 94           21778\n 9 76          256813\n10 06            7353\n11 28          107186\n12 53          111291\n13 24           81379\n14 93          254405\n15 11          469181\n16 04           30767\n17 44          181130\n18 27           96309\n\n\n\n\nOn peut examiner la requête SQL construite par duckdb avec la fonction show_query().\n\nbpe_ens_2018_dataset |&gt;\n  group_by(REG) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT REG, SUM(NB_EQUIP) AS NB_EQUIP_TOT\nFROM \"bpe_ens_2018_dataset/*.parquet\"\nGROUP BY REG\n\n\nCette requête est envoyée au serveur SQL et exécutée de façon différente en fonction de la dernière instruction du traitement:\n\nsi le traitement se termine par collect(): le calcul est exécuté en entier et le résultat est retourné sous la forme d’un tibble,\nsi le traitement se termine par print(n=nb_lignes): le calcul est exécuté partiellement, et seules les nb_lignes demandées sont affichées. Cela permet de minimiser les ressources et la mémoire utilisées.\n\nCe point est important: en utilisant print(), on peut prévisualiser le résultat d’une requête duckdb de façon très rapide, sans exécuter tout le traitement. Il ne faut pas hésiter à s’en servir pour explorer les données et pour construire le traitement étape par étape, en ajustant en fonction des résultats.\n\n21.4.5 Écriture au format Parquet\nPour écrire une table (ou le résultat de n’importe quelle requête) sur le disque au format Parquet, il est recommandé d’utiliser la librairie arrow.\n\nbpe_ens_2018_dataset %&gt;% \n  arrow::to_arrow() %&gt;% arrow::write_dataset(\"temp_dataset\")\nlist.files(\"temp_dataset\") # liste des fichiers du répertoire temp_dataset/\n\n[1] \"part-0.parquet\"\n\n\nPour un usage basique en syntaxe dplyr, passer par arrow (au lieu de SQL) est plus facile à manipuler, notamment quand on souhaite ajouter des options telle que le partitionnement.\n\n21.4.6 Erreurs courantes\nCette section présente quelques erreurs classiques.\n\n21.4.6.1 On a éliminé des colonnes nécessaires\n\nbpe_ens_2018_dataset |&gt; select(DEP) |&gt;\n  mutate(NB_EQUIP_TOTAL_DEP  = sum(NB_EQUIP))\n\nError in `mutate()`:\nℹ In argument: `NB_EQUIP_TOTAL_DEP = sum(NB_EQUIP)`\nCaused by error:\n! Object `NB_EQUIP` not found.\n\n\n\n21.4.6.2 Convertir les types\nDans cet exemple, on veut multiplier un nombre par une indicatrice.\n\nbpe_ens_2018_dataset %&gt;%\n  summarise(nb_boulangeries  = sum(NB_EQUIP * (TYPEQU == \"B203\")), .by = DEP)\n\nError in `collect()`:\n! Failed to collect lazy table.\nCaused by error:\n! rapi_prepare: Failed to prepare query SELECT DEP, SUM(NB_EQUIP * (TYPEQU = 'B203')) AS nb_boulangeries\nFROM \"bpe_ens_2018_dataset/*.parquet\"\nGROUP BY DEP\nLIMIT 11\nError: Binder Error: No function matches the given name and argument types '*(DOUBLE, BOOLEAN)'. You might need to add explicit type casts.\n    Candidate functions:\n    *(TINYINT, TINYINT) -&gt; TINYINT\n    *(SMALLINT, SMALLINT) -&gt; SMALLINT\n    *(INTEGER, INTEGER) -&gt; INTEGER\n    *(BIGINT, BIGINT) -&gt; BIGINT\n    *(HUGEINT, HUGEINT) -&gt; HUGEINT\n    *(FLOAT, FLOAT) -&gt; FLOAT\n    *(DOUBLE, DOUBLE) -&gt; DOUBLE\n    *(DECIMAL, DECIMAL) -&gt; DECIMAL\n    *(UTINYINT, UTINYINT) -&gt; UTINYINT\n    *(USMALLINT, USMALLINT) -&gt; USMALLINT\n    *(UINTEGER, UINTEGER) -&gt; UINTEGER\n    *(UBIGINT, UBIGINT) -&gt; UBIGINT\n    *(UHUGEINT, UHUGEINT) -&gt; UHUGEINT\n    *(INTERVAL, BIGINT) -&gt; INTERVAL\n    *(BIGINT, INTERVAL) -&gt; INTERVAL\n\nLINE 1: SELECT DEP, SUM(NB_EQUIP * (TYPEQU = 'B203')) AS nb_boulangeries\n                                 ^\n\n\nAvec duckdb, il faut transformer explicitement un booléen en nombre (entier ou flottant).\n\nbpe_ens_2018_dataset %&gt;%\n  summarise(nb_boulangeries  = sum(NB_EQUIP * as.integer(TYPEQU == \"B203\")), .by = DEP)\n\n# Source:   SQL [?? x 2]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n   DEP   nb_boulangeries\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 13               1626\n 2 66                463\n 3 15                143\n 4 45                396\n 5 80                330\n 6 976               101\n 7 48                 88\n 8 63                525\n 9 92                835\n10 22                460\n# ℹ more rows",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html#notions-avancées-bien-utiliser-duckdb",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html#notions-avancées-bien-utiliser-duckdb",
    "title": "21  Manipuler des données avec duckdb",
    "section": "\n21.5 Notions avancées / bien utiliser duckdb\n",
    "text": "21.5 Notions avancées / bien utiliser duckdb\n\n\n21.5.1 Configurer duckdb\n\nduckdb propose de nombreux paramètres mais nous n’allons voir que les principaux. Vous pouvez vous reporter à la documentation officielle pour en apprendre davantage sur la configuration de duckdb.\n\n21.5.1.1 Configuration lors de l’initialisation\nPour configurer duckdb lors de l’initialisation de la base de données (c’est-à-dire au moment où on utilise DBI::dbConnect(drv = duckdb::duckdb())), on utilise les arguments du driver duckdb.\n\n# Configurer le driver duckdb\ndrv &lt;- duckdb::duckdb(\n  dbdir = \"fichier.db\", \n  config = list(\n    threads = \"4\",\n    memory_limit = \"40GB\",\n    temp_directory = \"tmp_path/\",\n    preserve_insertion_order = \"true\")\n)\n\n# Initaliser la base de données duckdb avec la configuration\nconn_ddb &lt;- DBI::dbConnect(drv = drv)\n\nVoici une description des principaux paramètres de configuration:\n\n\ndbdir : utiliser une base de données persistante. Par défaut, duckdb crée une base de données dans la mémoire vive, qui est automatiquement détruite lorsque vous fermez la session R ou la connexion duckdb. Si vous mettez un chemin dans le paramètre dbdir, duckdb créera une base de données sur disque que vous pourrez réouvrir à votre prochaine session.\n\nSi vous utilisez principalement dplyr, les bases de données en mémoire sont certainement suffisantes. En revanche, ce paramètre peut éventuellement vous être utile si vous utilisez du SQL, si vous créez des vues ou si vous utilisez dplyr::compute.\n\nthreads : limiter le nombre de threads utilisés par duckdb. Pour simplifier, un thread est un processeur ou un morceau de processeur (l’unité électronique qui réalise les calculs). Par défaut, duckdb utilise tous les processeurs disponibles, ce qui n’est pas forcément souhaitable pour plusieurs raisons :\nsur un serveur partagé, vos collègues seront gênés ;\nil est conseillé de disposer de 5 à 10Go de mémoire par thread (5 pour des aggrégations, 10 pour des jointures) donc beaucoup de threads implique beaucoup de mémoire ;\navoir trop de threads peut être contre-productif.\n\nIl n’existe pas de règle générale pour définir le nombre de threads, mais utiliser 4 à 8 threads (en respectant le ratio threads/mémoire ci-dessus) constitue un point de départ raisonnable. Au delà, les performances augmentent généralement peu pour une consommation mémoire plus importante.\n\nmemory_limit : limiter la mémoire vive utilisée par duckdb. Par défaut, duckdb limite la mémoire à 80% de la mémoire disponible sur le serveur. Si vous avez une quantité limitée de mémoire, essayez plutôt de limiter le nombre de threads en respectant la règle de 5 à 10 Go par thread.\ntemp_directory : définir le dossier sur disque dans lequel duckdb peut écrire des fichiers temporaires. Un avantage de duckdb est qu’il sait “déborder” sur disque pour une grande partie de ces opérations. Cela signifie que duckdb va écrire dans des fichiers temporaires sur le disque les données qu’il ne peut conserver en mémoire car il a atteint la limite de mémoire fixée. Le paramètre temp_directory permet de choisir dans quel dossier ces fichiers temporaires seront écrits. Toutefois, il est généralement beaucoup plus efficace de diminuer le nombre de threads que de déborder sur disque mais dans le cas où vous avez besoin de “juste un peu plus” de mémoire cela peut se révéler utile. A noter que ce paramètre est automatiquement fixé si vous avez décidé d’utiliser une base persistante.\npreserve_insertion_order : préserver l’ordre de lecture/écriture ou non. duckdb peut consommer beaucoup de mémoire pour conserver l’ordre de lecture et d’écriture. Ce dernier paramètre permet d’autoriser duckdb à ne pas préserver l’ordre des données à la lecture et à l’écriture des fichiers dans le cas où il n’y a pas de clause ORDER BY / arrange.\n\n21.5.1.2 Fixer les paramètres après l’initialisation\nVous pouvez également changer les paramètres d’une base après son initialisation en utilisant la commande dbExecute. Par exemple, pour fixer le nombre de threads à 4 :\n\ndbExecute(conn_ddb, \"SET threads = '4';\")\n\n\n21.5.2 L’évaluation différée avec duckdb (lazy evaluation)\n\n\n\n\n\n\nTip\n\n\n\nIl est vivement conseillé de lire la fiche Manipuler des données avec arrow avant de lire cette section, en particulier la partie sur l’évaluation différée.\n\n\nQuand on manipule des objets duckdb, on construit des requêtes SQL. Le package duckdb se contente de traduire le code dplyr en SQL sans l’exécuter (de la même façon que le package arrow traduit du code dplyr en instructions C++). On rappelle qu’il faut utiliser show_query() pour visualiser la requête. La fonction print() permet de pré-visualiser le résultat.\n\n# Étape 1: compter les équipements\nreq_dep &lt;- \n  bpe_ens_2018_dataset |&gt;\n  group_by(DEP) |&gt; \n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) \nreq_dep |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT DEP, SUM(NB_EQUIP) AS NB_EQUIP_TOT\nFROM \"bpe_ens_2018_dataset/*.parquet\"\nGROUP BY DEP\n\n# Étape 2: filtrer sur le département\nreq_dep_filter &lt;- req_dep |&gt; \n  filter(DEP == \"59\") \nreq_dep_filter |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT DEP, SUM(NB_EQUIP) AS NB_EQUIP_TOT\nFROM \"bpe_ens_2018_dataset/*.parquet\"\nGROUP BY DEP\nHAVING (DEP = '59')\n\n\nLa fonction collect() envoie à duckdb l’instruction d’exécuter le calcul, et transmet les résultats à R. Un point essentiel est qu’avant l’instruction collect(), c’est le moteur SQL de duckdb qui fait les calculs, tandis qu’après l’instruction collect(), c’est le moteur de R qui fait les calculs car on manipule un objet R (tibble) standard. Par conséquent, il faut faire le plus de calculs possibles avant collect() pour bénéficier de la rapidité du moteur SQL !\n\nreq_dep_filter |&gt; collect()\n\n# A tibble: 1 × 2\n  DEP   NB_EQUIP_TOT\n  &lt;chr&gt;        &lt;dbl&gt;\n1 59           76125\n\n\nOn pourrait penser que, lorsqu’on exécute l’ensemble de ce traitement, duckdb se contente d’exécuter les instructions les unes après les autres: compter les équipements par département, puis conserver uniquement le département 59. Mais en réalité duckdb fait beaucoup mieux que cela: duckdb analyse la requête avant de l’exécuter, et optimise le traitement pour minimiser le travail. Dans le cas présent, duckdb repère que la requête ne porte en fait que sur le département 59, et commence donc par filtrer les données sur le département avant de compter les équipements, de façon à ne conserver que le minimum de données nécessaires et à ne réaliser que le minimum de calculs. Ce type d’optimisation s’avère très utile quand les données à traiter sont très volumineuses.\n\n\nSituation à éviter\nLa première étape de traitement est déclenchée par collect(), la table intermédiaire res_etape1 est donc un tibble. C’est le moteur d’exécution de dplyr qui est utilisé pour manipuler res_etape1 lors de la seconde étape, ce qui dégrade fortement les performances sur données volumineuses.\n\n# Etape 1\nres_etape1 &lt;- \n  bpe_ens_2018_dataset |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) |&gt;\n  collect()\n\n# Etape 2\nres_final &lt;- res_etape1 |&gt; \n  filter(DEP == \"59\") |&gt; \n  collect()\n\n# Sauvegarder les résultats\narrow::write_parquet(res_final, \"resultats.parquet\")\n\n\n\n\nUsage recommandé\nLa première étape construit une requête SQL, sans effectuer de calcul. La deuxième étape complète la requête sans effectuer de calcul. Ici, pas de fonction print(), donc pas de calcul partiel. Le calcul n’est exécuté qu’au moment de la sauvegarde des résultats par DuckDB, ce qui assure de bonnes performances notamment sur données volumineuses. Les données ne sont chargées dans la mémoire de R à aucun moment.\n\n# Etape 1\nres_etape1 &lt;- bpe_ens_2018_dataset |&gt;\n  group_by(DEP) |&gt;\n  summarise(\n    NB_EQUIP_TOT = sum(NB_EQUIP)\n  ) \n\n# Etape 2\nres_final &lt;- res_etape1 |&gt; \n  filter(DEP == \"59\") \n\n# Sauvegarder les résultats\nres_final |&gt; arrow::to_arrow() |&gt; \n  arrow::write_parquet(\"resultats.parquet\")\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous ne savez plus si une table de données est une requête SQL ou un tibble, il suffit d’exécuter print(votre_table) ou class(votre_table).\n\n\n\n21.5.3 Fonctions non traduites et/ou comment passer des paramètres ?\nIl peut arriver que le package duckdb ne parvienne pas à traduire votre code dplyr en SQL, par exemple lorsque vous voulez utiliser une fonction R dont duckdb ne connaît pas la traduction SQL, ou lorsque vous voulez passer un paramètre à une fonction. Pour surmonter ce problème (heureusement peu fréquent), il faut mettre les mains dans le mécanisme de traduction vers SQL. Il y a deux points importants:\n\nLorsque duckdb ne connaît pas la traduction SQL d’une fonction R est que la fonction inconnue est reprise directement dans le code SQL sans aucune modification. Voici un exemple, dans lequel on peut voir que la fonction fonction_inexistante() apparaît telle quelle dans le code SQL.\n\n\nreq &lt;- bpe_ens_2018_dataset |&gt; \n  mutate(test = fonction_inexistante(DEP)) |&gt; \n  show_query()\n\n&lt;SQL&gt;\nSELECT \"bpe_ens_2018_dataset/*.parquet\".*, fonction_inexistante(DEP) AS test\nFROM \"bpe_ens_2018_dataset/*.parquet\"\n\n\n\n\nDuckDB contient un grand nombre de fonctions optimisées (documentation ici), et il est possible de les utiliser directement dans du code R.\n\nCes deux points ensemble permettent de surmonter dans la plupart des cas le problème des fonctions R inconnues de duckdb: il suffit d’appeler la fonction de DuckDB qui fait la même chose. Voici un exemple qui explique cela en détail dans le cas de la fonction R as.Date(). On commence par créer une petite table duckdb contenant des dates sous forme de chaînes de caractères avec le format “DD/MM/YYYY”.\n\n# Créer des dates sous forme de chaînes de caractères\ndates &lt;- tibble(\n  date_naissance = c(\"02/07/1980\", \"29/02/2004\"),\n  date_deces = c(\"05/06/2001\", \"12/07/2023\")\n)\n\n# Créer une connexion entre ces données et la base de données duckdb\nconn_ddb %&gt;% duckdb::duckdb_register(name = \"dates_duckdb\", df = dates, overwrite = TRUE)\n\nLe package duckdb dispose d’une traduction SQL de la fonction as.Date(), mais cette traduction a deux limites: elle n’accepte que les données en format “YYYY-MM-DD”, et ne supporte pas l’argument format qui permet de préciser que les données sont en format “DD/MM/YYYY”. Par conséquent, on rencontre une erreur si on essaie d’utiliser la fonction as.Date() avec l’argument format (car duckdb ne sait pas gérer cet argument), et on rencontre une erreur si on essaie d’utiliser la fonction as.Date() sans cet argument (car les données n’ont pas le bon format).\n\nconn_ddb %&gt;% tbl(\"dates_duckdb\") %&gt;% \n  mutate(date_naissance = as.Date(date_naissance, format = \"%d/%m/%Y\")) # erreur\n\nError in as.Date(date_naissance, format = \"%d/%m/%Y\"): unused argument (format = \"%d/%m/%Y\")\n\n\n\nconn_ddb %&gt;% tbl(\"dates_duckdb\") %&gt;% \n  mutate(date_naissance = as.Date(date_naissance)) # erreur\n\nError in `collect()`:\n! Failed to collect lazy table.\nCaused by error:\n! rapi_execute: Failed to run query\nError: Conversion Error: date field value out of range: \"02/07/1980\", expected format is (YYYY-MM-DD)\nLINE 1: SELECT CAST(date_naissance AS DATE) AS date_na...\n               ^\n\n\nOn pourrait penser que ce problème est sérieux. En fait, la solution est très simple: il suffit d’utiliser la fonction strptime du moteur SQL DuckDB en indiquant le paramètre adéquat. Comme vous pouvez voir dans l’exemple suivant, on appelle cette fonction directement dans le code R. Par ailleurs, cette façon d’utiliser les fonctions de DuckDB dans du code R permet de passer facilement un paramètre à une fonction (le format “%d/%m/%Y” dans le cas présent).\n\nconn_ddb %&gt;% tbl(\"dates_duckdb\") %&gt;% \n  mutate(date_naissance = strptime(date_naissance, \"%d/%m/%Y\"))\n\n# Source:   SQL [2 x 2]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n  date_naissance      date_deces\n  &lt;dttm&gt;              &lt;chr&gt;     \n1 1980-07-02 00:00:00 05/06/2001\n2 2004-02-29 00:00:00 12/07/2023\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa logique présentée ici fonctionne également dans un cas plus avancé: l’utilisation d’une fonction sur plusieurs variables avec mutate_at. L’exemple ci-dessous reprend l’exemple ci-dessus avec deux variables.\n\nliste_variables &lt;- c(\"date_naissance\",\"date_deces\")\nconn_ddb %&gt;% tbl(\"dates_duckdb\") %&gt;% \n  mutate_at(liste_variables, ~ strptime(.,\"%d/%m/%Y\"))\n\n# Source:   SQL [2 x 2]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n  date_naissance      date_deces         \n  &lt;dttm&gt;              &lt;dttm&gt;             \n1 1980-07-02 00:00:00 2001-06-05 00:00:00\n2 2004-02-29 00:00:00 2023-07-12 00:00:00\n\n\n\n\n\n21.5.4 Manipulation des données avec SQL\nDuckDB étant un moteur SQL à part entière, on peut interagir avec DuckDB directement avec des requêtes SQL. Par exemple, en reprenant une table enregistrée plus haut avec la fonction duckdb::duckdb_register :\n\nDBI::dbGetQuery(conn_ddb, \"SELECT * FROM bpe_ens_2018_duckdb\") |&gt; head()\n\n  REG DEP DEPCOM DCIRIS   AN TYPEQU NB_EQUIP\n1  84  01  01001  01001 2018   A401        2\n2  84  01  01001  01001 2018   A404        4\n3  84  01  01001  01001 2018   A504        1\n4  84  01  01001  01001 2018   A507        1\n5  84  01  01001  01001 2018   B203        1\n6  84  01  01001  01001 2018   C104        1\n\n\nVous pouvez créer des vues ou des tables explicitement. La fonction dbExecute() retourne le nombre de lignes modifiées, tandis que la fonction dbGetQuery retourne le résultat sous la forme d’un tibble. Si vous n’avez pas l’intention de conserver durablement une table intermédiaire, il est préférable de créer une vue (qui ne consomme pas de mémoire) plutôt qu’une table (qui consomme de la mémoire). On peut d’ailleurs noter que les fonctions read_parquet() en SQL et duckdb_register du package utilisent CREATE VIEW implicitement.\n\n# Créer une table dans la base de données DuckDB\nDBI::dbExecute(conn_ddb, \"\n               CREATE TABLE bpe_ens_2018_table AS \n               SELECT REG, SUM(NB_EQUIP) AS NB_EQUIP_TOT \n               FROM bpe_ens_2018_duckdb \n               GROUP BY REG\") # Utilise de la mémoire\n\n[1] 18\n\n# Créer une vue dans la base de données DuckDB\nDBI::dbExecute(conn_ddb, \"\n               CREATE VIEW bpe_ens_2018_view AS \n               SELECT REG, SUM(NB_EQUIP) AS NB_EQUIP_TOT \n               FROM bpe_ens_2018_duckdb \n               GROUP BY REG\")   # n'utilise pas de mémoire\n\n[1] 0\n\n\nVous pouvez ensuite requêter les objets créés dans la base SQL via dplyr:\n\nconn_ddb %&gt;% tbl(\"bpe_ens_2018_view\")\n\n# Source:   table&lt;bpe_ens_2018_view&gt; [?? x 2]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n   REG   NB_EQUIP_TOT\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 84          303775\n 2 32          175859\n 3 28          107186\n 4 53          111291\n 5 06            7353\n 6 76          256813\n 7 01           23939\n 8 75          237008\n 9 94           21778\n10 52          119689\n# ℹ more rows\n\n\nVous pouvez bien sûr lire des fichiers Parquet, CSV ou autres en utilisant les fonctions de duckdb :\n\nDBI::dbGetQuery(conn_ddb, \"SELECT * FROM read_parquet('bpe_ens_2018_dataset/**/*.parquet') LIMIT 5\")\n\n  REG DEP DEPCOM DCIRIS   AN TYPEQU NB_EQUIP\n1  84  01  01001  01001 2018   A401        2\n2  84  01  01001  01001 2018   A404        4\n3  84  01  01001  01001 2018   A504        1\n4  84  01  01001  01001 2018   A507        1\n5  84  01  01001  01001 2018   B203        1\n\n\n\n\n\n\n\n\nTip\n\n\n\nLe SQL de duckdb est très proche de celui de PostgreSQL avec quelques évolutions très pertinentes.\n\n\n\n21.5.4.1 Séparer vos traitements SQL en blocs\nSi vos requêtes deviennent trop complexes et/ou longues, vous pouvez facilement les découper en créant des vues intermédiaires que vous réutiliserez plus tard :\n\n# Créer une vue qui correspond à la première étape du traitement\ndbExecute(conn_ddb, \"CREATE OR REPLACE VIEW data1_nettoye AS SELECT ... FROM read_parquet('data1.parquet')\")\n\n# Créer une vue qui correspond à la deuxième étape du traitement\ndbExecute(conn_ddb, \"CREATE OR REPLACE VIEW data2_nettoye AS SELECT ... FROM read_parquet('data2.parquet')\")\n\n# Faire la dernière étape du traitement et récupérer les résultats dans un tibble\nresultats &lt;- dbGetQuery(conn_ddb, \"SELECT * FROM data1_nettoye LEFT JOIN data2_nettoye ON data1.id = data2.id\")\n\nEt vous pouvez bien sûr créer des tables intermédiaires (temporaires ou non) à la place des vues (en utilisant CREATE TABLE pluôt que CREATE VIEW) pour éviter de les recalculer à chaque fois.\n\n21.5.4.2 Écrire des fichiers\nVous pouvez exporter des données vers des fichiers en utilisant COPY ... TO ... :\n\ndbExecute(conn_ddb, \"COPY (SELECT * FROM read_parquet('bpe_ens_2018_dataset/**/*.parquet')) \n                      TO 'mon_dataset_parquet' (FORMAT PARQUET, PARTITION_BY (REG), OVERWRITE_OR_IGNORE 1)\")\n\n[1] 1035564\n\n\nSi vous préférez utiliser les fonctions de arrow, vous pouvez créez une vue et utiliser dbplr::tbl avec arrow::write_dataset :\n\ndbExecute(conn_ddb, \"CREATE OR REPLACE VIEW output AS SELECT ...\")\n\ntbl(conn_ddb, \"output\") |&gt;\n  arrow::to_arrow() |&gt;\n  write_dataset(\"mon_dataset\")\n\n\n21.5.5 Optimisations\nLes opérations difficiles en SQL, longues, nécessitant beaucoup de mémoire, sont les fonctions dites “fenêtre”: jointures, GROUP BY avec beaucoup de petits groupes, dédoublonnage, etc. On propose ici quelques techniques pour faire passer ces calculs difficiles.\n\n21.5.5.1 Utilisation de la mémoire vive\nComme expliqué plus haut, les objets manipulés dans cette fiche sont des requêtes SQL, et ne nécessitent pas de mémoire vive. Les données déclarées par read_parquet sont stockées sur le disque dur, lues à la demande, et “oubliées” à la fin du calcul. On retourne le résultat du calcul.\nPour les opérations compliquées, il peut être nécessaire de charger les données en mémoire pour effectuer le calcul, au risque de saturer la mémoire. Lorsque ce problème se pose, duckdb renvoie un message du type:\nError: rapi_execute: Failed to run query\nError: Out of Memory Error: could not allocate block of size 262KB (99.7MB/100.0MB used)\nDatabase is launched in in-memory mode and no temporary directory is specified.\nUnused blocks cannot be offloaded to disk.\n\nLaunch the database with a persistent storage back-end\nOr set PRAGMA temp_directory='/path/to/tmp.tmp'\nPour contourner le manque de mémoire vive, on propose les quatre techniques suivantes :\n\ndiminuer le nombre de threads utilisés par duckdb, donc moins de besoins de mémoire (mais aussi moins de parallélisme):\n\n\nconn_ddb &lt;- dbConnect(duckdb(),\nconfig=list(\"threads\"=\"1\")))\n\nou\n\ndbExecute(conn_ddb, \"SET threads = '1';\")\n\n\nexécuter et sauvegarder les résultats au fur et à mesure. La commande arrow::write_dataset et la commande SQL COPY request TO filename.parquet savent le faire automatiquement, sans faire déborder la mémoire, pour certains calculs.\ndécouper le calcul et sauvegarder une base intermédiaire (cf ci-dessous).\nadosser un fichier sur le disque dur à la base de données en mémoire au moment de la création de la connexion. Cela ralentit considérablement les calculs, et ne permet pas toujours d’obtenir un résultat.\n\n\nconn_ddb &lt;- dbConnect(duckdb(), dbdir = \"my-db.duckdb\")\n\nL’interaction entre les différentes options de duckdb est complexe et rendent difficile l’élaboration de recommandations claires. Nous mettrons à jour cette fiche quand des benchmarks plus poussés seront disponibles.\n\n21.5.5.2 Sauvegarder des résultats intermédiaires\nDans plusieurs cas, vous pouvez vouloir passer par des résultats intermédiaires :\n\nVotre traitement est long et vous ne souhaitez pas le recalculer entièrement à chaque fois ;\nCertaines requêtes sont trop compliquées pour le moteur SQL et/ou pour la traduction automatique, vous devez le découper.\n\nVous avez plusieurs méthodes possibles :\n\n\narrow::write_dataset() sait faire les calculs par morceaux automatiquement, et libère la mémoire au fur et à mesure.\n\n\nconn_ddb %&gt;% calcul1() %&gt;%\n  arrow::to_arrow() %&gt;%\n  arrow::write_dataset(\"base_intermediaire\")\n\narrow::open_dataset(\"base_intermediaire\") %&gt;%\n  arrow::to_duckdb(conn_ddb) %&gt;%\n  calcul2()\n\n\nVous pouvez utiliser dbplyr::compute() pour créer une table duckdb stockée sur le disque (si vous avez préalablement créé une base sur disque) que vous pourrez directement utiliser par la suite dans une autre session :\n\n\nconn_ddb %&gt;%\n  calcul1() %&gt;%\n  compute(name = \"matable\", temporary = FALSE)\n\ntbl(conn_dbb, \"matable\") %&gt;%\n  calcul2()\n\nLa première méthode avec arrow est généralement la plus rapide et la seconde avec dbplyr::compute sur une table nommée est la plus efficace (de loin) en terme d’occupation mémoire.\nA noter que vous pouvez également utiliser dbplyr::compute pour créer une table temporaire duckdb stockée en mémoire qui disparaitra à la fin de votre session :\n\ntable_temporaire &lt;- conn_ddb %&gt;%\n  calcul1() %&gt;%\n  compute()\n\ntable_temporaire %&gt;%\n  calcul2()\n\n\n21.5.5.3 Partitionner les données\n\nPour exécuter une fonction fenêtre, il faut pouvoir localiser les données en mémoire avec un index.\nLes fichiers parquet ont un index min-max : les fichiers sont structurés en blocs, et on indique le minimum et maximum des valeurs du bloc dans les métadonnées. Ceci permet de sauter la lecture d’un bloc si l’on s’intéresse à des valeurs en dehors de la plage min-max, parce que l’on filtre les données par exemple.\nEn SQL, on peut créer un index, mais il faut que les données soient en mémoire, ce qui peut s’avérer être incompatible avec de très grosses volumétries.\nPar contre, on peut partitionner les données, et le moteur SQL sait utiliser le partitionnement comme un index.\n\n\nbpe_ens_2018_dataset %&gt;% \n  arrow::to_arrow() %&gt;% \n  arrow::write_dataset(\"bpe_ens_2018_dataset_parts\", partitioning = \"REG\" )\nlist.files(\"bpe_ens_2018_dataset_parts\") # on obtient un sous-répertoire par région\n\n [1] \"REG=01\" \"REG=02\" \"REG=03\" \"REG=04\" \"REG=06\" \"REG=11\" \"REG=24\" \"REG=27\"\n [9] \"REG=28\" \"REG=32\" \"REG=44\" \"REG=52\" \"REG=53\" \"REG=75\" \"REG=76\" \"REG=84\"\n[17] \"REG=93\" \"REG=94\"\n\n\n\n21.5.5.4 Exécuter les traitements par groupe explicitement\n\nS’il faut absolument charger des données en mémoire, on peut découper le calcul pour ne charger qu’une partie des données. Par exemple, faire une jointure région par région au lieu de faire la jointure sur toute la base d’un coup. On peut utiliser le partitionnement pour sauvegarder les résultats partiels, et les ré-assembler ensuite.\n\ngroups &lt;- bpe_ens_2018_dataset %&gt;% \n  distinct(REG) %&gt;%\n  pull() # liste des modalités de la variable REG\n\n# Définir une fonction qui traite un morceau des données\n# Puis exporte le résultats dans un Parquet partitionné\nf &lt;- function(x) {\n  bpe_ens_2018_dataset %&gt;% \n    filter(REG == x) %&gt;% \n    calcul_long() %&gt;% \n    arrow::to_arrow() %&gt;% \n    arrow::write_dataset(\"resultat\", partitioning = \"REG\")\n}\n\n# Appliquer la fonction à chaque groupe\npurrr::walk(f, groups)",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html#sec-arrow",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html#sec-arrow",
    "title": "21  Manipuler des données avec duckdb",
    "section": "\n21.6 Comparaison avec arrow\n",
    "text": "21.6 Comparaison avec arrow\n\narrow et duckdb partagent de nombreux concepts. Voici quelques différences :\n\n\nduckdb comprend parfaitement SQL. Si vous savez utiliser PROC SQL avec SAS, vous ne serez pas dépaysés.\nLe projet duckdb est très récent. Il y a régulièrement des évolutions qui sont souvent des extensions ou des optimisations, et parfois la résolution de bugs. arrow est un projet plus ancien et plus mature.\nCertaines fonctions standards de R ne sont pas traduites, mais la situation est meilleure du côté de duckdb que d’arrow. Hormis write_dataset(), la plupart des traitements peuvent être effectués en utilisant uniquement duckdb, sans passer par arrow.\nLes conversions de type: duckdb est plus permissif que arrow et fera plus facilement des conversions automatiques sans danger.\nLes jointures de tables volumineuses: arrow ne parvient pas à joindre des tables de données très volumineuses; il est préférable d’utiliser duckdb pour ce type d’opération.\nLes réorganisations de données : les fonctions pivot_wider et pivot_longer existent nativement dans duckdb mais pas dans arrow.\nLes fonctions fenêtre (window functions): arrow ne permet pas d’ajouter directement à une table des informations issues d’une agrégation par groupe de la même table. Par exemple, arrow ne peut pas ajouter directement à la base permanente des équipements une colonne égale au nombre total d’équipements du département. Le code fonctionne en duckdb.\n\n\n# arrow ne peut pas exécuter ceci\nbpe_ens_2018_dataset |&gt;\n  group_by(DEP) |&gt;\n  mutate(NB_EQUIP_TOTAL_DEP  = sum(NB_EQUIP)) |&gt;\n  select(DEP, NB_EQUIP, NB_EQUIP_TOTAL_DEP)\n\n# Source:   SQL [?? x 3]\n# Database: DuckDB v1.1.0 [unknown@Linux 6.8.0-1021-azure:R 4.4.1/:memory:]\n# Groups:   DEP\n   DEP   NB_EQUIP NB_EQUIP_TOTAL_DEP\n   &lt;chr&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n 1 08           2               9233\n 2 08           2               9233\n 3 08           2               9233\n 4 08           1               9233\n 5 08           2               9233\n 6 08           6               9233\n 7 08           1               9233\n 8 08           8               9233\n 9 08           1               9233\n10 08           1               9233\n# ℹ more rows\n\n\n\nles empilements de tables: il est facile d’empiler plusieurs tibbles avec dplyr grâce à la fonction bind_rows(): bind_rows(table1, table2, table3, table4). En revanche, il n’existe pas à ce jour de fonction équivalente dans arrow ou dans duckdb: il faut empiler les tables deux à deux avec les fonctions union_all() et union(). La différence entre arrow et duckdb est que duckdb est plus souple et acceptera d’empiler des tables qui ne sont pas exactement compatibles (exemple: pas le même nombre de colonnes), tandis qu’arrow exige que les deux tables soient parfaitement compatibles (il faut le même nombre de colonnes avec le même nom et le même type, ce qui n’est pas toujours le cas en pratique). Dans l’exemple suivant, on empile deux tables qui n’ont pas exactement le même nombre de colonnes:\n\n\n# Comment empiler de multiples tables\ntable_empilees  &lt;- bpe_ens_2018_dataset %&gt;% \n  union_all(bpe_ens_2018_dataset |&gt; select(-DEPCOM))",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_duckdb.html#Ressourcesduckdb",
    "href": "03_Fiches_thematiques/Fiche_duckdb.html#Ressourcesduckdb",
    "title": "21  Manipuler des données avec duckdb",
    "section": "\n21.7 Pour en savoir plus",
    "text": "21.7 Pour en savoir plus\n\nla documentation officielle du moteur DuckDB (en anglais) ;\nla documentation du package R DuckDB ;\nla documentation du package DBI décrit les mécanismes de traduction dplyr vers SQL utilisés dans toutes les bases de données interfacées avec R.",
    "crumbs": [
      "Introduction",
      "Choisir son paradigme d'analyse des données avec R",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Manipuler des données avec `duckdb`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_joindre_donnees.html",
    "href": "03_Fiches_thematiques/Fiche_joindre_donnees.html",
    "title": "22  Joindre des tables de données",
    "section": "",
    "text": "22.1 Tâches concernées et recommandations\nVous souhaitez apparier deux tables de données selon une ou plusieurs variables de jointure.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joindre des tables de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_joindre_donnees.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_joindre_donnees.html#tâches-concernées-et-recommandations",
    "title": "22  Joindre des tables de données",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nPour des tables de données de taille petite et moyenne (inférieure à 1 Go ou moins d’un million d’observations), il est recommandé d’utiliser le package dplyr ou R base. Le package dplyr est présenté en détail dans la fiche [Manipuler des données avec le tidyverse] ;\nPour des tables de données de grande taille (plus de 1 Go ou plus d’un million d’observations), il est recommandé d’utiliser le package data.table qui fait l’objet d’une autre fiche. Ce package est présenté en détail dans la fiche [Manipuler des données avec data.table].\nIl est vivement recommandé de lire la section Quelques bonnes pratiques sur les jointures avant de réaliser des jointures.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCertains exemples de cette fiche utilisent les données disponibles dans le package doremifasolData ; vous ne pourrez reproduire ces exemples que si ce package est installé sur la machine sur laquelle vous travaillez. Si vous ne savez pas si ce package est déjà installé, consultez la fiche Comment utiliser la documentation utilitR.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joindre des tables de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_joindre_donnees.html#les-différents-types-de-jointure",
    "href": "03_Fiches_thematiques/Fiche_joindre_donnees.html#les-différents-types-de-jointure",
    "title": "22  Joindre des tables de données",
    "section": "\n22.2 Les différents types de jointure",
    "text": "22.2 Les différents types de jointure\nUne jointure consiste à associer les observations de deux tables à l’aide d’un identifiant présent dans les deux tables, qui prend généralement la forme d’une ou plusieurs variables de jointure. Il existe plusieurs types de jointure. Les principales sont :\n\n\njointure interne (inner join) : la plus courante. Il s’agit de retourner les observations lorsque l’identifiant est présent dans les deux tables.\n\njointure à gauche (left join) : renvoie toutes les observations de la table de gauche, même si l’identifiant n’est pas présent dans la table de droite.\n\njointure à droite (right join) : l’inverse de la jointure à gauche ; toutes les observations de la table de droite.\n\njointure externe (full join) : retourne l’ensemble des observations des deux tables, même lorsque l’identifiant ne se retrouve que dans l’une des tables.\n\nanti-jointures : retourne les observations de la table de gauche seulement si l’identifiant n’est pas trouvé dans la table de droite (renvoyer les observations dont l’identifiant n’existe pas dans l’autre table). Ce type de jointure est peu utilisé en pratique.\n\nLe graphique ci-dessous illustre les différentes méthodes de jointure :\n\n\n\n\n\n\nType de jointure\nExemple\n\n\n\nJointure interne (inner join)\n\n\n\nJointure à gauche (left join)\n\n\n\nJointure à droite (right join)\n\n\n\nJointure externe (full join)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPour vous aider à comprendre le principe des jointures, voici une rapide description des opérations que R réalise quand on fait une jointure interne entre deux tables :\n\n\nR isole la première observation de la table de gauche et en extrait l’identifiant (la (ou les) variable(s) de jointure) ;\nil compare cet identifiant avec l’identifiant de chacune des observations de la table de droite :\n\nsoit l’identifiant est identique, auquel cas il rajoute dans la table de sortie une nouvelle observation avec les valeurs de l’observation extraite de la table de gauche et celles de l’observation extraite de la table de droite ;\nsoit l’identifiant est différent, auquel cas il passe à la suivante de la table de droite.\n\n\n\nR passe ensuite à la deuxième observation de la table de gauche, et ainsi de suite.\n\nIl est très important de comprendre qu’une ligne présente dans une des deux tables peut ainsi se retrouver dans la table finale en plusieurs exemplaires, ou pas du tout.\nDans le cas d’une jointure à gauche (left join), le fonctionnement est le même. La différence est que, pour chaque ligne de la table de gauche, si après avoir balayé toutes les lignes de la table de droite aucune correspondance n’a été trouvée, R ajoute quand même la ligne de gauche dans la table de sortie et met des valeurs manquantes dans les colonnes correspondant à la table de droite. On ne peut donc avoir moins de lignes en sortie que dans la table de gauche ; en revanche on peut en avoir plus si on a des doublons d’identifiant.\nLe cas d’une jointure à droite est tout à fait symétrique, et on déduit facilement le cas d’une jointure complète (full join).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joindre des tables de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_joindre_donnees.html#réaliser-une-jointure-avec-r",
    "href": "03_Fiches_thematiques/Fiche_joindre_donnees.html#réaliser-une-jointure-avec-r",
    "title": "22  Joindre des tables de données",
    "section": "\n22.3 Réaliser une jointure avec R\n",
    "text": "22.3 Réaliser une jointure avec R\n\nCette section présente les principales fonctions permettant de réaliser des jointures selon trois approches de manipulation de données : R base, dplyr et data.table. Les deux dernières approches font l’objet de présentations détaillées dans les fiches [Manipuler des données avec le tidyverse] et [Manipuler des données avec data.table].\n\n\n\n\n\n\nTip\n\n\n\nIl est possible de réaliser les jointures avec chacune des trois approches présentées ici. Il est néanmoins conseillé d’être cohérent avec les outils de manipulation de données que vous utilisez. Si vous manipulez vos données avec data.table, il sera préférable d’utiliser la fonction merge de ce package qui est optimisée pour les objets data.table. Si vous manipulez vos données avec le tidyverse, il est recommandé d’utiliser les fonctions du package dplyr.\n\n\nLes fonctions de jointure vont être illustrées avec la table des communes du code officiel géographique 2019 et les données du répertoire Filosofi 2016 agrégées par commune. Ces tables sont disponibles dans le package doremifasolData. Pour alléger les sorties, on conserve uniquement l’identifiant de commune, le revenu médian 2016 MED16 et le taux de pauvreté TP6016 dans la première table, et l’identifiant de commune com, le type de commune typecom, le nom officiel de la commune libelle et son département dep dans la seconde table.\n\nlibrary(doremifasolData)\nfilosofi_com_2016 &lt;- filosofi_com_2016[, c(\"CODGEO\", \"MED16\", \"TP6016\")]\ncog_com_2019 &lt;- cog_com_2019[, c(\"com\", \"typecom\", \"libelle\", \"dep\")]\n\nVoici un aperçu des données :\n\nhead(filosofi_com_2016)\n\n  CODGEO    MED16 TP6016\n1  01001 22679.00     NA\n2  01002 24382.08     NA\n3  01004 19721.00     17\n4  01005 23378.00     NA\n5  01006       NA     NA\n6  01007 22146.45     NA\n\n\n\nhead(cog_com_2019)\n\n    com typecom                 libelle dep\n1 01001     COM L'Abergement-Clémenciat  01\n2 01002     COM   L'Abergement-de-Varey  01\n3 01004     COM       Ambérieu-en-Bugey  01\n4 01005     COM     Ambérieux-en-Dombes  01\n5 01006     COM                 Ambléon  01\n6 01007     COM                Ambronay  01\n\n\n\n\n\n\n\n\nNote\n\n\n\nContrairement à d’autres logiciels statistiques, il n’est pas nécessaire ni même utile de trier les tables avant de les joindre avec R. Le faire n’apporte pas de gain de performance (une exception à cette règle est évoquée dans la fiche [Manipuler des données avec data.table]). Il n’est pas non plus nécessaire que les variables de jointure portent les mêmes noms dans les deux tables.\n\n\n\n22.3.1 Jointure avec R base\n\nLa fonction pour effectuer des jointures en base R est la fonction merge (fusionner). Elle prend les arguments suivants :\n\nle nom des deux data.frame à joindre ;\nles variables de jointure, définies par les arguments by.x et by.y ou par l’argument by ;\nle type de jointure défini par les arguments all, all.x et all.y.\n\nVoici quelques remarques sur l’usage de merge :\n\nLorsque les variables de jointure portent exactement le même nom dans les deux tables, on les définit par l’argument by. Sinon, il faut préciser by.x = nom_var_joint_table_gauche et by.y = nom_var_joint_table_droite ;\n\nPar défaut, merge réalise une jointure interne. On peut obtenir d’autres types de jointure avec les arguments all, all.x et all.y. Voici un aide mémoire :\n\n\n\n\n\n\nType de jointure\nArgument de la fonction merge\n\n\n\n\nJointure à gauche (left join)\nall.x = TRUE\n\n\nJointure à droite (right join)\nall.y = TRUE\n\n\nJointure externe (full join)\nall = TRUE\n\n\n\n\nSi deux variables portent le même nom dans les deux tables, alors dans la table jointe le nom de ces variables sera complété par un suffixe .x et .y pour pouvoir les différencier. Il est possible de modifier ces suffixes avec l’option suffixes.\nIl y a d’autres options plus avancées que vous pouvez consulter avec ?merge.\n\nVoici un exemple dans lequel on utilise merge pour réaliser une jointure à gauche entre la table des données Filosofi et la table des communes du COG.\n\ntable_jointe &lt;- base::merge(filosofi_com_2016,\n                            cog_com_2019,\n                            by.x = \"CODGEO\",\n                            by.y = \"com\",\n                            all.x = TRUE)\nhead(table_jointe)\n\n  CODGEO    MED16 TP6016 typecom                 libelle dep\n1  01001 22679.00     NA     COM L'Abergement-Clémenciat  01\n2  01002 24382.08     NA     COM   L'Abergement-de-Varey  01\n3  01004 19721.00     17     COM       Ambérieu-en-Bugey  01\n4  01005 23378.00     NA     COM     Ambérieux-en-Dombes  01\n5  01006       NA     NA     COM                 Ambléon  01\n6  01007 22146.45     NA     COM                Ambronay  01\n\n\n\n22.3.2 Jointure avec dplyr\n\nAvec dplyr, les jointures se réalisent grâce aux fonctions left_join, right_join, inner_join, full_join et anti_join. Ces fonctions prennent les arguments suivants :\n\nle nom des deux data.frame à joindre ;\nles variables de jointure, définies par l’argument by. Lorsque la variable de jointure ne porte pas le même nom dans les deux tables, on utilise le paramètre by = c(\"var_x\" = \"var_y\"). S’il y a plusieurs variables de jointure, on écrit by = c(\"var_x1\" = \"var_y1\", \"var_x2\" = \"var_y2\").\n\nIl est préférable d’utiliser ces fonctions sur des objets tibble plutôt que data.frame. On va donc convertir les deux tables avant de présenter un exemple :\n\nlibrary(dplyr)\nfilosofi_com_2016_tbl &lt;- as_tibble(filosofi_com_2016)\ncog_com_2019_tbl &lt;- as_tibble(cog_com_2019)\n\nVoici un exemple dans lequel on utilise la fonction left_join pour réaliser une jointure à gauche entre la table des données Filosofi et la table des communes du COG.\n\ntable_jointe_tbl &lt;- filosofi_com_2016_tbl %&gt;% \n                        left_join(y = cog_com_2019_tbl, \n                                  by = c(\"CODGEO\" = \"com\"))\nhead(table_jointe_tbl)\n\n# A tibble: 6 × 6\n  CODGEO  MED16 TP6016 typecom libelle                 dep  \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;                   &lt;chr&gt;\n1 01001  22679      NA COM     L'Abergement-Clémenciat 01   \n2 01002  24382.     NA COM     L'Abergement-de-Varey   01   \n3 01004  19721      17 COM     Ambérieu-en-Bugey       01   \n4 01005  23378      NA COM     Ambérieux-en-Dombes     01   \n5 01006     NA      NA COM     Ambléon                 01   \n6 01007  22146.     NA COM     Ambronay                01   \n\n\nLa syntaxe pour réaliser les autres types de jointure est très similaire :\n\n\n\n\n\n\nType de jointure\nSyntaxe dplyr\n\n\n\n\nJointure à gauche\nleft_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\nJointure à droite\nright_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\nJointure externe\nfull_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\nAnti-jointure\nanti_join(x = filosofi_com_2016_tbl, y = cog_com_2019_tbl, by = c(\"CODGEO\" = \"com\"))\n\n\n\n22.3.3 Jointure avec data.table\n\nIl existe deux façons de réaliser des jointures avec data.table :\n\n\nla fonction merge de data.table, dont le fonctionnement est identique à celui de la fonction merge de R base ; de façon générale, c’est cette approche qu’il faut privilégier pour faire des jointures avec data.table ;\n\nl’opérateur crochet []. Cette approche convient pour des utilisations avancées, lorsque l’approche merge ne peut pas être utilisée.\n\nIl est préférable d’utiliser ces fonctions sur des objets data.table plutôt que data.frame. On va donc convertir les deux tables avant de présenter des exemples.\n\nlibrary(data.table)\nfilosofi_com_2016_dt &lt;- as.data.table(filosofi_com_2016)\ncog_com_2019_dt &lt;- as.data.table(cog_com_2019)\n\n\n\n\n\n\n\nTip\n\n\n\nL’un des intérêts d’utiliser le package data.table est qu’il est très efficace pour effectuer des jointures car il est possible d’indexer les tables. L’indexation permet à R de retrouver rapidement les lignes qui correspondent à une valeur de la clé de jointure, accélérant les combinaisons de données.\nIl est possible de définir une (ou plusieurs) clé(s) grâce à la commande setkeyv. Pour plus de détails, se reporter à la fiche [Manipuler des données avec data.table].\n\n\n\n22.3.3.1 Joindre des objets data.table avec merge\n\nLa fonction merge de data.table a un fonctionnement identique à celui de la fonction merge de R base, mais elle est plus rapide lorsqu’on l’utilise pour joindre des objets data.table plutôt que des objets data.frame. Elle prend les arguments suivants :\n\nle nom des deux data.frame à joindre ;\nles variables de jointure, définies par les arguments by.x et by.y ou par l’argument by ;\nle type de jointure défini par les arguments all, all.x et all.y.\n\nVoici un exemple dans lequel on utilise la fonction merge de data.table pour réaliser une jointure à gauche entre la table des données Filosofi et la table des communes du COG.\n\ntable_jointe_dt &lt;- \n  merge(x = filosofi_com_2016_dt,\n        y = cog_com_2019_dt,\n        by.x = \"CODGEO\",\n        by.y = \"com\",\n        all.x = TRUE)\nhead(table_jointe_dt)\n\nKey: &lt;CODGEO&gt;\n   CODGEO    MED16 TP6016 typecom                 libelle    dep\n   &lt;char&gt;    &lt;num&gt;  &lt;num&gt;  &lt;char&gt;                  &lt;char&gt; &lt;char&gt;\n1:  01001 22679.00     NA     COM L'Abergement-Clémenciat     01\n2:  01002 24382.08     NA     COM   L'Abergement-de-Varey     01\n3:  01004 19721.00     17     COM       Ambérieu-en-Bugey     01\n4:  01005 23378.00     NA     COM     Ambérieux-en-Dombes     01\n5:  01006       NA     NA     COM                 Ambléon     01\n6:  01007 22146.45     NA     COM                Ambronay     01\n\n\nComme pour les jointures avec base R, la syntaxe pour réaliser les autres types de jointure est très similaire, il suffit de modifier l’option all, all.x ou all.y. Voici un aide mémoire :\n\n\n\n\n\n\nType de jointure\nArgument de la fonction merge\n\n\n\n\nJointure à gauche (left join)\nall.x = TRUE\n\n\nJointure à droite (right join)\nall.y = TRUE\n\n\nJointure externe (full join)\nall = TRUE\n\n\n\nIl est difficile de réaliser une anti-jointure avec la commande merge. Pour cela, la syntaxe ci-dessous, fondée sur les crochets, s’avère nécessaire.\n\n22.3.3.2 Inner join avec la syntaxe []\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous utilisez data.table, il est préférable de joindre des tables avec merge car cette approche permet d’écrire des codes lisibles et couvre la majorité des besoins. La syntaxe utilisant les crochets [] peut être utilisée dans les cas (peu fréquents) où l’utilisation de merge s’avère complexe, par exemple dans le cas de l’anti-jointure. Avant de lire ce paragraphe, il est recommandé de vous familiariser avec l’opérateur [] dans data.table. Pour cela, vous pouvez vous reporter à la fiche [Manipuler des données avec data.table].\n\n\nUne manière équivalente mais moins lisible que merge pour effectuer des appariements consiste à utiliser la syntaxe x[y, on = c(xvar1 = yvar1, ...)]. On peut voir x[y, on = c(xvar1 = yvar1)] comme une manière concise d’écrire x[x$var1 %in% y$yvar1, ] en R base. Les personnes habituées à la syntaxe SQL retrouveront une similarité avec la requête suivante :\nSELECT *\nFROM x\nINNER JOIN y ON x.var1 = y.var1\nPar exemple, pour faire une jointure interne, on pourrait écrire merge(x = filosofi_com_2016_dt, y = cog_com_2019_dt, by.x = \"CODGEO\", by.y = \"com\", all = FALSE). Avec la syntaxe alternative, on peut écrire :\n\nfilosofi_com_2016_dt[cog_com_2019_dt, on = c(\"CODGEO\" = \"com\"), nomatch=NULL]\n\n       CODGEO    MED16 TP6016 typecom                 libelle    dep\n       &lt;char&gt;    &lt;num&gt;  &lt;num&gt;  &lt;char&gt;                  &lt;char&gt; &lt;char&gt;\n    1:  01001 22679.00     NA     COM L'Abergement-Clémenciat     01\n    2:  01002 24382.08     NA     COM   L'Abergement-de-Varey     01\n    3:  01004 19721.00     17     COM       Ambérieu-en-Bugey     01\n    4:  01005 23378.00     NA     COM     Ambérieux-en-Dombes     01\n    5:  01006       NA     NA     COM                 Ambléon     01\n   ---                                                              \n35605:  97420 15110.67     38     COM          Sainte-Suzanne    974\n35606:  97421 11280.94     58     COM                 Salazie    974\n35607:  97422 14243.81     41     COM               Le Tampon    974\n35608:  97423 14031.79     42     COM       Les Trois-Bassins    974\n35609:  97424 12034.38     53     COM                  Cilaos    974\n\n\nL’argument nomatch sert à définir le comportement pour les valeurs des observations de la table de gauche (filosofi_com_2016_dt) n’ayant pas de clé correspondante dans la table de droite (cog_com_2019_dt). La valeur par défaut est NA, qui signifie que les valeurs de gauche sans correspondance auront des valeurs NA. Avec nomatch=NULL (ou nomatch=0), on retire les observations de x sans correspondance dans y : on effectue ainsi une jointure interne.\nPour effectuer une anti-jointure, il faut sélectionner les clés de x qui n’ont pas de contrepartie dans y, ce qui s’écrit de manière générale x[!y]. Dans notre exemple, cela donne :\n\nfilosofi_com_2016_dt[!cog_com_2019_dt, on = c(\"CODGEO\" = \"com\")]\n\nEmpty data.table (0 rows and 3 cols): CODGEO,MED16,TP6016\n\n\nLe tableau précédent peut être enrichi :\n\n\n\n\n\n\n\nType de jointure\nArgument(s) de la fonction merge\n\nApproche alternative\n\n\n\nJointure interne (inner join)\n\nall = FALSE (défaut)\nX[Y, nomatch=0]\n\n\nJointure à gauche (left join)\nall.x = TRUE\nY[X]\n\n\nJointure à droite (right join)\nall.y = TRUE\nX[Y]\n\n\nJointure externe (full join)\nall = TRUE\n-\n\n\nAnti-jointure (anti join)\nall = TRUE, subset = NA\nX[!Y]",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joindre des tables de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_joindre_donnees.html#quelques-bonnes-pratiques-sur-les-jointures",
    "href": "03_Fiches_thematiques/Fiche_joindre_donnees.html#quelques-bonnes-pratiques-sur-les-jointures",
    "title": "22  Joindre des tables de données",
    "section": "\n22.4 Quelques bonnes pratiques sur les jointures",
    "text": "22.4 Quelques bonnes pratiques sur les jointures\nDe manière générale, il est important de bien préparer une jointure. Voici une liste non exhaustive de bonnes pratiques qui peuvent permettre d’éviter des erreurs. Ces bonnes pratiques seront illustrées par la jointure entre la table communale du répertoire Filosofi 2016 et la table des communes du code officiel géographique 2019. Cette jointure présente des problèmes dus au fait que le référentiel communal a évolué entre les deux dates.\n\n22.4.1 Avant la jointure\nAvant de procéder à une jointure, il est essentiel de vérifier la qualité des identifiants dans les deux tables que l’on veut joindre.\n\n\nRègle n°1 : vérifier la présence de valeurs manquantes dans les variables de jointure.\nUne première approche consiste à rechercher les valeurs manquantes (NA) dans les variables de jointure. Le code suivant permet de calculer le nombre d’observations pour lesquelles l’identifiant est manquant :\n\nsum(is.na(filosofi_com_2016$CODGEO))\n\n[1] 0\n\nsum(is.na(cog_com_2019$com))\n\n[1] 0\n\n\nOn voit ici que les variables de jointure ne contiennent aucun NA dans les deux tables. Toutefois, les valeurs manquantes peuvent prendre des formes plus complexes que NA : 0, ., 999… c’est pourquoi il est important de procéder à une inspection visuelle des variables de jointure. Pour ce faire, vous pouvez utilisez la fonction unique(), qui permet d’afficher la liste des valeurs qui apparaissent dans une variable.\n\nunique(filosofi_com_2016$CODGEO)\n\n [1] \"01001\" \"01002\" \"01004\" \"01005\" \"01006\" \"01007\" \"01008\" \"01009\" \"01010\"\n[10] \"01011\" \"01012\" \"01013\" \"01014\" \"01015\" \"01016\" \"01017\" \"01019\" \"01021\"\n[19] \"01022\" \"01023\" \"01024\" \"01025\" \"01026\" \"01027\" \"01028\"\n [ reached getOption(\"max.print\") -- omitted 34907 entries ]\n\n\n\n\nRègle n°2 : vérifier la présence de doublons dans les variables de jointure.\nSi les variables de jointure contiennent un grand nombre de fois les mêmes valeurs, la jointure peut devenir très gourmande en ressources, voire irréalisable. Il est donc indispensable de repérer les doublons, et de les traiter si nécessaire. Les deux codes suivants calculent le nombre d’observations dans la table pour chaque valeur des variables de jointure, et affichent les premières lignes par nombre d’observations décroissant. Si la variable nb_obs est supérieure ou égale à 2, alors il y a des doublons. Le premier code est adapté si vous utilisez tidyverse pour manipuler des données, le second si vous utilisez data.table.\n\n# Approche dplyr/tidyverse\ndoublons &lt;- filosofi_com_2016_tbl %&gt;% \n              group_by(CODGEO) %&gt;% \n              summarise(nb_obs = n()) %&gt;% \n              filter(nb_obs &gt; 1) %&gt;% \n              arrange(-nb_obs)\ndoublons\n\n# A tibble: 0 × 2\n# ℹ 2 variables: CODGEO &lt;chr&gt;, nb_obs &lt;int&gt;\n\n\n\n# Approche data.table\ndoublons &lt;- filosofi_com_2016_dt[, .(nb_obs = .N), by = CODGEO\n                            ][nb_obs &gt; 1][order(-nb_obs)]\ndoublons\n\nOn voit ici qu’il n’y a aucun doublon sur les identifiants dans la table issue du répertoire Filosofi. En revanche, on constate qu’il y a un grand grand nombre de doublons sur les identifiants dans la table du code officiel géographique. Cette présence de doublons rend nécessaire une analyse de cette table avant de réaliser la jointure.\n\n# Approche dplyr/tidyverse\ndoublons &lt;- cog_com_2019_tbl %&gt;% \n              group_by(com) %&gt;% \n              summarise(nb_obs = n()) %&gt;% \n              filter(nb_obs &gt; 1) %&gt;% \n              arrange(-nb_obs)\ndoublons\n\n# A tibble: 677 × 2\n  com   nb_obs\n  &lt;chr&gt;  &lt;int&gt;\n1 01015      2\n2 01025      2\n3 01033      2\n4 01036      2\n5 01080      2\n6 01095      2\n# ℹ 671 more rows\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLe fait que les variables de jointure contiennent des valeurs manquantes ou des doublons n’est pas nécessairement un problème. C’est à vous de déterminer si cela pose un problème. Voici deux questions que vous pouvez vous poser pour analyser la situation :\n\nLa (ou les) variable(s) de jointure doi(ven)t-elle(s) impérativement être renseignée(s) pour chaque observation ? Si oui, il ne doit pas y avoir de valeurs manquantes.\nLa (ou les) variable(s) de jointure doi(ven)t-elle(s) identifier de façon unique chaque observation ? Si oui, il ne doit pas y avoir de doublons.\n\n\n\n\n\nRègle n°3 : vérifier la compatibilité des variables de jointure.\nIl faut vérifier deux choses :\n\n\nUne jointure ne peut être réalisée avec R que si les variables de jointure sont de même type. Il faut donc vérifier que c’est le cas. Les types de variables les plus fréquemment utilisées pour des jointures avec R sont integer (nombre entier), character (chaîne de caractères) et factor (catégorie). Vous pouvez utiliser la fonction class pour connaître le type d’une variable. Dans l’exemple suivant, on voit que les deux variables de jointure sont de type character.\n\n\n# Type de la variable de jointure dans la table Filosofi\nclass(filosofi_com_2016$CODGEO)\n\n[1] \"character\"\n\n# Type de la variable de jointure dans le COG 2019\nclass(cog_com_2019$com)\n\n[1] \"character\"\n\n\nSi les variables sont de type différent, alors il faut convertir les variables de jointure dans l’une des tables. Pour ce faire, vous pouvez utiliser les fonctions as.integer, as.character et as.factor. Si vous convertissez une variable de jointure, il faut refaire les vérifications sur la présence de valeurs manquantes et de doublons.\n\n\nUne jointure ne peut être réalisée que si les valeurs des variables de jointure de la table de gauche figurent (au moins pour certaines d’entre elles) parmi les valeurs des variables de jointure de la table de droite. Il faut donc le vérifier. Voici deux exemples de code qui peuvent vous aider.\n\nLe premier code affiche le nombre d’identifiants distincts dans chaque table, et le nombre d’identifiants communs aux deux tables. On voit que tous les identifiants de la table issue de Filosofi figurent dans le COG 2019, mais que l’inverse n’est pas vrai.\n\n# Nombre d'identifiants distincts dans la table Filosofi\nlength(unique(filosofi_com_2016$CODGEO))\n\n[1] 34932\n\n# Nombre d'identifiants distincts dans le COG 2019\nlength(unique(cog_com_2019$com))\n\n[1] 37253\n\n# Nombre d'identifiants communs\nsum(unique(filosofi_com_2016$CODGEO) %in% unique(cog_com_2019$com))\n\n[1] 34932\n\n\nLe second code donne la liste des identifiants de la table du COG 2019 qui ne figurent pas parmi les identifiants de la table issue de Filosofi.\n\nunique(cog_com_2019$com)[!(unique(cog_com_2019$com) %in% unique(filosofi_com_2016$CODGEO))]\n\n [1] \"01059\" \"01091\" \"01097\" \"01119\" \"01120\" \"01122\" \"01137\" \"01144\" \"01154\"\n[10] \"01172\" \"01176\" \"01182\" \"01186\" \"01205\" \"01218\" \"01221\" \"01271\" \"01292\"\n[19] \"01300\" \"01312\" \"01316\" \"01324\" \"01340\" \"01341\" \"01409\"\n [ reached getOption(\"max.print\") -- omitted 2296 entries ]\n\n\n\n\n22.4.2 Après la jointure\nAprès une jointure, il est essentiel de vérifier que la jointure a bien produit le résultat attendu.\n\n\nRègle n°4 : vérifier que le nombre d’observations de la table de sortie est cohérent.\nPar exemple, dans le cas d’une jointure à gauche (left join) et si les variables de la table de droite ne présentent aucun doublon, alors la table de sortie doit avoir le même nombre d’observations que la table de gauche. Dans l’exemple qui suit, on voit que la table issue de la jointure comprend 35609 observations, tandis que la table communale de Filosofi n’en comprenait que 34932. La jointure présente donc un problème (due aux doublons dans la table du COG 2019).\n\ndim(filosofi_com_2016_tbl)\n\n[1] 34932     3\n\ntable_jointe_tbl &lt;- left_join(x = filosofi_com_2016_tbl, \n                              y = cog_com_2019_tbl, \n                              by = c(\"CODGEO\" = \"com\"))\ndim(table_jointe_tbl)\n\n[1] 35609     6\n\n\n\n\nRègle n°5 : vérifier la présence éventuelle de valeurs manquantes (NA) dans les variables d’intérêt.\nVous pouvez par exemple utiliser la fonction is.na() qui permet de repérer les observations manquantes dans les variables provenant de la table de droite. S’il y a des valeurs manquantes, cela peut indiquer que cette variable contient des valeurs manquantes dans la table de droite, ou que certaines observations de la table de gauche n’ont pas de correspondances dans la table de droite. Dans l’exemple qui suit, on voit que le département est manquant pour plusieurs centaines d’observations de la table jointe.\n\ntable_jointe_tbl %&gt;% filter(is.na(dep))\n\n# A tibble: 677 × 6\n  CODGEO  MED16 TP6016 typecom libelle                  dep  \n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;                    &lt;chr&gt;\n1 01015  22357      NA COMD    Arbignieu                &lt;NA&gt; \n2 01025  21971.      6 COMD    Bâgé-la-Ville            &lt;NA&gt; \n3 01033  21405.     16 COMD    Bellegarde-sur-Valserine &lt;NA&gt; \n4 01036  21142.     NA COMD    Belmont-Luthézieu        &lt;NA&gt; \n5 01080  21599      NA COMD    Champdor                 &lt;NA&gt; \n6 01095  20242.     NA COMD    Chavannes-sur-Suran      &lt;NA&gt; \n# ℹ 671 more rows\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous faites des jointures avec dplyr, vous pouvez utiliser le package tidylog pour obtenir des informations détaillées sur les jointures réalisées. Cela aide dans la mise en œuvre des bonnes pratiques présentées plus haut. Il faut toutefois être prudent avec ce package si les tables sont volumineuses (plus d’un million d’observations). En effet, ce package réalise de manière sous-jacente plusieurs opérations de jointures, ce qui risque de saturer la mémoire de R.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joindre des tables de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_joindre_donnees.html#RessourcesJointures",
    "href": "03_Fiches_thematiques/Fiche_joindre_donnees.html#RessourcesJointures",
    "title": "22  Joindre des tables de données",
    "section": "\n22.5 Pour en savoir plus",
    "text": "22.5 Pour en savoir plus\n\nla fiche [Manipuler des données avec le tidyverse] ;\nla fiche [Manipuler des données avec data.table] ;\nun tutoriel sur les jointures en SQL (entièrement en français) ;\nun tutoriel de Lise Vaudor sur les jointures en dplyr ;\nla section consacrée à dplyr de la formation de Julien Barnier ;\nles formations du MTES ;\nce tutoriel sur les jointures avec data.table.\nla vignette du package tidylog",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Joindre des tables de données</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_textuelles.html",
    "href": "03_Fiches_thematiques/Fiche_donnees_textuelles.html",
    "title": "23  Manipuler des données textuelles",
    "section": "",
    "text": "23.1 Tâches concernées et recommandations\nL’utilisateur souhaite manipuler du texte (repérer et extraire une chaîne de caractères, concaténer, remplacer une chaîne par une autre, modifier la casse…).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Manipuler des données textuelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#tâches-concernées-et-recommandations",
    "title": "23  Manipuler des données textuelles",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nIl est recommandé d’utiliser le package stringr qui répond à la plupart des besoins courants ;\nPour les utilisateurs plus avancés, le package stringi propose un plus grand nombre de fonctionnalités ;\nL’usage du package rex est utile pour construire des expressions régulières complexes.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Manipuler des données textuelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#manipuler-des-chaînes-de-caractères-avec-stringr",
    "href": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#manipuler-des-chaînes-de-caractères-avec-stringr",
    "title": "23  Manipuler des données textuelles",
    "section": "\n23.2 Manipuler des chaînes de caractères avec stringr\n",
    "text": "23.2 Manipuler des chaînes de caractères avec stringr\n\n\n23.2.1 Présentation du package\n\nLes packages stringi et stringr facilitent beaucoup le travail sur les chaînes de caractères. stringi propose un grand nombre de fonctions avancées, et stringr propose un petit nombre de fonctions simples à utiliser (qui reposent sur les fonctions de stringi). Il est donc préférable de commencer à travailler avec stringr. Toutes les fonctions de stringi et stringr sont structurées de la même façon : le premier argument de la fonction est toujours une chaîne de caractères ; les arguments suivants sont des options.\nPour utiliser stringr, il faut charger le package :\n\nlibrary(stringr)\n\n\n23.2.2 Manipulations simples\n\n23.2.2.1 Convertir en majuscules / minuscules\nLes fonctions str_to_lower, str_to_upper et str_to_title permettent respectivement de mettre en minuscules, mettre en majuscules, ou de capitaliser les éléments d’un vecteur de chaînes de caractères :\n\nstr_to_lower(\"Hello world\")\n\n[1] \"hello world\"\n\n\n\nstr_to_upper(\"Hello world\")\n\n[1] \"HELLO WORLD\"\n\n\n\nstr_to_title(\"Hello world\")\n\n[1] \"Hello World\"\n\n\n\n23.2.2.2 Gérer les espaces\nLa fonction str_pad() permet de compléter une chaîne de caractères pour qu’elle atteigne une taille fixe. Le caractère utilisé en complément est défini dans l’argument pad (espace par défaut). L’option side permet de choisir de compléter à gauche (left) ou à droite (right). Le cas typique d’usage est la gestion des codes communes Insee. Voici deux exemples :\n\ncode_insee &lt;- 1001\nstr_pad(code_insee, 5, pad = \"0\", side = \"left\")\n\n[1] \"01001\"\n\nstr_pad(code_insee, 5, pad = \"Z\", side = \"right\")\n\n[1] \"1001Z\"\n\n\nLa fonction str_trim() permet de supprimer les espaces aux extrémités d’une chaîne de caractères. On peut choisir de nettoyer à gauche (left), à droite (right) ou des deux côtés (both) avec l’option side (des deux côtés par défaut).\n\nstring &lt;- \"   Les espaces inutiles doivent être supprimés.  \"\nstr_trim(string)\n\n[1] \"Les espaces inutiles doivent être supprimés.\"\n\nstr_trim(string, side = \"left\")\n\n[1] \"Les espaces inutiles doivent être supprimés.  \"\n\n\n\n23.2.2.3 Concaténer des chaînes de caractères\nLa fonction str_c permet de concaténer des chaînes de caractères entre elles, avec un délimiteur défini dans l’argument sep. Dans l’exemple suivant, on concatène les éléments de deux vecteurs deux à deux (le premier élément du premier vecteur avec le premier élément du second vecteur, etc.) :\n\nstr_c(c(\"Hello\", \"Bonjour\"), c(\"World\", \"Monde\"), sep = \" \")\n\n[1] \"Hello World\"   \"Bonjour Monde\"\n\n\nL’argument supplémentaire collapse permet de gérer la concaténation pour les éléments contenus dans un vecteur ou une liste (très utile en pratique, en particulier si on utilise la fonction str_split présentée ci-dessous). Ainsi, il est possible de concaténer l’ensemble des éléments d’un vecteur en une seule chaîne de caractère :\n\nstr_c(c(\"B\", \"o\", \"n\", \"j\", \"o\", \"u\", \"r\"))\n\n[1] \"B\" \"o\" \"n\" \"j\" \"o\" \"u\" \"r\"\n\nstr_c(c(\"B\", \"o\", \"n\", \"j\", \"o\", \"u\", \"r\"), collapse = \"\")\n\n[1] \"Bonjour\"\n\n\n\n23.2.2.4 Scinder des chaînes de caractères\nLa fonction str_sub permet d’extraire une sous-chaîne de caractères en fonction de sa position dans une chaîne de caractères. On peut préciser les positions des premiers et derniers caractères extraits par les arguments start et end. Par défaut, l’extraction commence au premier caractère et se termine au dernier. Il est préférable d’ajouter un L aux nombres dans start et end pour indiquer clairement qu’il s’agit de nombres entiers.\n\nstr_sub(\"abcdefghikl\", start = 3L, end = 5L) \n\n[1] \"cde\"\n\nstr_sub(\"abcdefghikl\", end = 5L) \n\n[1] \"abcde\"\n\nstr_sub(\"abcdefghikl\", start = 6L) \n\n[1] \"fghikl\"\n\n\nLa fonction str_split permet de scinder une chaîne de caractères en fonction d’un délimiteur. Le délimiteur est défini par l’argument pattern. Il est possible d’utiliser une expression régulière (voir ci-après) pour définir le délimiteur. Voici un exemple simple :\n\nstr_split(\"Chat-Chien-Canari\", pattern = \"-\") \n\n[[1]]\n[1] \"Chat\"   \"Chien\"  \"Canari\"\n\n\nOn peut appliquer la fonction à un vecteur de chaîne de caractères. Dans ce cas le résultat sera une liste :\n\nstr_split(c(\"Chat-Chien-Canari\", \"Dauphin-Dodo\"), pattern = \"-\")\n\n[[1]]\n[1] \"Chat\"   \"Chien\"  \"Canari\"\n\n[[2]]\n[1] \"Dauphin\" \"Dodo\"   \n\n\nOn peut obtenir une matrice en ajoutant l’option simplify = TRUE.\n\nstr_split(\n    c(\"Chat-Chien-Canari\", \"Dauphin-Dodo\"), \n    pattern = \"-\", simplify = TRUE\n)\n\n     [,1]      [,2]    [,3]    \n[1,] \"Chat\"    \"Chien\" \"Canari\"\n[2,] \"Dauphin\" \"Dodo\"  \"\"      \n\n\n\n\n\n\n\n\nNote\n\n\n\nLa fonction tstrsplit du package data.table permet de découper efficacement des chaînes de caractères et de transformer les sous-chaînes en variables dans un objet data.table (voir ?data.table::tstrsplit pour les détails). Voici un exemple, dans lequel on découpe une chaîne de caractères pour récupérer une latitude et une longitude :\n\nlibrary(data.table)\ndf &lt;- data.table(string = c(\"48.853_2.35\",\"48.8162841_2.3082668\"))\ndf[,c(\"longitude\",\"latitude\") := tstrsplit(string, \"_\")]\ndf\n\n                 string  longitude  latitude\n                 &lt;char&gt;     &lt;char&gt;    &lt;char&gt;\n1:          48.853_2.35     48.853      2.35\n2: 48.8162841_2.3082668 48.8162841 2.3082668\n\n\n\n\n\n23.2.3 Manipuler des motifs avec stringr\n\n\n23.2.3.1 Qu’est-ce qu’un motif (pattern) ?\nUn grand nombre de fonctions de stringr prennent comme argument un pattern (ou motif en français), pour le détecter, l’extraire ou le modifier. Un motif est une description abstraite d’un ensemble de chaînes de caractères possibles. Rechercher un motif dans une chaîne de caractères revient donc à vérifier si cette chaîne (ou une partie de cette chaîne) correspond à cette description.\nDans le cas le plus simple, un motif est une chaîne de caractères précise (par exemple le mot voiture). Toutefois, il prend le plus souvent la forme d’une règle logique (par exemple : une chaîne constituée uniquement de lettres minuscules sans espace, commençant par v et finissant par ure). Il est possible de décrire un motif à l’aide d’une expression régulière (regular expressions ou regex en anglais). La construction des expressions régulières en R est détaillée dans la section @ref(regex).\n\n23.2.3.2 Détecter un motif dans une chaîne de caractères\nLe package stringr propose quatre fonctions pour vérifier si les éléments d’un vecteur de type character respectent un certain motif (pattern) :\n\n\nstr_detect() vérifie si chaque élément du vecteur respecte le pattern et renvoie un vecteur de valeurs logiques (TRUE et FALSE) ;\n\nstr_count() compte le nombre de fois que le pattern est trouvé dans chaque élément du vecteur ;\n\nstr_which() renvoie la position des éléments qui respectent le pattern ;\n\nstr_subset() sélectionne les éléments du vecteur qui respectent le pattern.\n\nDans l’exemple qui suit, on cherche la lettre a dans chaque élément du vecteur fruits :\n\nfruits &lt;- c(\"pomme\", \"banane\", \"orange\", \"clémentine\")\n# Chaque élément contient-il la lettre 'a' ?\nstr_detect(fruits, pattern = \"a\")\n\n[1] FALSE  TRUE  TRUE FALSE\n\n\n\n# Combien de fois trouve-t-on la lettre 'a' dans chaque élément ?\nstr_count(fruits, pattern = \"a\")\n\n[1] 0 2 1 0\n\n\n\n# Quelle est la position des éléments qui contiennent la lettre 'a' ?\nstr_which(fruits, pattern = \"a\")\n\n[1] 2 3\n\n\n\n# Garder uniquement les éléments qui contiennent la lettre 'a'\nstr_subset(fruits, pattern = \"a\")\n\n[1] \"banane\" \"orange\"\n\n\n\n23.2.3.3 Extraire un motif\nLa fonction str_extract() permet d’extraire une sous-chaîne de caractères correspondant à un motif. Cette fonction n’est vraiment utile que si l’on utilise une expression régulière pour décrire le motif des chaînes que l’on souhaite extraire. La construction des expressions régulières en R est détaillée dans la section @ref(regex). La fonction prend deux arguments : la chaîne de caractères analysée et le motif recherché.\nDans l’exemple suivant, on recherche une sous-chaîne de caractères constituée uniquement de chiffres :\n\nstr_extract(\"J'habite au 12 rue des Arts\", pattern = \"\\\\d+\")\n\n[1] \"12\"\n\n\nSi plusieurs sous-chaînes correspondent au motif recherché, str_extract() n’extrait que la première occurrence du motif. On peut extraire toutes les occurrences du motif en utilisant str_extract_all(). Dans l’exemple suivant, str_extract_all() extrait les deux sous-chaînes numériques.\n\nstr_extract_all(\"J'habite au 12 rue des Arts 69000 Lyon\", pattern = \"\\\\d+\")\n\n[[1]]\n[1] \"12\"    \"69000\"\n\n\n\n23.2.3.4 Remplacer un motif\nLes fonctions str_replace() et str_replace_all() permettent de remplacer un motif (spécifié dans l’argument pattern) par une autre chaîne de caractères (définie dans l’argument replacement). Attention, str_replace() ne remplace que la première occurrence du motif rencontrée dans la chaîne de caractères. Si vous voulez remplacer toutes les occurrences, il faut utiliser la fonction str_replace_all(). Voici quelques exemples :\n\nstr_replace(\"j'ai une voiture\", \n            pattern = \"voiture\", \n            replacement = \"bicyclette\")\n\n[1] \"j'ai une bicyclette\"\n\nstr_replace(\"j'ai une première voiture et une deuxième voiture\", \n            pattern = \"voiture\", \n            replacement = \"bicyclette\")\n\n[1] \"j'ai une première bicyclette et une deuxième voiture\"\n\nstr_replace_all(\"j'ai une première voiture et une deuxième voiture\", \n                pattern = \"voiture\", \n                replacement = \"bicyclette\")\n\n[1] \"j'ai une première bicyclette et une deuxième bicyclette\"",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Manipuler des données textuelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#regex",
    "href": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#regex",
    "title": "23  Manipuler des données textuelles",
    "section": "\n23.3 Les expressions régulières en R\n",
    "text": "23.3 Les expressions régulières en R\n\n\n23.3.1 Que sont les expressions régulières ?\nLes expressions régulières sont un outil permettant de décrire un ensemble de chaînes de caractères possibles selon une syntaxe précise, et donc de définir un motif (ou pattern). Les expressions régulières servent par exemple lorsqu’on veut extraire une partie d’une chaîne de caractères, ou remplacer une partie d’une chaîne de caractères. Une expression régulière prend la forme d’une chaîne de caractères, qui peut contenir à la fois des éléments littéraux et des caractères spéciaux qui ont un sens logique.\nPar exemple, \"ch.+n\" est une expression régulière qui décrit le motif suivant : la chaîne littérale ch, suivi de n’importe quelle chaîne d’au moins un caractère (.+), suivie de la lettre n. Dans la chaîne \"J'ai un chien.\", la sous-chaîne \"chien\" correspond à ce motif. De même pour \"chapeau ron\" dans \"J'ai un chapeau rond\". En revanche, dans la chaîne \"La soupe est chaude.\", aucune sous-chaîne ne correspond à ce motif (car aucun n n’apparaît après le ch).\n\n\n\n\n\n\nNote\n\n\n\nLes expressions régulières (regex) sont notoirement difficiles à maîtriser. Il existe des outils qui facilitent le travail avec les expressions régulières. Ils sont présentés dans la section Comment construire des expressions régulières en R.\n\n\n\n23.3.2 Les éléments de base des expressions régulières en R\n\n\n23.3.2.1 Les types de caractères\n\n\n\n\n\n\nSymbole\nSignification\n\n\n\n.\nN’importe quel caractère\n\n\n\n[:digit:] ou \\\\d\n\nTous les chiffres de 0 à 9\n\n\n[:alpha:]\nToutes les lettres\n\n\n[:lower:]\nToutes les lettres minuscules\n\n\n[:upper:]\nToutes les lettres majuscules\n\n\n[:alnum:]\nTous les caractères alphanumériques\n\n\n[:punct:]\nTous les signes de ponctuation\n\n\n\n23.3.2.2 Les quantificateurs\nLes quantificateurs s’appliquent à l’élément qui précède. Par exemple, l’expression régulière \"abc\\\\d{3,5}\" décrit une chaîne constituée des lettres abc suivies de trois à cinq chiffres. Le quantificateur s’applique à \\\\d, pas à abc.\n\n\n\n\n\n\nQuantificateur\nSignification\n\n\n\n?\nl’élément précédent est présent zéro ou une seule fois\n\n\n*\nl’élément précédent est éventuellement présent, une fois ou plus\n\n\n+\nl’élément précédent est présent une fois ou plus\n\n\n{n}\nl’élément précédent est présent n fois\n\n\n{n,}\nl’élément précédent est présent au moins n fois\n\n\n{n,m}\nl’élément précédent est présent entre n et m fois\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLes quantificateurs + et * peuvent être difficiles à distinguer. Ils ont pourtant un sens très différent :\n\n\n+ signifie que l’élément précédent est nécessairement présent, et peut être répété. Exemple : [\\\\d]+ décrit une suite de chiffres comprenant au moins un chiffre ;\n\n* signifie que l’élément précédent est éventuellement présent, et peut être répété. Exemple : [\\\\d]* décrit une suite de chiffres qui peut éventuellement être vide.\n\n\n\n\n23.3.2.3 Les conditions logiques\nCertains caractères ont un sens particulier dans les expressions régulières, et permettent de coder des conditions logiques (ou, et…). Le petit tableau qui suit détaille ces caractères spéciaux. Deux points sont à garder en mémoire :\n\n\nles parenthèses () permettent de définir des groupes. Elles sont notamment utiles pour l’usage des quantificateurs. Voici deux exemples :\n\nl’expression régulière \"^(a|b)\\\\d+\" décrit une chaîne qui commence par a ou b suivi d’au moins un chiffre ;\nl’expression régulière \"(abc\\\\d){3,5}\" décrit une chaîne constituée des lettres abc suivies d’un chiffre, le tout répété entre trois et cinq fois. Le quantificateur s’applique à abc\\\\d, en raison de la présence des parenthèses.\n\n\nles caractères ^ (début de chaîne) et $ (fin de chaîne) sont souvent très utiles.\n\n\n\n\n\n\n\nSymbole\nSignification\n\n\n\n^a\nLa lettre “a” en première position\n\n\n^abc\nLa chaîne “abc” en première position\n\n\na$\nLa lettre “a” en dernière position\n\n\nab|de\nLa chaîne ab ou la chaîne de\n\n\n\n[abc]\nL’un des caractères a, b, c\n\n\n\n[^abc]\nTous les caractères sauf a, b, et c\n\n\n\n[a-z]\nTous les caractères de a à z\n\n\n\n[A-Z]\nTous les caractères de A à Z\n\n\n\n\n23.3.2.4 Les caractères spéciaux\nCertains caractères sont utilisés dans les expressions régulières pour décrire une caractéristique du motif (^ pour le début de la chaîne, . pour désigner n’importe quel caractère, $ pour la fin de la chaîne…). Mais il peut arriver que le motif que l’on recherche comprenne justement l’un de ces caractères spéciaux (exemple : billet de 5$). En ce cas, il faut utiliser \\ pour échapper ce caractère, pour que l’expression régulière le recherche exactement. Toutefois, le caractère \\ étant lui-même un caractère spécial, il faut l’échapper également, donc il faut utiliser \\\\. Ainsi pour indiquer dans une expression régulière que l’on recherche un ., on écrit \\\\..\nLe tableau suivant présente le code de quelques caractères spéciaux :\n\n\n\n\n\n\nSymbole\nSignification\n\n\n\n\\\\.\nLe caractère .\n\n\n\n\\\\!\nLe caractère !\n\n\n\n\\\\?\nLe caractère ?\n\n\n\n\\\\\\\\\nLe caractère \\\n\n\n\n\\\\$\nLe caractère $\n\n\n\n\\\\\"\nLe caractère \"\n\n\n\n\n\\\\( et \\\\)\n\nLes caractères ( et )\n\n\n\n\\\\s\nN’importe quel espace (tabulation, espace, retour à la ligne)\n\n\n\\\\d\nN’importe quel chiffre\n\n\n\\\\w\nN’importe quel caractère figurant dans un mot, sauf - (équivalent à [A-z0-9_])\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous recherchez une chaîne de caractères qui contient des caractères spéciaux, vous pouvez utiliser la fonction fixed. Cette fonction permet de rechercher une chaîne de caractères telle quelle, sans aucune interprétation des caractères spéciaux. Ainsi, fixed(\"20$\") désigne littéralement la chaîne \"20$\" (et est équivalente à l’expression régulière \"20\\\\$\"). Voici un exemple :\n\nstr_detect(\"Le chapeau coûte 20$.\", fixed(\"20$\"))\n\n[1] TRUE\n\n\n\n\n\n23.3.3 Quelques exemples d’expressions régulières\nVoici quelques exemples un peu complexes pour vous aider à construire vos propres expressions régulières :\n\nL’expression régulière \"(Mr|M|Mme)\\\\.?\\\\s+[\\\\w\\\\-]+\" permet de rechercher un nom de famille éventuellement composé, précédé de Mr, M ou Mme. Exemple : str_detect(\"Mme Dupont-Durand habite au 21\", pattern = \"(Mr|M|Mme)\\\\.?\\\\s+[\\\\w\\\\-]+\") ;\nL’expression régulière \"([0-9]{2}\\\\.*){5}\" permet de rechercher cinq séries de deux chiffres éventuellement séparées par un point. Exemple : str_detect(\"12.12.12.12.12.\", pattern = \"([0-9]{2}\\\\.*){5}\").",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Manipuler des données textuelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#comment-construire-des-expressions-régulières-en-r",
    "href": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#comment-construire-des-expressions-régulières-en-r",
    "title": "23  Manipuler des données textuelles",
    "section": "\n23.4 Comment construire des expressions régulières en R\n",
    "text": "23.4 Comment construire des expressions régulières en R\n\nLes expressions régulières (regex) sont notoirement difficiles à maîtriser. C’est pourquoi il existe de nombreux outils pour faciliter la construction des expressions régulières. Nous présentons ici deux types d’outils :\n\nles testeurs en ligne de regex ;\nle package rex.\n\nIl peut également être efficace pour ce genre de questions précises d’utiliser des outils d’intelligence artificielle générative, comme ChatGPT. La discussion avec l’intelligence artificielle permet d’accroître progressivement la complexité de la requête, pour prendre en compte des cas particuliers non envisagés initialement. Dans tous les cas, il est fortement conseillé de tester les suggestions de code sur les sites internet testeurs d’expressions régulières en ligne.\n\n23.4.1 Les testeurs d’expressions régulières en ligne\nPlusieurs sites internet proposent des interfaces interactives pour construire, interpréter et tester des expressions régulières sur des exemples. Voici quelques sites :\n\n\nhttps://regex101.com/ ;\n\nhttps://regexr.com/ ;\n\nhttps://www.regextester.com/ ;\n\nhttps://pythonium.net/regex.\n\n\n\n\n\n\n\nTip\n\n\n\nUn problème fréquent avec les testeurs d’expressions régulières est qu’ils utilisent des expressions régulières déjà interprétées. Cela signifie qu’une expression régulière valable dans le testeur ne le sera pas nécessairement dans R, en raison d’un traitement différent des caractères échappés. Si votre expression régulière est correcte d’après le testeur mais erronée dans R, faites attention aux caractères échappés : vous devez peut-être remplacer les \\ par \\\\. Par exemple, sur le site https://regex101.com/, l’expression régulière ([0-9]{2}\\.*){5} est correcte pour extraire la chaîne “12.12.12.12.12.”, mais elle ne fonctionne pas avec R. Il faut utiliser l’expression régulière \"([0-9]{2}\\\\.*){5}\".\n\n\n\n23.4.2 Composer une expression régulière avec le package rex\n\n\n23.4.2.1 Principe : la fonction rex\n\nLe package rex permet de construire des expressions régulières complexes avec une syntaxe relativement facile à comprendre. Vous pouvez utiliser la fonction rex pour assembler les différents éléments qui décrivent un motif, en les séparant par des virgules. Une expression régulière construite avec rex ressemble à ceci : rex(element1, element2, element3). Voici un exemple : rex(start, \"Le pivert\", anything, n_times(\"toc\", 3)) décrit le motif suivant : la chaîne de caractères commence par “Le pivert”, suivi de n’importe quelle chaîne de caractères (anything), suivie de “toc” répété trois fois (n_times(\"toc\", 3)). Ce motif est équivalent à l’expression régulière \"^Le pivert.*(?:toc ){3}\". La fonction rex peut être utilisée de deux façons :\n\nsoit pour obtenir une expression régulière :\n\n\nlibrary(rex)\nrex(start, \"Le pivert\", anything, n_times(\"toc\", 3))\n\n^Le pivert.*(?:toc){3}\n\n\n\nsoit directement dans une fonction du package stringr :\n\n\nstr_detect(\"Le pivert fait toctoctoc.\", pattern = rex(start, \"Le pivert\", anything, n_times(\"toc\", 3)))\n\n[1] TRUE\n\n\n\n23.4.2.2 La syntaxe de rex\n\nTrois types d’éléments peuvent être combinés dans la fonction rex :\n\ndes suites de caractères : une suite de caractères précise, une suite de caractères composée de caractères figurant dans une liste (exemple : une suite composée des voyelles aeiouy), une suite de caractères d’un certain type (par exemple des lettres minuscules, ou des chiffres)… Le package rex propose des shortcuts pour désigner des ensembles cohérents de caractères (par exemple : tous les chiffres, tous les signes de ponctuation…) ;\ndes quantificateurs : une certaine chaîne doit être répétée trois fois, ou entre deux et quatre fois ;\ndes liens logiques. Par exemple : la chaîne de caractère doit commencer par “Madame” ou “Monsieur”, ou doit comprendre au moins l’une des chaînes “Sarl”, “Société anonyme” ou “Société par actions simplifiée”.\n\nLe tableau suivant présente quelques-uns des shortcuts de rex, avec leur signification. Pour la liste complète, vous pouvez consulter names(shortcuts).\n\n\n\n\n\n\n\nArgument rex\n\nExpression régulière\nSignification\n\n\n\nstart\n\"^\"\nDébut de la chaîne de caractères\n\n\nend\n\"$\"\nFin de la chaîne de caractères\n\n\ndot\n\"\\\\.\"\nUn point\n\n\nany\n\".\"\nN’importe quel caractère\n\n\nsomething\n\".+\"\nN’importe quelle suite de caractères non vide\n\n\nanything\n\".*\"\nN’importe quelle suite de caractères, éventuellement vide\n\n\nalnum\n\"[:alnum:]\"\nUn caractère alphanumérique\n\n\nalpha\n\"[:alpha:]\"\nUne lettre\n\n\nnumber\n\"[:digit:]\"\nUn chiffre\n\n\nlower\n\"[:lower:]\"\nUne lettre minuscule\n\n\npunct\n\"[:punct:]\"\nUn signe de ponctuation\n\n\nspace\n\"[:space:]\"\nUn espace\n\n\nupper\n\"[:upper:]\"\nUne lettre majuscule\n\n\nalnums\n\"[[:alnum:]]+\"\nUne suite d’au moins un caractère alphanumérique\n\n\nalphas\n\"[[:alpha:]]+\"\nUne suite d’au moins une lettre\n\n\ndigits\n\"[[:digit:]]+\"\nUne suite d’au moins un chiffre\n\n\nlowers\n\"[[:lower:]]+\"\nUne suite d’au moins une lettre minuscule\n\n\npuncts\n\"[[:punct:]]+\"\nUne suite d’au moins un signe de ponctuation\n\n\nspaces\n\"[[:space:]]+\"\nUne suite d’au moins un espace\n\n\nuppers\n\"[[:upper:]]+\"\nUne suite d’au moins une lettre majuscule\n\n\nany_alnums\n\"[[:alnum:]]*\"\nUne suite de caractères alphanumériques, éventuellement vide\n\n\nany_alphas\n\"[[:alpha:]]*\"\nUne suite de lettres, éventuellement vide\n\n\nany_digits\n\"[[:digit:]]*\"\nUne suite de chiffres, éventuellement vide\n\n\nany_lowers\n\"[[:lower:]]*\"\nUne suite de lettres minuscules, éventuellement vide\n\n\nany_uppers\n\"[[:upper:]]*\"\nUne suite de lettres majuscules, éventuellement vide\n\n\nany_puncts\n\"[[:punct:]]*\"\nUne suite de signes de ponctuation, éventuellement vide\n\n\nnon_alphas\n\"[^[:alpha:]]+\"\nUne suite d’au moins un caractère autre qu’une lettre\n\n\nnon_digits\n\"[^[:digit:]]+\"\nUne suite d’au moins un caractère autre qu’un chiffre\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDans le tableau précédent, vous pouvez remarquer que certains shortcuts de rex se ressemblent deux à deux, par exemple alphas et any_alphas. En fait, la différence entre ces deux shortcuts est identique à la différence entre les quantificateurs + et * (voir la remarque de la partie Les quantificateurs) : rex(alphas) décrit une suite de lettres comprenant au moins une lettre, tandis que rex(any_alphas) décrit une suite de lettres qui peut éventuellement être vide.\n\n\nVoici la liste des principaux quantificateurs utilisables dans rex :\n\n\n\n\n\n\nArgument rex\n\nSignification\n\n\n\nzero_or_more(x)\nLes éléments de x sont présents zéro fois ou plus\n\n\none_or_more(x)\nLes éléments de x sont présents au moins une fois\n\n\nmaybe(x)\nLes éléments de x sont présents zéro ou une fois\n\n\nn_times(x, n)\nL’expression x répétée n fois\n\n\nbetween(x, low = n, high = m)\nL’expression x répétée entre n et m fois\n\n\nat_least(x, n)\nL’expression x répétée au moins n fois\n\n\nat_most(x, n)\nL’expression x répétée au plus n fois\n\n\n\nVoici la liste des principaux liens logiques utilisables dans rex :\n\n\n\n\n\n\n\nArgument rex\n\nExpression régulière\nSignification\n\n\n\nor(x, y)\n\"|\"\nL’une des expressions x ou y est présente\n\n\none_of(\"abyz\")\n[abyz]\nUn seul des caractères spécifiés\n\n\nsome_of(\"abyz\")\n[abyz]+\nUn ou plusieurs des caractères spécifiés (éventuellement répétés)\n\n\nnone_of(\"abyz\")\n[^abyz]\nAucun des caractères spécifiés n’est présent",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Manipuler des données textuelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#les-fonctions-de-base-r",
    "href": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#les-fonctions-de-base-r",
    "title": "23  Manipuler des données textuelles",
    "section": "\n23.5 Les fonctions de base R\n",
    "text": "23.5 Les fonctions de base R\n\nBeaucoup de fonctions disponibles dans stringi et stringr sont en réalité des réécritures de fonctions de R base. Le tableau suivant vous indique quel est l’équivalent en base R des principales fonctions de stringr. Pour mémoire, il est recommandé d’utiliser stringr plutôt que les fonctions de R base, car ce package est plus robuste et plus cohérent que les fonctions de base.\n\n\n\n\n\n\nstringr\nR base\n\n\n\nstr_to_lower(x)\ntolower(x)\n\n\nstr_to_upper(x)\ntoupper(x)\n\n\nstr_to_title(x)\ntools::toTitleCase(x)\n\n\nstr_trim(x)\ntrimws(x)\n\n\nstr_c(x)\n\npaste0(x) ou paste(x)\n\n\n\nstr_sub(x, start, end)\nsubstr(x, start, end)\n\n\nstr_split(x, pattern)\nstrsplit(x, pattern)\n\n\nstr_locate(x, pattern)\nregexpr(pattern, x)\n\n\nstr_locate_all(x, pattern)\ngregexpr(pattern, x)\n\n\nstr_subset(x, pattern)\ngrep(pattern, x, value = TRUE)\n\n\nstr_which(x, pattern)\ngrep(pattern, x)\n\n\nstr_detect(x, pattern)\ngrepl(pattern, x)\n\n\nstr_match(x, pattern)\n\nregexec(pattern, x) + regmatches()\n\n\n\nstr_replace(x, pattern, replacement)\nsub(pattern, replacement, x)\n\n\nstr_replace_all(x, pattern, replacement)\ngsub(pattern, replacement, x)\n\n\nstr_extract(x, pattern)\n\nregexpr(pattern, x) + regmatches()\n\n\n\nstr_length(x)\nnchar(x)\n\n\nstr_order(x)\norder(x)\n\n\nstr_sort(x)\nsort(x)\n\n\nstr_dup(x, n)\nstrrep(x, n)\n\n\nstr_wrap(x)\nstrwrap(x)",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Manipuler des données textuelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#stringrRessources",
    "href": "03_Fiches_thematiques/Fiche_donnees_textuelles.html#stringrRessources",
    "title": "23  Manipuler des données textuelles",
    "section": "\n23.6 Pour en savoir plus",
    "text": "23.6 Pour en savoir plus\n\nsur le package stringr :\n\nla documentation du package (en anglais) ;\nune vignette d’introduction à stringr (en anglais) ;\nun aide-mémoire sur stringr et les expressions régulières ;\n\n\nsur les expressions régulières :\n\n\nune page en français sur la construction des expressions régulières avec R ;\nun très bon article de blog en français sur les expressions régulières en R ;\nune vignette sur les expressions régulières avec stringr (en anglais) ;\nla partie stringr de l’introduction à R et au tidyverse (en français) ;\n\nun aide-mémoire en anglais sur la construction des expressions régulières avec R ;\n\nle chapitre dédié aux expressions régulières du livre R for Data Science ;\n\n\nsur le package RVerbalExpressions :\n\nla documentation du package (en anglais) ;\nune vignette d’exemples d’utilisation du package (en anglais).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Manipuler des données textuelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_survey.html",
    "href": "03_Fiches_thematiques/Fiche_survey.html",
    "title": "24  Utiliser des données d’enquêtes",
    "section": "",
    "text": "24.1 Tâches concernées et recommandations\nL’utilisateur souhaite exploiter des données d’enquête pour calculer des indicateurs. En particulier, l’enjeu est d’utiliser correctement les pondérations des données dans le calcul des indicateurs.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Utiliser des données d'enquêtes</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_survey.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_survey.html#tâches-concernées-et-recommandations",
    "title": "24  Utiliser des données d’enquêtes",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nPour calculer des indicateurs nécessitant l’usage des pondérations, il est recommandé d’utiliser les packages stats et Hmisc ;\nPour des estimations qui prendraient en compte de manière formelle la théorie des sondages (en particulier pour l’estimation de la variance), il est conseillé d’utiliser le package survey ;\nLe package survey fonctionne dans des conditions particulières (plan de sondage simple, ou poids bootstrap). L’utilisateur pourra se référer au package gustave pour les enquêtes de l’Insee ;\nS’agissant de l’estimation économétrique, les fonctions d’estimation offrent généralement une option weight. Par ailleurs, le package survey contient la fonction svyglm qui permet l’estimation des modèles les plus courants.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Utiliser des données d'enquêtes</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_survey.html#pourquoi-lusage-des-données-denquêtes-est-il-particulier",
    "href": "03_Fiches_thematiques/Fiche_survey.html#pourquoi-lusage-des-données-denquêtes-est-il-particulier",
    "title": "24  Utiliser des données d’enquêtes",
    "section": "\n24.2 Pourquoi l’usage des données d’enquêtes est-il particulier ?",
    "text": "24.2 Pourquoi l’usage des données d’enquêtes est-il particulier ?\n\n\n\n\n\n\nNote\n\n\n\nCe paragraphe théorique expose en quoi la spécificité des données d’enquêtes rend leur usage complexe avec un logiciel statistique. Le lecteur intéressé par les aspects pratiques de l’estimation sur données d’enquêtes avec R peut se passer de la lecture de ce paragraphe.\nCes considérations ne sont pas propres au logiciel R : le logiciel SAS propose différentes méthodes d’exploitation des données d’enquêtes et opère un calcul par défaut. L’enjeu majeur du passage de SAS à Rest alors d’assurer la cohérence et la continuité des méthodes de calcul, indépendamment du logiciel utilisé.\n\n\nLa spécificité des données d’enquêtes est qu’il s’agit de données granulaires, au sens où une observation représente en réalité un grand nombre d’unités statistiques. La théorie des sondages assure les propriétés statistiques des estimateurs dérivés des données d’enquêtes, et en particulier l’estimateur d’Horwitz-Thompson \\(\\sum_{s \\in S}\\frac{y_s}{\\pi_s}\\). L’utilisation des données pondérées ne va pas de soi, y compris dans un langage dédié à la statistique tel que R ; une des difficultés de l’estimation sur données pondérées est liée à la granularité des données, qui rend difficile la représentation de la fonction de distribution cumulative. Or la fonction de distribution cumulative est sous-jacente à de nombreux estimateurs : quantiles, indice de Gini, etc.\nPour rappel, la fonction de distribution cumulative, ou fonction de répartition, se formalise de la façon suivante : pour \\(x\\) donné, elle s’écrit \\(F(x) = P(X \\leq x)\\). Or en présence de données granulaires, une grande partie des valeurs prises par \\(x\\) n’est pas connue (car l’information n’a pas été collectée), et il existe un grand nombre de méthodes pour estimer la valeur de la fonction de répartition dans cette situation. En conséquence, et de manière symétrique, il existe une grande variété de façons d’estimer un quantile, puisque le quantile se définit de la manière suivante :\n\\(Q(p) = F^{-1}(p) = inf\\{x : F(x) \\geq p\\}\\).\nEn pratique, le quantile se calcule comme une interpolation pour les deux observations l’encadrant dans les données : \\({\\hat Q}(p) = (1-\\gamma) X_{(j)} + \\gamma X_{(j+1)}\\). Toute la question est donc la forme que l’on donne à \\(\\gamma\\) ; la variété de choix de méthodes, déjà importante dans le cas non pondéré, est démultiplié dans le cas pondéré, comme le montre la documentation de la fonction wtd.quantile et en particulier le paramètre type du package Hmisc. Pour plus de détails sur le calcul des quantiles dans les logiciels statistiques, se reporter à l’article de Hyndman et Fan.\nL’évaluation de la fonction de distribution cumulative n’est pas la seule difficulté ; d’autres estimateurs sont délicats à calculer sur des données d’enquête. En particulier, l’estimation de l’incertitude relatives aux indicateurs peut s’avérer complexe et délicate.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Utiliser des données d'enquêtes</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_survey.html#faire-une-estimation-sur-données-pondérées",
    "href": "03_Fiches_thematiques/Fiche_survey.html#faire-une-estimation-sur-données-pondérées",
    "title": "24  Utiliser des données d’enquêtes",
    "section": "\n24.3 Faire une estimation sur données pondérées",
    "text": "24.3 Faire une estimation sur données pondérées\nLe package Hmisc fournit la boîte à outils la plus simple pour calculer divers indicateurs très classiques sur des données pondérées (moyenne, variance, quantile, rang). Les fonctions de Hmisc portent le même nom que les fonctions de Base R, avec le préfixe wtd, et ont un argument supplémentaire weights pour la pondération. Ainsi la fonction qui calcule une moyenne pondérée s’appelle wtd.mean.\n\nlibrary(Hmisc)\n## table t, avec une variable quantitative y et une variable de poids p\nwith(t, wtd.mean(y, weights = p)) ## moyenne\nwith(t, wtd.std(y, weights = p)) ## écart-type\nwith(t, wtd.quantile(y, weights = p, probs = 0.5, type = 'quantile'))  ## médiane\n\n\n\n\n\n\n\nNote\n\n\n\nIl existe des packages dédiés spécifiquement aux indicateurs d’inégalité (sujet très largement éclairé par les données d’enquête), tels les packages laeken ou dineq. Cette fiche aborde uniquement les quelques indicateurs les plus classiques.\n\n\nEn sortie, on obtient les résultats attendus bruts, sans mise en forme particulière. Pour présenter les résultats, il faudra les concaténer et utiliser des packages de création de tableaux.\n\nwith(t, wtd.quantile(y, weights = p, probs = (1:9)/10), \n     type = 'quantile') ### calculer les déciles de la variable y\n\nLe code ci-dessus permet d’obtenir un vecteur (object vector) contenant les 9 déciles permettant de séparer la population, classée en fonction de la variable \\(y\\), en 10 groupes de taille égale.\nPour les adeptes de la syntaxe tidyverse, les fonctions issues du package Hmisc s’intègrent aisément dans une décomposition de la statistique sous forme de group_by lorsqu’on souhaite obtenir une estimation par groupe de population par exemple. Ainsi pour une variable categorie constituant une partition de la population, on obtient la syntaxe suivante :\n\nstat &lt;- t %&gt;%\n  group_by(categorie) %&gt;%\n  summarise(mediane = wtd.quantile(y, weights = p, probs = 0.5, type = 'quantile'))\n\nOn obtient un objet tibble qui contient la variable categorie ainsi que la valeur médiane de \\(y\\) pour chaque modalité de categorie.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Utiliser des données d'enquêtes</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_survey.html#estimer-à-la-fois-valeur-et-variance",
    "href": "03_Fiches_thematiques/Fiche_survey.html#estimer-à-la-fois-valeur-et-variance",
    "title": "24  Utiliser des données d’enquêtes",
    "section": "\n24.4 Estimer à la fois valeur et variance",
    "text": "24.4 Estimer à la fois valeur et variance\nComme toute estimation réalisée sur une partie incomplète de l’information, l’estimation sur données d’enquêtes est entourée d’incertitude. La théorie des sondages propose des méthodes pour estimer cette incertitude. Pour ce faire, il existe différents outils disponibles sous R ; le package survey est l’outil le plus répandu et couvre une gamme relativement simple de plans de sondage.\n\n\n\n\n\n\nNote\n\n\n\nPour mettre en oeuvre certains outils de calcul de la variance tels que survey, il faut disposer d’informations qui sont en pratique rarement accessibles, concernant le plan de sondage et les variables de stratification. Il existe néanmoins d’autres situations où le calcul de variance est possible sans ces informations, en particulier lorsque les producteurs mettent à disposition des poids répliqués. C’est une pratique très fréquente pour les données américaines par exemple (cf. par exemple ces exemples de codes fournis sur ce site, et en particulier l’exemple du SCF américain). À l’Insee, un des rares exemples est l’enquête HVP, ainsi qu’au niveau européen le HFCS, pour laquelle des poids répliqués sont disponibles sur une partie des millésimes. L’utilisation du package survey dans ce contexte est très utile.\n\n\n\n24.4.1 Le package survey\n\nEn préalable à tout calcul, il faut créer un objet survey.design qui contient les éléments nécessaires au calcul des estimateurs et de la variance associée. La création de cet objet se fait au travers de la fonction svydesign, ou si l’on veut utiliser des poids répliqués svrepdesign.\n\nPour utiliser la fonction svydesign, il faut spécifier les éventuelles variables de stratification, les variables de grappe, les variables permettant de définir les tirages de première et de seconde phases.\nPour utiliser la fonction svrepdesign, il suffit de spécifier la matrice des poids répliqués. Un exemple sur le HFCS est donné ci-après.\n\nLa spécification du plan de sondage est une étape complexe, car elle fait intervenir de multiples options dans les fonctions svydesign et svrepdesign, mais c’est ce qui permet à ce package d’offrir des fonctionnalités assez complètes en matière de description des estimateurs et des plans de sondage. Il faut donc prendre grand soin à considérer l’ensemble des paramètres des fonctions utilisées.\n\nlibrary(survey)\n\nhfcs.design &lt;- svrepdesign(weights = ~ HW0010, repweights = W, data = H1, \n                           scale = 1, rscales = rep(1/999, 1000), \n                           mse = FALSE, type = 'bootstrap', combined.weights = TRUE)\n\nUne fois le plan de sondage spécifié, il est possible de mettre en œuvre le calcul des estimateurs et des écarts-types associés. Ainsi quelques exemples ci-dessous :\n\nsvymean(~DA3001, \n        design = hfcs.design) ### valeur moyenne de DA3001 sur l'ensemble de l'échantillon\nsvyquantile(~DA3001, design = hfcs.design, quantiles = 0.5, \n            method = \"constant\", interval.type = \"quantile\") ### valeur médiane de DA3001\n\nLa fonction svyquantile est l’équivalent de la fonction wtd.quantile dans le package survey. Elle doit donc donner des estimations identiques pour un paramétrage équivalent. En particulier, le paramètre method permet de définir la façon dont la fonction de répartition est approximée.\nPar ailleurs, si l’on souhaite décomposer une estimation par catégorie, le package survey contient une fonction svyby qui permet de réaliser cette décomposition simplement. Il n’est de toute manière pas possible d’utiliser la syntaxe tidyverse sur des objets de type survey.design. On peut ainsi décomposer la valeur médiane de DA3001 par pays (donné par SA0100) de la façon suivante :\n\nsvyby(~DA3001, ~SA0100, hfcs.design, svyquantile, quantiles = 0.5, \n      method = \"constant\", interval.type = \"quantile\")\n\n\n24.4.2 Le package gustave\n\nLe package survey présente l’avantage de fournir un cadre général pour l’estimation sur données d’enquête, mais il occulte des aspects pratiques communs à bon nombre d’enquêtes, ce qui rend souvent les calculs de variance inadaptés à la réalité des enquêtes. En particulier, les estimations de variance prennent difficilement voire pas du tout en compte l’effet de la non-réponse totale et du calage sur marges sur la précision.\nPour répondre à cette difficulté, la division Sondages a développé un package qui offre un cadre d’analyse complet pour l’estimation de la variance. La fonction qvar permet d’estimer la variance pour un estimateur donné ; les estimations de variance pour les enquêtes de l’Insee se basent désormais très largement sur cet outil. Vous pouvez en apprendre davantage sur gustave ici.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Utiliser des données d'enquêtes</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_survey.html#quelques-bonnes-pratiques",
    "href": "03_Fiches_thematiques/Fiche_survey.html#quelques-bonnes-pratiques",
    "title": "24  Utiliser des données d’enquêtes",
    "section": "\n24.5 Quelques bonnes pratiques",
    "text": "24.5 Quelques bonnes pratiques\nDe manière générale, l’estimation sur données d’enquêtes est une estimation sur données pondérées, dont le principal enjeu est par conséquent la prise en compte de la pondération. De ce point de vue, le package Hmisc suffit largement.\nL’utilisateur qui souhaite avoir un usage avancé des données d’enquêtes, et estimer la précision (par exemple pour construire un intervalle de confiance), se trouvera face à l’alternative suivante :\n\nil travaille sur un jeu de données d’enquêtes avec des poids répliqués, qui synthétisent de facto l’intégralité des étapes dans le processus de sondage, et dans ce cas, le package survey est adapté ;\nil doit estimer la variance de manière analytique - comme c’est généralement le cas dans les enquêtes de l’Insee - et dans ce cas, c’est le package gustave qui sera le plus adapté à son besoin.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Utiliser des données d'enquêtes</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_survey.html#RessourcesSurvey",
    "href": "03_Fiches_thematiques/Fiche_survey.html#RessourcesSurvey",
    "title": "24  Utiliser des données d’enquêtes",
    "section": "\n24.6 Pour en savoir plus",
    "text": "24.6 Pour en savoir plus\n\nSample Quantiles in Statistical Packages, R. J. Hyndman and Y. Fan, https://www.amherst.edu/media/view/129116/original/Sample+Quantiles.pdf\nTutoriel d’exploitation des données d’enquêtes (essentiellement américaines) avec R, http://asdfree.com/\nVignettes du package, et en particulier celle-ci ainsi que le site compagnon du package\n\nle README du package gustave",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Utiliser des données d'enquêtes</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_spatiales.html",
    "href": "03_Fiches_thematiques/Fiche_donnees_spatiales.html",
    "title": "25  Manipuler des données spatiales",
    "section": "",
    "text": "25.1 Tâches concernées et recommandations\nL’utilisateur souhaite traiter avec R des données spatiales (données géolocalisées, polygones…). Si vous ne savez pas si cette fiche répond à votre besoin, les données spatiales sont définies dans le paragraphe Définition des données spatiales.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Manipuler des données spatiales</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#tâches-concernées-et-recommandations",
    "title": "25  Manipuler des données spatiales",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\n\nIl est recommandé d’utiliser le package sf qui couvre les principaux besoins (lecture des formats de données géographiques, traitements des données spatiales, représentations graphiques et cartographiques) ;\n\nSauf cas particuliers, il est recommandé de ne pas utiliser le package sp, qui est rendu obsolète par sf ;\nSelon le besoin exact, plusieurs autres packages proposent des fonctionnalités complémentaires (voir section Pour aller plus loin).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Manipuler des données spatiales</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#définition-des-données-spatiales",
    "href": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#définition-des-données-spatiales",
    "title": "25  Manipuler des données spatiales",
    "section": "\n25.2 Définition des données spatiales",
    "text": "25.2 Définition des données spatiales\nLe terme “données spatiales” désigne les données qui portent sur les caractéristiques géographiques des objets (localisation, contours, liens). Les caractéristiques géographiques des objets sont décrites à l’aide d’un système de coordonnées qui permettent une représentation dans un espace euclidien (\\((x,y)\\)). Le passage de l’espace réel (la Terre, qui est une sphère) à l’espace plan se fait grâce à un système de projection. Voici quelques exemples de données spatiales :\n\nUne table décrivant des bâtiments, avec les coordonnées géographiques de chaque bâtiment ;\nLe découpage communal du territoire, avec le contour du territoire de chaque commune ;\nLes routes terrestres, avec les coordonnées décrivant leur parcours.\n\nLes données spatiales rassemblent classiquement deux types de données :\n\ndes données géographiques (ou géométries) : objets géométriques tels que des points, des lignes, des polygones, ou des maillages (raster). Exemple : la forme de chaque commune, les coordonnées d’un bâtiment ;\ndes données attributaires (ou attributs) : des mesures et des caractéristiques associés aux objets géométriques. Exemple : la population de chaque commune, le nombre de fenêtres et le nombre d’étages d’un bâtiment.\n\nLes données spatiales sont fréquemment traitées à l’aide d’un système d’information géographique (SIG), c’est-à-dire un système d’information capable de stocker, d’organiser et de présenter des données alphanumériques spatialement référencées par des coordonnées dans un système de référence (CRS). R dispose de fonctionnalités lui permettant de réaliser les mêmes tâches qu’un SIG (traitement de données spatiales, représentations cartographiques).\nLes systèmes de projection font l’objet de standards internationaux et sont souvent désignés par des codes dits codes EPSG . Ce site est un bon aide-mémoire. Les plus fréquents, pour les utilisateurs français, sont les suivants (plus d’infos ici) :\n\n\n2154 : système de projection Lambert 93. Il s’agit du système de projection officiel : la plupart des données diffusées par l’administration pour la métropole sont disponibles dans ce système de projection.\n\n4326 : WGS 84 ou système de pseudo-Mercator. C’est le système de projection des données GPS. \n\n\n27572 : Lambert II étendu. Il s’agit de l’ancien système de projection officiel. Les données spatiales anciennes peuvent être dans ce format.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Manipuler des données spatiales</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#comment-utiliser-le-package-sf",
    "href": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#comment-utiliser-le-package-sf",
    "title": "25  Manipuler des données spatiales",
    "section": "\n25.3 Comment utiliser le package sf\n",
    "text": "25.3 Comment utiliser le package sf\n\nLe package sf est une boîte à outils conçue pour faciliter la manipulation de données spatiales. La grande force de sf est qu’il permet de manipuler des données spatiales comme s’il s’agissait de données traditionnelles, car il repose sur le standard ISO 19125 simple feature access défini conjointement par l’Open Geospatial Consortium (OGC) et l’International Organization for Standardization (ISO). Cette fiche illustre l’usage de sf avec le jeu de données martinique du package cartography.\nLes fonctions de sf sont pour la plupart préfixées par st_ (Spatial Type) et permettent notamment de :\n\nCalculer des distances et des surfaces ;\nAgréger rapidement des zonages (regrouper les communes en département par exemple) ;\nTrouver dans quelle commune se trouve un bâtiment à partir de ses coordonnées géographiques ;\nRecalculer des coordonnées dans un autre système de projection.\n\nLe package sf introduit un objet géographique particulier : la table de données spatiale appelée sf. Un sf est simplement une table de données R traditionnelle (un data.frame), enrichie d’une colonne supplémentaire geometry qui contient l’information géographique. Par conséquent, toutes les fonctions qui s’appliquent à un data.frame s’appliquent exactement de la même façon aux attributs des objets sf.\n\n\n\n\n\n\nNote\n\n\n\nLe package sf est une extension du package dplyr pour les objets géographiques. On peut donc utiliser le pipe (%&gt;%) pour chaîner des opérations avec sf, ce qui est souvent pratique (voir la fiche Manipuler des données avec le tidyverse).\n\n\n\n25.3.1 Créer une table de données spatiales\nCréer une table de données spatiales avec sf est très simple. Il y a deux méthodes, selon que les données spatiales sont directement disponibles dans un format de données géographiques (par un exemple un fichier shapefile) ou dans un fichier de données tabulées (par exemple un fichier .csv).\n\n25.3.1.1 Importer des fichiers de données géographiques\nLa fonction sf::st_read() permet de lire différents formats de données géographiques. Les paramètres attendus par la fonction sont :\n\n\ndsn, le nom de la source de donnée (nom du fichier ou chemin vers le répertoire contenant les données géographiques). Le format le plus commun est le .shp mais sf peut lire d’autres formats (.map, .geojson…) ;\n\nlayer, le nom de la couche. Ce paramètre est facultatif si les données ne contiennent qu’une seule couche ;\n\ncrs, pour définir le système de projection. Ce paramètre est facultatif. Si le système de projection a été défini à l’écriture du fichier, sf peut le repérer automatiquement à la lecture. Sinon, il faut le définir à la lecture avec l’argument crs.\n\n\n# Installation du package cartography\n# install.packages(\"cartography\")\n\n# Charger les données sur la Martinique\nmartinique &lt;- sf::st_read(\n  dsn = system.file(\"gpkg/mtq.gpkg\", package=\"cartography\"),\n  quiet = TRUE)\n\n\n25.3.1.2 Reconstituer des données géographiques à partir de données brutes\nIl peut arriver que les données géographiques ne soient pas stockées dans un format adapté. Par exemple, vous pouvez disposer d’un fichier .csv qui contient une table géolocalisée de logements avec leurs caractéristiques. En ce cas, vous pouvez créer une table de données géographiques en deux temps :\n\nImporter les données brutes (ou tabulées, cf + haut) dans R voir la fiche [Importer des fichiers plats (.csv, .tsv, .txt)] ;\n\nTransformer le data.frame en objet sf grâce à la commande sf::st_set_geometry(). Par exemple, si les variables lon et lat contiennent des données GPS (code epsg 4326) alors on peut faire :\n\ndf_spatial &lt;- sf::st_as_sf(x = df,                         \n             coords = c(\"lon\", \"lat\"),\n             crs = 4326)\n\n\n\n25.3.2 Structure d’un objet sf\n\nLa commande head() fait apparaître la structure de l’objet sf martinique. L’en-tête précise le type de geometry (polygon, en l’occurrence le contour des communes), et le système de projection (WGS 84 / UTM zone 20N). La table de données contient la liste des communes avec leurs caractéristiques (nom, population…). Enfin, la dernière colonne (geometry) contient l’information géographique (le contour des communes).\n\nhead(martinique, 3)\n\nSimple feature collection with 3 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 697602 ymin: 1598818 xmax: 710462 ymax: 1645182\nProjected CRS: WGS 84 / UTM zone 20N\n  INSEE_COM              STATUS            LIBGEO  POP   MED CHOM  ACT\n1     97201 Simple municipality L'Ajoupa-Bouillon 1902 13633  254  801\n2     97202 Simple municipality Les Anses-d'Arlet 3737 14558  425 1659\n3     97203 Simple municipality      Basse-Pointe 3357 14456  400 1395\n                            geom\n1 MULTIPOLYGON (((699261 1637...\n2 MULTIPOLYGON (((709840 1599...\n3 MULTIPOLYGON (((697602 1638...\n\n\nPour revenir à un dataframe traditionnel, c’est-à-dire sans la dimension géographique, la manière la plus simple est de rendre nulle la géométrie grâce à la commande sf::drop_geometry. Exemple : martinique &lt;- martinique %&gt;% sf::st_drop_geometry().\nOn peut produire très rapidement une représentation cartographique d’un attribut avec la fonction plot() :\n\n# Produire une carte de la population par commune\nplot(martinique['POP'])\n\n\n\n\n\n\n\n\n25.3.3 Effectuer des opérations sur un objet sf\n\n\n25.3.3.1 Effectuer des opérations sur les attributs\nIl est possible d’appliquer aux attributs des objets sf les fonctions de la grammaire dplyr comme si l’objet était un data.frame standard. On peut ainsi sélectionner un sous-champ de la base avec dplyr::filter(), faire du tri avec dplyr::arrange() et créer de nouvelles variables avec dplyr::mutate() :\n\ngrandes_villes_martinique &lt;- martinique %&gt;% \n  dplyr::filter(POP &gt; 10000) %&gt;% \n  dplyr::arrange(POP) %&gt;% \n  dplyr::mutate(log_pop = log(POP))\n\nOn peut également appliquer des fonctions de statistiques descriptives avec dplyr::summarise() et construire des groupes avec dplyr::group_by(). Le group_by() sur un objet spatial va également agréger les géométries : il va créer une nouvelle géométrie recoupant l’ensemble des points, des lignes ou des polygones appartenant au groupe. Par exemple, on peut construire très facilement une géométrie plus agrégée par regroupement des géométries plus fines. Imaginons que la table de données sur la Martinique comprend deux groupes : les villes de plus de 10 000 habitants et celles de moins de 10 000 habitants. On désire faire la somme de la variable POP pour chaque groupe. Dans ce cas, après avoir créé les groupes, on va effectuer un group_by() puis un summarise() :\n\nmartinique_agg &lt;- martinique %&gt;%\n  dplyr::mutate(group = (POP &lt; 10000)) %&gt;%\n  dplyr::group_by(group) %&gt;%\n  dplyr::summarise(POP = sum(POP))\n\nhead(martinique_agg)\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 690574 ymin: 1592536 xmax: 735940.2 ymax: 1645660\nProjected CRS: WGS 84 / UTM zone 20N\n# A tibble: 2 × 3\n  group    POP                                                              geom\n  &lt;lgl&gt;  &lt;dbl&gt;                                                &lt;MULTIPOLYGON [m]&gt;\n1 FALSE 281471 (((722317 1613892, 722287 1613877, 722225 1613781, 722215 161375…\n2 TRUE   99406 (((703149 1624633, 703169 1624533, 703588 1623940, 703848 162345…\n\n\nLe résultat est analogue à celui qu’on obtiendrait avec undata.frame, à l’exception de la colonne geometry. L’agrégation d’une géométrie de nature POLYGON aboutit à une colonne de format MULTIPOLYGON qui agrège des polygones. La représentation graphique rend plus évidente cette agrégation :\n\nplot(martinique['POP'])\n\n\n\n\n\n\nplot(martinique_agg['POP'])\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLes manipulations de données sur un sf sont nettement plus lentes que sur un data.frame traditionnel (car R doit gérer les informations géographiques pendant la manipulation des données). Lorsque vous manipulez des données de grandes dimensions, il peut être préférable d’effectuer les opérations sur les données avant de joindre une géométrie à celles-ci.\n\n\n\n25.3.4 Effectuer des opérations sur les géométries d’un objet géométrique\nsf propose de nombreuses fonctions pour manipuler la dimension spatiale. On en trouve un certain nombre ici et ici.\n\nPar exemple, on peut calculer la superficie d’un polygone avec sf::st_area(). Si l’unité ne convient pas, il est possible de convertir cette superficie en \\(km^2\\) :\n\nmartinique %&gt;%\n  dplyr::mutate(area = sf::st_area(.)) %&gt;%\n  dplyr::mutate(x = units::set_units(area, km^2)) %&gt;%\n  head()\n\nSimple feature collection with 6 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 695444 ymin: 1598818 xmax: 717731 ymax: 1645182\nProjected CRS: WGS 84 / UTM zone 20N\n  INSEE_COM              STATUS            LIBGEO  POP   MED CHOM  ACT\n1     97201 Simple municipality L'Ajoupa-Bouillon 1902 13633  254  801\n2     97202 Simple municipality Les Anses-d'Arlet 3737 14558  425 1659\n3     97203 Simple municipality      Basse-Pointe 3357 14456  400 1395\n4     97204 Simple municipality         Le Carbet 3683 18847  285 1568\n5     97205 Simple municipality       Case-Pilote 4458 21005  347 2096\n6     97206 Simple municipality        Le Diamant 5976 19704  430 2654\n                            geom           area               x\n1 MULTIPOLYGON (((699261 1637... 12176271 [m^2] 12.17627 [km^2]\n2 MULTIPOLYGON (((709840 1599... 25783065 [m^2] 25.78307 [km^2]\n3 MULTIPOLYGON (((697602 1638... 27753371 [m^2] 27.75337 [km^2]\n4 MULTIPOLYGON (((702229 1628... 17806777 [m^2] 17.80678 [km^2]\n5 MULTIPOLYGON (((698805 1621... 18609342 [m^2] 18.60934 [km^2]\n6 MULTIPOLYGON (((709840 1599... 28042394 [m^2] 28.04239 [km^2]\n\n\n\n25.3.5 Joindre des données géographiques et attributaires\n\n25.3.5.1 Jointure sur des attributs\nOn peut effectuer une jointure sur un objet (ou deux) objet(s) sf en fonction d’une ou plusieurs variables communes. Dans ce cas, on parle de jointure attributaire. Ces opérations peuvent être effectuées comme si les objets sf étaient des data.frames traditionnels avec les fonctions dplyr::_*join.\n\n25.3.5.2 Jointure à partir de la géométrie\nDeux objets géométriques peuvent également être associés en fonction de leur dimension spatiale. Par exemple, les fonctions ci-dessous renvoient des TRUE/FALSE pour chaque couple de géométrie des bases x et y (il y en a beaucoup d’autres, voir ?sf::geos_binary_pred) :\n\n\n\n\n\n\nFonction\nOpération\n\n\n\nst_intersects()\nQuelles géométries de x intersectent celles de y ?\n\n\nst_contains()\nQuelles géométries de x contiennent celles de y ?\n\n\nst_disjoint()\nQuelles géométries de x sont disjointes à celles de y ?\n\n\nst_is_within_distance()\nQuelles géométries de x est à moins de \\(X\\) m/km de celles de y ?\n\n\n\nLa fonction sf::st_join() permet d’appliquer ces fonctions pour réaliser la jointure à partir de la dimension spatiale. Deux arguments sont importants :\n\nl’argument join = indique la méthode à utiliser pour la jointure. Par défaut, la méthode est st_intersects, ce qui signifie que la table de sortie contient une observation pour chaque couple de géométrie qui a une intersection non vide ;\nOn utilise l’argument left = TRUE si on désire effectuer une jointure sur la gauche (par défaut, un inner join est effectué).\n\nLa vignette du package permet d’aller plus loin sur le sujet.\n\n\n\n\n\n\nTip\n\n\n\nLes jointures spatiales peuvent être très gourmandes en ressources (car il peut être nécessaire de croiser toutes les géométries de x avec toutes les géométries de y). Voici deux conseils qui peuvent vous aider :\n\nIl est préférable de tester les jointures géographiques sur un petit échantillon de données, pour estimer le temps et les ressources nécessaires à la réalisation de la jointure.\nIl est parfois possible d’écrire une fonction qui réduit la taille du problème. Exemple : vous voulez déterminer dans quelle commune se situe un logement dont vous connaissez les coordonnées et le département ; vous pouvez écrire une fonction qui réalise pour chaque département une jointure spatiale entre les logements situés dans ce département et les communes de ce département, puis empiler les 101 tables de sorties.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUne application possible de la jointure spatiale est l’obtention de données sur un zonage à façon.\nIl est, par exemple, possible d’utiliser les données carroyées mises en ligne sur le site de l’Insee pour obtenir des données sur chaque zone du zonage à façon en transformant des agrégats au niveau carré dans la granularité spatiale désirée.\nUne fonction R réalisant une jointure spatiale entre carreaux et zones est déjà disponible sur le site de l’Insee. Le principe de la fonction consiste à déterminer, pour une zone donnée, l’ensemble des carreaux qui la recouvrent puis à calculer les agrégats sur cet ensemble de carreaux. En plus des agrégats, elle permet de calculer, pour chaque zone, la part de la population vivant dans des carreaux imputés, ce qui donne une idée de la fiabilité des résultats obtenus.\n\n\n\n25.3.6 Gérer le système de projection\nLe système de projection est fondamental pour que la dimension spatiale soit bien interprétée par R. Si un système de projection est défini, il s’affiche dans la ligne Projected CRS lorsqu’on applique la fonction head() à un objet sf. Pour vérifier le système de projection d’une base de données, on peut utiliser sf::st_crs :\n\nsf::st_crs(martinique)\n\nCoordinate Reference System:\n  User input: WGS 84 / UTM zone 20N \n  wkt:\nPROJCRS[\"WGS 84 / UTM zone 20N\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"UTM zone 20N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-63,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n        AREA[\"Between 66°W and 60°W, northern hemisphere between equator and 84°N, onshore and offshore. Anguilla. Antigua and Barbuda. Bermuda. Brazil. British Virgin Islands. Canada - New Brunswick; Labrador; Nova Scotia; Nunavut; Prince Edward Island; Quebec. Dominica. Greenland. Grenada. Guadeloupe. Guyana. Martinique. Montserrat. Puerto Rico. St Kitts and Nevis. St Barthelemy. St Lucia. St Maarten, St Martin. St Vincent and the Grenadines. Trinidad and Tobago. Venezuela. US Virgin Islands.\"],\n        BBOX[0,-66,84,-60]],\n    ID[\"EPSG\",32620]]\n\n\nLes deux principales fonctions pour définir le système de projection utilisé sont :\n\n\nsf::st_set_crs : cette commande sert à préciser quel est le système de projection utilisé, c’est-à-dire comment les coordonnées (x,y) sont reliées à la surface terrestre. Cette commande ne doit pas être utilisée pour transformer le système de coordonnées, seulement pour le définir.\n\nsf::st_transform : cette commande sert à projeter les points d’une géométrie dans une autre, c’est-à-dire à recalculer les coordonnées selon un autre système de projection. Par exemple, si on désire produire une carte avec un fond openstreetmaps ou une carte dynamique leaflet à partir de données projetées en Lambert 93, il est nécessaire de re-projeter les données dans le système WGS 84 (code EPSG 4326).\n\nLa définition du système de projection se fait de la manière suivante :\n\nmartinique &lt;- martinique %&gt;% sf::st_set_crs(32620)\n\nAlors que la reprojection s’obtient de la manière suivante :\n\nmartinique &lt;- martinique %&gt;% sf::st_transform(4326)",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Manipuler des données spatiales</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#pour-aller-plus-loin",
    "href": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#pour-aller-plus-loin",
    "title": "25  Manipuler des données spatiales",
    "section": "\n25.4 Pour aller plus loin",
    "text": "25.4 Pour aller plus loin\nSelon le besoin exact, plusieurs autres packages proposent des fonctionnalités complémentaires :\n\nle package raster pour gérer le format de données de type maillage ;\nles packages pour l’analyse statistique spatiale :\n\n\nspdep pour l’économétrie spatiale : relations de voisinage entre objets spatiaux, indices d’autocorrélation spatiale, … ;\n\nspatstat pour l’économétrie spatiale ;\n\ngstat et geoR pour la géostatistique ;\n\n\n\nbtb (produit par l’Insee) pour du lissage ;\nle package rpostgis pour interfacer R à une base de données PostGIS.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Manipuler des données spatiales</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#où-trouver-des-données-géographiques-et-des-shapefiles",
    "href": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#où-trouver-des-données-géographiques-et-des-shapefiles",
    "title": "25  Manipuler des données spatiales",
    "section": "\n25.5 Où trouver des données géographiques et des shapefiles ?",
    "text": "25.5 Où trouver des données géographiques et des shapefiles ?\nIl s’agit du domaine où la diffusion de données par l’open data a permis une grande diversité des sources disponibles. Certaines données sont disponibles sous forme de shapefiles, d’autres avec des identifiants géographiques comme le code commune à apparier à un shapefile proposant la même variable.\nLes limites administratives de référence sont disponibles sur le site Admin Express de l’IGN (anciennement GeoFla).\nLe code officiel géographique (COG), qui tient à jour les entités administratives (codes communes, régions, etc.) est disponible sur le site de l’Insee et via l’API Métadonnées. La fiche API indique comment accéder à des données via une API.\n\n\n\n\n\n\nTip\n\n\n\nContrairement à ce qui pourrait être pensé, la géographie et le COG sont régulièrement modifiés, pour prendre notamment en compte des fusions et scissions de communes. Deux ensembles apparemment identiques de codes communes au 1er janvier 2022 et 1er janvier 2021 peuvent ainsi différer, et il convient de retraiter ses données pour s’assurer qu’elles sont toutes définies dans une même géographie.\n\n\nLe package COGugaison fournit un ensemble d’outils répondant à ce besoin. Il permet de réaliser de nombreuses modifications utiles au chargé d’études (remplacement des codes arrondissements dans Paris, Lyon, Marseille, identification du millésime géographique des données, visualisation des changements de géographie, transformation d’un millésime à un autre pour les communes, les zonages standards d’études de l’Insee, et les zonages à façon, etc.) sans avoir à mobiliser le COG.\n\n\n\n\n\n\nNote\n\n\n\nCe package n’étant pas disponible sur le CRAN, dans un environnement connecté à internet, il est nécessaire de l’installer depuis Github:\n\nremotes::install_github(\"antuki/COGugaison\")\n\n\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nCe package n’est pas disponible sur le CRAN. Sur AUS, on peut l’installer avec la commande install.packages(\"COGugaison\", repos = \"https://nexus.insee.fr/repository/r-public\").\n\n\nIl est également possible de trouver des données géographiques sur data.gouv, insee.fr ou sur d’autres ressources d’open data, qu’ils soient génériques comme opendatasoft ou plus institutionnels comme opendata.paris.fr/\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nL’Insee propose un outil pour sélectionner et télécharger des données géographiques via l’application creacartes.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Manipuler des données spatiales</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#RessourcesDonneesSpatiales",
    "href": "03_Fiches_thematiques/Fiche_donnees_spatiales.html#RessourcesDonneesSpatiales",
    "title": "25  Manipuler des données spatiales",
    "section": "\n25.6 Pour en savoir plus",
    "text": "25.6 Pour en savoir plus\n\nla documentation du package sf :\n\nle site officiel du package ;\nla documentation du package ;\nla cheatsheet du package (en anglais) ;\n\n\nun tutoriel détaillé sur les données spatiales et sur la cartographie avec R (en français).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Manipuler des données spatiales</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html",
    "title": "26  Manipuler des données temporelles",
    "section": "",
    "text": "26.1 Tâches concernées et recommandations\nL’utilisateur souhaite manipuler des dates (import et export du format Date, manipulation et formatage de la date, différentes fréquences) ou des séries temporelles (création d’objets ts en R).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#tâches-concernées-et-recommandations",
    "title": "26  Manipuler des données temporelles",
    "section": "",
    "text": "Information sur les packages utilisés\n\n\n\n\nLes packages base et stats proposent déjà une bonne diversité de fonctionnalités pour traiter les dates et séries temporelles ;\nLe package lubridate ajoute des fonctionnalités supplémentaires sur le traitement des dates ;\nLe package parsedate permet de contrôler les normes de format de date ;\nLe package zoo permet de gérer les données journalières ou infra-journalières ;\nPour les graphiques, ce sont les packages ggplot2 et dygraphs qui seront utilisés.\n\nPour installer les packages :\n\ninstall.packages(c(\"lubridate\", \"parsedate\", \"zoo\"))\ninstall.packages(c(\"ggplot2\", \"dygraphs\"))\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nLa fonction as.Date() existe dans les packages base et zoo, mais n’a pas la même fonctionnalité. À chaque fois qu’elle sera utilisée, on précisera son package d’origine au moyen de :: (base::as.Date() ou zoo::as.Date()).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#définitions",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#définitions",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.2 Définitions",
    "text": "26.2 Définitions\n\n26.2.1 Les dates\nUne date en R signifie généralement un jour d’une année. Elle peut s’écrire sous différents formats (voir encadré sur les normes ISO8601 et RFC3339).\nUne date peut aussi désigner un horaire ou un moment précis. On peut alors spécifier les heures, les minutes, les secondes.\n\n26.2.2 Les séries temporelles\nUne série temporelle est une données indexées sur le temps. Généralement on parle de séries mensuelles lorsque le pas entre chaque date est un mois mais on parle aussi de données annuelles, trimestrielles ou journalières voire plus haute fréquence.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#création-de-date",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#création-de-date",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.3 Création de date",
    "text": "26.3 Création de date\n\n26.3.1 Le format Date en R\nPour créer une date en R, il suffit de faire appel à la fonction as.Date() du package base. Par défaut, l’argument format correspond au format américain :\n\nM_Drucker_birth &lt;- base::as.Date(x = \"1942-09-12\")\n\nIl suffit de le changer pour créer une date à partir d’un autre format :\n\nnoel_2023 &lt;- base::as.Date(x = \"25/12/2023\", format = \"%d/%m/%Y\")\n\nLa liste des formats de données est disponible sur la page d’aide de strptime() (accessible via help(strptime)).\n\n26.3.2 Les autres formats de date POSIXct et POSIXlt\n\nPour les horaires ou pour désigner un instant, il faut créer un objet de classe POSIXt. Pour cela, on peut utiliser les fonctions as.POSIXct() et as.POSIXlt().\n\nLa fonction as.POSIXct() (ct pour Calendar Time) va stocker la date sous la forme d’un nombre de secondes depuis le 1er janvier 1970.\nLa fonction as.POSIXlt() (lt pour List Time) va stocker la date sous la forme d’une liste contenant tous les composants de la date (année, mois, jour, heure…).\n\n\npied_sur_la_lune &lt;- as.POSIXct(x = \"1969-07-21 02:56:20\", \n                               format = \"%Y-%m-%d %H:%M:%S\", tz = \"UTC\")\n\n\n26.3.3 Les différents fuseaux horaires\nIci le fuseau horaire (argument tz) est fixé sur UTC (Coordinated Universal Time = Temps Universel Coordonné). Mais il est possible d’appliquer différents fuseaux horaires à un évènement.\nPar exemple :\n\n# changement du fuseau horaire avec le package base\nheure_en_france &lt;- pied_sur_la_lune\nattr(heure_en_france , \"tzone\") &lt;- \"Europe/Paris\"\n\n# changement du fuseau horaire avec le package lubridate\nheure_los_angeles &lt;- lubridate::with_tz(\n    time = pied_sur_la_lune, \n    tzone = \"America/Los_Angeles\"\n)\n\nNeil Armstrong a posé le pied sur la lune le 1969-07-21 03:56:20 à Paris mais le 1969-07-20 19:56:20 à Los Angeles.\nPour connaître la liste des différents fuseaux horaires, il faut appeler la fonction OlsonNames() (du nom de l’Olson Database).\n\n26.3.4 Gérer les changements d’heure\nEn France, depuis 1979, on avance d’une heure en mars pour l’heure d’été. Ainsi le fuseau horaire européen est un peu plus compliqué que le fuseau UTC (qui est linéaire tout au long de l’année).\nIl suit 2 fuseaux :\n\nle CET (Central European Time) correspond à l’heure d’hiver. Cela vaut UTC+1.\nEn été, on utilise le CEST (Central European Summer Time), qui est à UTC+2.\n\nOn entend aussi parler de GMT (Greenwich Mean Time) qui correspond à l’heure de Greenwich. Aujourd’hui, c’est l’heure UTC (même fuseau que GMT) qui est la référence.\nR gère en interne automatiquement les changements d’horaire à partir des fuseaux horaires.\n\nPar exemple en hiver (CET = UTC+1) :\n\n\n# définition en date et heure locale avec le bon fuseau horaire\nchute_mur_berlin &lt;- as.POSIXct(x = \"1989-11-09 18:00\", tz = \"Europe/Berlin\")\n\n# Heure locale\nprint(chute_mur_berlin)\n\n[1] \"1989-11-09 18:00:00 CET\"\n\n# Heure UTC (exemple en islande)\nprint(chute_mur_berlin, tz = \"UTC\")\n\n[1] \"1989-11-09 17:00:00 UTC\"\n\n\n\nEn été (CEST = UTC+2) :\n\n\n# définition en date et heure locale avec le bon fuseau horaire\nvictoire_fifa_1998 &lt;- as.POSIXct(x = \"1998-07-12 21:00\", tz = \"Europe/Paris\")\n\n# Heure locale\nprint(victoire_fifa_1998)\n\n[1] \"1998-07-12 21:00:00 CEST\"\n\n# Heure UTC (exemple au Burkina Faso)\nprint(victoire_fifa_1998, tz = \"UTC\")\n\n[1] \"1998-07-12 19:00:00 UTC\"",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#autres-fonctions",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#autres-fonctions",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.4 Autres fonctions",
    "text": "26.4 Autres fonctions\n\n26.4.1 Formater une date avec la fonction format()\n\nLa fonction format() est utile avec les dates car elle permet de formater une date selon n’importe quelle représentation.\nPar exemple pour des dates :\n\n# On prend la date d'aujourd'hui\nformat(Sys.Date(), format = \"Nous sommes le %A %d %B %Y.\")\n\n[1] \"Nous sommes le Tuesday 22 April 2025.\"\n\nformat(Sys.Date(), format = \"Date du jour : %x\")\n\n[1] \"Date du jour : 04/22/25\"\n\n\nPar exemple pour des temps :\n\n# On prend la date d'aujourd'hui\nformat(Sys.time(), format = \"Nous sommes le %d %B %Y et il est %Hh%M et %Ss.\")\n\n[1] \"Nous sommes le 22 April 2025 et il est 17h39 et 55s.\"\n\nformat(Sys.time(), format = \"Il s'est écoulé %ss depuis le 1er janvier 1970.\")\n\n[1] \"Il s'est écoulé 1745343595s depuis le 1er janvier 1970.\"\n\nformat(Sys.time(), format = \"Heure : %X\")\n\n[1] \"Heure : 17:39:55\"\n\n\nLa liste des formats de données est disponible sur la page d’aide de strptime() (accessible via help(strptime)).\n\n\n\n\n\n\nLes normes ISO8601 et RFC3339\n\n\n\nLes normes ISO8601 et RFC3339 sont des conventions de représentation des dates. Selon ces 2 normes, certains formats de dates sont acceptés ou non.\nPar exemple, voici quelques formats représentant la date du 24 mai 2023 à 8h43 (UTC) :\n\n\n\"2023-05-24T08:43:00Z\" ou \"2023-05-24T08:43:00+08:00\" sont des formats acceptés par ces 2 normes.\n\n\"2023-05-24t08:43:00z\" est un format accepté uniquement par la norme RFC3339.\n\n\"2023-05-24T08:43Z\" est un format accepté uniquement par la norme ISO8601.\n\nPour savoir quels formats sont acceptés par ces normes, une infographie est disponible ici : https://ijmacd.github.io/rfc3339-iso8601/.\nOn peut aussi utiliser le package parsedate qui permet de lire une date au format ISO8601\n\nlibrary(\"parsedate\")\n\nparse_iso_8601(\"2023-05-24T08:43:00+08:00\") # Accepté par ISO8601\n\n[1] \"2023-05-24 00:43:00 UTC\"\n\nparse_iso_8601(\"2023-05-24t08:43:00z\") # Refusé par ISO8601\n\n[1] NA\n\nparse_iso_8601(\"2023-05-24T08:43Z\") # Accepté par ISO8601\n\n[1] \"2023-05-24 08:43:00 UTC\"\n\n\n\n\n\n26.4.2 Paramètres régionaux\nPour obtenir des dates, d’autres fonctions existent comme :\n\n\nSys.Date() pour connaitre la date du jour,\n\nSys.time() pour connaitre l’horaire actuel (date + heure),\n\nSys.timezone() pour le fuseau horaire actuel.\n\nIl peut être utile de vouloir changer les paramètres régionaux sous R. Pour cela, il faut faire appel à la fonction Sys.setlocale()\n\n# Paramètres locaux en France\nSys.setlocale(\"LC_TIME\", \"fr_FR.UTF-8\")\n\nWarning in Sys.setlocale(\"LC_TIME\", \"fr_FR.UTF-8\"): OS reports request to set\nlocale to \"fr_FR.UTF-8\" cannot be honored\n\n\n[1] \"\"\n\nformat(Sys.time(), format = \"%c\")\n\n[1] \"Tue Apr 22 17:39:55 2025\"\n\n# Paramètres locaux aux USA\nSys.setlocale(\"LC_TIME\", \"en_US\")\n\nWarning in Sys.setlocale(\"LC_TIME\", \"en_US\"): OS reports request to set locale\nto \"en_US\" cannot be honored\n\n\n[1] \"\"\n\nformat(Sys.time(), format = \"%c\")\n\n[1] \"Tue Apr 22 17:39:55 2025\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa fonction Sys.setlocale() a un impact sur l’affichage mais pas sur la valeur de l’objet. Ainsi, cela ne change pas le fuseau horaire (par exemple).",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#création-de-vecteur-de-date",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#création-de-vecteur-de-date",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.5 Création de vecteur de date",
    "text": "26.5 Création de vecteur de date\nPour créer un vecteur de date, il faut utiliser la fonction seq().\nLes arguments de cette fonction sont :\n\n\nfrom : une date de départ\n\nto : une date d’arrivée\n\nby : un pas\n\nlength.out : une longueur\n\nfrom est obligatoire et il faut préciser au moins 2 autres arguments parmi les 3 restants.\nExemple :\n\ndate1 &lt;- base::as.Date(\"2016-02-29\")\ndate2 &lt;- base::as.Date(\"2021-10-02\")\ndate3 &lt;- base::as.Date(\"2023-08-15\")\n\nseq(from = date1, to = date3, by = \"year\")\n\n[1] \"2016-02-29\" \"2017-03-01\" \"2018-03-01\" \"2019-03-01\" \"2020-02-29\"\n[6] \"2021-03-01\" \"2022-03-01\" \"2023-03-01\"\n\nseq(from = date2, to = date3, by = \"quarter\")\n\n[1] \"2021-10-02\" \"2022-01-02\" \"2022-04-02\" \"2022-07-02\" \"2022-10-02\"\n[6] \"2023-01-02\" \"2023-04-02\" \"2023-07-02\"\n\ntime1 &lt;- as.POSIXct(x = \"2023-05-26 15:00\")\ntime2 &lt;- as.POSIXct(x = \"2023-05-26 20:59:47\")\ntime3 &lt;- as.POSIXct(x = \"2023-05-26 21:00\")\n\nseq(from = time1, to = time3, by = \"hour\")\n\n[1] \"2023-05-26 15:00:00 UTC\" \"2023-05-26 16:00:00 UTC\"\n[3] \"2023-05-26 17:00:00 UTC\" \"2023-05-26 18:00:00 UTC\"\n[5] \"2023-05-26 19:00:00 UTC\" \"2023-05-26 20:00:00 UTC\"\n[7] \"2023-05-26 21:00:00 UTC\"\n\nseq(from = time2, to = time3, by = \"sec\")\n\n [1] \"2023-05-26 20:59:47 UTC\" \"2023-05-26 20:59:48 UTC\"\n [3] \"2023-05-26 20:59:49 UTC\" \"2023-05-26 20:59:50 UTC\"\n [5] \"2023-05-26 20:59:51 UTC\" \"2023-05-26 20:59:52 UTC\"\n [7] \"2023-05-26 20:59:53 UTC\" \"2023-05-26 20:59:54 UTC\"\n [9] \"2023-05-26 20:59:55 UTC\" \"2023-05-26 20:59:56 UTC\"\n[11] \"2023-05-26 20:59:57 UTC\" \"2023-05-26 20:59:58 UTC\"\n[13] \"2023-05-26 20:59:59 UTC\" \"2023-05-26 21:00:00 UTC\"",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#autres-objets-de-la-famille-des-dates",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#autres-objets-de-la-famille-des-dates",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.6 Autres objets de la famille des dates",
    "text": "26.6 Autres objets de la famille des dates\n\n26.6.1 Les objets difftime\n\nLe objets de classe difftime représentent des durées.\n\nage &lt;- Sys.Date() - M_Drucker_birth\nprint(age)\n\nTime difference of 30173 days\n\ndifftime(Sys.Date(), M_Drucker_birth)\n\nTime difference of 30173 days\n\n# Changement d'heure\nheure_hiver &lt;- as.POSIXct(x = \"2023-03-26 01:00\", tz = \"Europe/Paris\")\nheure_ete &lt;- as.POSIXct(x = \"2023-03-26 03:00\", tz = \"Europe/Paris\")\nheure_ete - heure_hiver\n\nTime difference of 1 hours\n\n# Voyage sur différents fuseaux horaires\ndecollage_paris &lt;- as.POSIXct(x = \"2023-07-20 10:30\", tz = \"Europe/Paris\")\narrivee_toronto &lt;- as.POSIXct(x = \"2023-07-20 01:00 PM\", \n                    format = \"%Y-%m-%d %I:%M %p\", \n                    tz = \"America/Toronto\")\narrivee_toronto - decollage_paris\n\nTime difference of 8.5 hours\n\n\nAvec la fonction units(), on peut changer l’unité de la durée.\n\n# En minutes\nunits(age) &lt;- \"mins\"\nprint(age)\n\nTime difference of 43449120 mins\n\n# En semaines\nunits(age) &lt;- \"weeks\"\nprint(age)\n\nTime difference of 4310.429 weeks\n\n\nLe package lubridate propose aussi d’autres formatages des durées :\n\nlibrary(\"lubridate\")\n\ntime_length(age, unit = \"year\")\n\n[1] 82.60917\n\n\n\n26.6.2 Fonctionnalités avancées avec lubridate\n\nLe package lubridate propose des fonctions de lecture de date semblables aux fonctions (base::as.Date(), format(), as.POSIXct())\nOn peut résumer les équivalences des principales fonctions dans le tableau ci-dessous :\n\n\n\n\n\n\n\nFonction lubridate\n\nÉquivalent R base\n\nType\n\n\n\n\ntoday(), now()\n\n\nSys.Date(), Sys.time()\n\nCréation de date\n\n\n\nyears(), weeks(), hours(), seconds() …\n-\nCréation de durée\n\n\nwith_tz()\n\nbase::as.Date() ou as.POSIXct() avec l’argument ts\n\nModification d’une date\n\n\n\nymd(), ydm_hm(), …\n\nbase::as.Date() ou as.POSIXct()\n\nLecture de date\n\n\n\nLes fonctions de création de durée permettent de modifier et décaler les dates.\nUne date au format Date est stockée sous la forme d’un nombre de jours ainsi :\n\n# En R base\nSys.Date() + 5L # Date dans 5 jours\n\n[1] \"2025-04-27\"\n\n# Avec lubridate\nSys.Date() + days(5L) # Date dans 5 jours\n\n[1] \"2025-04-27\"\n\n\nPar exemple, si je veux avoir la même date il y a 5 ou 4 ans :\n\nfev_29 &lt;- base::as.Date(\"2016-02-29\")\n\n# Il y a 5 ans, 29 fevrier 2011 n'existe pas :\nfev_29 - 365.2425 * 5 # Pas exactement la même date\n\n[1] \"2011-02-28\"\n\nfev_29 - years(5L) # Calcul impossible\n\n[1] NA\n\n# Il y a 4 ans :\nfev_29 - 365.2425 * 4\n\n[1] \"2012-02-29\"\n\nfev_29 - years(4L)\n\n[1] \"2012-02-29\"",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#les-séries-temporelles-en-r",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#les-séries-temporelles-en-r",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.7 Les séries temporelles en R",
    "text": "26.7 Les séries temporelles en R\n\n26.7.1 Créer une série temporelle\nPour créer une série temporelle, on utilise la fonction ts() du package stats. L’argument frequency est le nombre de période en 1 an.\n\n# Pour une série annuelle, frequency = 1L\nserie_annuelle &lt;- ts(1:20, start = 2003L, frequency = 1L)\n\n# Pour une série trimestrielle, frequency = 4L\nserie_trimestrielle &lt;- ts(21:40, end = 2023L, frequency = 4L)\n\n# Pour une série mensuelle, frequency = 12L\nserie_mensuelle &lt;- ts(41:60, start = 2020L, frequency = 12L)\n\n\n\n\n\n\n\n⚠️ Attention ⚠️\n\n\n\nLa fonction ts() du package stats n’est pas faite pour fonctionner avec des fréquences non entières.\n\n\n\n26.7.2 Autres formats de dates\nLes objets ts fonctionnent aussi avec des dates mais dans un format différent. Pour délimiter notre série temporelle, il faut utiliser les arguments start et/ou end ainsi qu’une fréquence (entière).\nLes arguments start et end doivent être au format AAAA, c(AAAA, PP) ou AAAA + (PP - 1)/T, avec :\n\n\nAAAA l’année en 4 caractère\net PP le numéro de la période (dépendant de la fréquence)\net T la fréquence de la série\n\nPlus d’informations sur la page d’aide de la fonction start() (accessible via help(start)).\nCes formats ne sont lisibles qu’avec la fréquence.\nExemple :\n\nPour frequency = 4L, 2021L et c(2021L, 1L) représentent la même date (1er trimestre 2021)\nPour frequency = 12L, 2019 + 3/12 = 2019.25 et c(2019L, 4L) représentent la même date\n\nAinsi c(2020L, 5L) désigne mai 2020 pour des séries mensuelles (frequency = 12L) mais le 1er trimestre 2021 pour des séries trimestrielles (frequency = 4L).\n\n26.7.3 Fonctions des objets ts\n\nPour obtenir des informations sur un objet ts, on peut utiliser les fonctions suivantes :\n\n\nstart() : retourne la date de début de la série,\n\nend() : retourne la date de fin de la série,\n\nfrequency() : retourne la fréquence de la série,\net time() : retourne le vecteur de date indexant la série temporelle\n\n\nstart(serie_mensuelle)\n\n[1] 2020    1\n\nend(serie_mensuelle)\n\n[1] 2021    8\n\nfrequency(serie_mensuelle)\n\n[1] 12\n\ntime(serie_mensuelle)\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2020 2020.000 2020.083 2020.167 2020.250 2020.333 2020.417 2020.500 2020.583\n2021 2021.000 2021.083 2021.167 2021.250 2021.333 2021.417 2021.500 2021.583\n          Sep      Oct      Nov      Dec\n2020 2020.667 2020.750 2020.833 2020.917\n2021                                    \n\n\nLes dates en output de time() sont au format ts et non au format Date. Pour les convertir, il faut utiliser la fonction zoo::as.Date() :\n\nlibrary(\"zoo\")\n\nzoo::as.Date(time(serie_mensuelle))\n\n [1] \"2020-01-01\" \"2020-02-01\" \"2020-03-01\" \"2020-04-01\" \"2020-05-01\"\n [6] \"2020-06-01\" \"2020-07-01\" \"2020-08-01\" \"2020-09-01\" \"2020-10-01\"\n[11] \"2020-11-01\" \"2020-12-01\" \"2021-01-01\" \"2021-02-01\" \"2021-03-01\"\n[16] \"2021-04-01\" \"2021-05-01\" \"2021-06-01\" \"2021-07-01\" \"2021-08-01\"\n\n\n\n26.7.4 Séries hautes fréquences avec zoo\n\nLe package zoo a d’autres utilités, notamment gérer les séries haute-fréquence.\nOn peut construire des séries journalières :\n\n#data : https://www.letour.fr/fr/parcours-general\ndate_tour_de_france &lt;- seq(from = as.Date(\"2023-06-29\"), \n                           to = as.Date(\"2023-07-21\"), by = \"day\")\nkilometre_etape &lt;- c(182, 209, 193.5, 182, 163, 145, 170, 201, \n                     182.5, 0, 167.5, 180, 169, 138, 152, 179, \n                     0, 22.4, 166, 185, 173, 133.5, 115.5)\n\ntour_de_france_ts &lt;- zoo(x = kilometre_etape, \n                         order.by = date_tour_de_france)\n\nOn peut construire des séries infra-journalières (heure par heure ou encore plus haute-fréquence) :\n\n#data : https://joint-research-centre.ec.europa.eu/photovoltaic-geographical-information-system-pvgis/pvgis-tools/hourly-radiation_en\nheures_journee &lt;- seq(from = as.POSIXct(\"2016-07-01 00:10:00\"), \n                           to = as.POSIXct(\"2016-07-01 23:10:00\"), by = \"hours\")\ntemperature &lt;- c(19.42, 19.21, 18.99, 18.78, 19.71, 20.64, 21.57, 22.82, \n                 24.06, 25.31, 26.25, 27.19, 28.12, 28.44, 28.75, 29.06, \n                 28.22, 27.38, 26.54, 24.81, 23.08, 21.35, 21.02, 20.69)\n\ntemp_ts &lt;- zoo(x = temperature, \n               order.by = heures_journee)",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#afficher-des-séries-temporelles",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#afficher-des-séries-temporelles",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.8 Afficher des séries temporelles",
    "text": "26.8 Afficher des séries temporelles\nPlusieurs packages permettent l’affichage des séries temporelles.\n\n26.8.1 Classique plot()\n\nLa fonction plot() du package graphics (méthode plot.ts() du package stats) permet d’afficher simplement des séries temporelles :\n\nplot(x = tour_de_france_ts, col = \"blue\", \n     ylab = \"Distance (en km)\", xlab = \"Time\", \n     main = \"Distance parcourue par jour (TDF 2023)\")\n\n\n\n\n\n\nplot(x = temp_ts, col = \"red\", \n     main = \"Temperature\")\n\n\n\n\n\n\n\n\n26.8.2 Package ggplot2\n\nLe package ggplot2 propose une grande variété de graphiques. Il est nécessaire au préalable de convertir l’objet en data.frame pour construire le graphique.\n\nlibrary(\"ggplot2\")\n\nsunspots_df &lt;- data.frame(date = time(sunspots), value = sunspots)\n\nggplot(sunspots_df, aes(x=date, y=value)) +\n  geom_line()\n\n\n\n\n\n\n\nPour plus d’informations sur l’utilisation de ggplot2, voir la fiche sur les graphiques\n\n26.8.3 Package dygraphs\n\nLe package dygraphs propose aussi des graphiques pour séries temporelles. L’avantage de ce package est l’interactivité et la possibilité de zoomer dans les graphiques.\n\nlibrary(\"dygraphs\")\n\ndygraph(temp_ts)\n\n\n\n\n\nOn peut aussi afficher plusieurs courbes sur le même graphique :\n\ndygraph(cbind(fdeaths, mdeaths))",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#pour-en-savoir-plus",
    "href": "03_Fiches_thematiques/Fiche_donnees_temporelles.html#pour-en-savoir-plus",
    "title": "26  Manipuler des données temporelles",
    "section": "\n26.9 Pour en savoir plus",
    "text": "26.9 Pour en savoir plus\n\n\nFormats acceptés par les normes ISO8601 et RFC3339 ;\n\nGalerie de graphique avec dygraph ;\n\nManuel d’utilisation package dygraph ;\n\nLes séries temporelles en ggplot2 ;",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Manipuler des données temporelles</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html",
    "href": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html",
    "title": "27  L’analyse de données (ACP, ACM, ACF…)",
    "section": "",
    "text": "27.1 Tâches concernées et recommandations\nVous souhaitez appliquer les méthodes classiques d’analyse de données, notamment l’analyse en composantes principales, l’analyse des correspondances multiples, l’analyse factorielle des correspondances…",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'analyse de données (ACP, ACM, ACF...)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#tâches-concernées-et-recommandations",
    "title": "27  L’analyse de données (ACP, ACM, ACF…)",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\n\nIl est recommandé d’utiliser le package FactoMineR qui permet d’appliquer toutes les méthodes de façon simple et efficace, et dont la documentation existe en français.\n\nIl est également recommandé d’utiliser le package factoextra qui propose des fonctions utiles pour analyser les résultats.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'analyse de données (ACP, ACM, ACF...)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#présentation-des-packages-factominer-et-factoextra",
    "href": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#présentation-des-packages-factominer-et-factoextra",
    "title": "27  L’analyse de données (ACP, ACM, ACF…)",
    "section": "\n27.2 Présentation des packages FactoMineR et factoextra\n",
    "text": "27.2 Présentation des packages FactoMineR et factoextra\n\nLe package FactoMineR (attention à la casse !) permet d’utiliser facilement les méthodes d’analyse de données classiques, notamment l’ACP, l’ACM, l’AFC, la CAH… Par chance, il existe de multiples ressources en français sur ce package car il a été développé par des chercheurs français (voir paragraphe Ressources).\nLe package factoextra permet par ailleurs d’extraire, de manipuler et de visualiser les résultats des méthodes d’analyse de données. Deux points sont à noter :\n\n\nfactoextra fonctionne également avec les autres packages d’analyse de données (stats, ade4, ExPosition) ;\n\nfactoextra produit des graphiques construits avec le package ggplot2, qui peuvent donc être facilement personnalisés (titre, couleurs…) et exportés.\n\nCette fiche présente rapidement comment utiliser ces deux packages pour réaliser une analyse en composantes principales. Les fonctions de ces packages qui appliquent les autres méthodes ont une grammaire similaire, et sont présentées en détail (et en français !) dans la documentation.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'analyse de données (ACP, ACM, ACF...)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#lacp-avec-factominer-et-factoextra",
    "href": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#lacp-avec-factominer-et-factoextra",
    "title": "27  L’analyse de données (ACP, ACM, ACF…)",
    "section": "\n27.3 L’ACP avec FactoMineR et factoextra\n",
    "text": "27.3 L’ACP avec FactoMineR et factoextra\n\n\n27.3.1 Etape 1 : réaliser l’ACP\nRappel rapide sur l’analyse en composantes principales : l’ACP est une méthode statistique qui permet d’explorer un jeu de données contenant des individus décrits par plusieurs variables quantitatives, pour lesquelles il est très difficile de visualiser les données dans un “hyper-espace” multidimensionnel. L’ACP est utilisée pour résumer le plus d’informations possibles en un nombre limité de nouvelles variables appelées composantes principales (deux ou trois). Ces composantes principales sont une combinaison linéaire des variables de départ, et peuvent être utilisées pour construire des représentations graphiques des donnes, en perdant le moins possible d’information.\n\nlibrary(FactoMineR)\nlibrary(factoextra)\n\n\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\nL’analyse en composantes principales est réalisée avec la commande PCA() de FactoMineR. Cette fonction produit une liste qui contient tous les résultats de l’ACP (valeurs propres, coordonnées…). On peut analyser la structure de cette liste avec la fonction str().\n\nPCA(df, scale.unit = TRUE, ncp = 5, graph = TRUE)\n\nLes arguments de la fonction PCA() sont les suivants :\n\n\ndf : jeu de données de type data frame. Les lignes sont des individus et les colonnes sont des variables numériques ;\n\nscale.unit : une valeur logique. Si TRUE, les données sont standardisées/normalisées avant l’analyse ;\n\nncp : nombre de dimensions conservées dans les résultats finaux ;\n\ngraph : une valeur logique. Si TRUE, deux graphiques sont affichés : la projection des individus dans l’espace des individus, et la projection des variables dans l’espace des variables.\n\nOn va illustrer l’utilisation de ACP() avec le jeu de données decathlon2 du package factoextra, qui donne les performances d’un ensemble d’athlètes aux dix épreuves du décathlon.\n\ndata(decathlon2)\nhead(decathlon2)\n# Garder les données pertinentes pour l'ACP\ndecathlon2_quant &lt;- decathlon2[c(1:23), c(1:10)]\n\n\n\n          X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus\nSEBRLE    11.04      7.58    14.83      2.07 49.81        14.69  43.75\nCLAY      10.76      7.40    14.26      1.86 49.37        14.05  50.72\nBERNARD   11.02      7.23    14.25      1.92 48.93        14.99  40.87\nYURKOV    11.34      7.09    15.19      2.10 50.42        15.31  46.26\nZSIVOCZKY 11.13      7.30    13.48      2.01 48.62        14.17  45.67\nMcMULLEN  10.83      7.31    13.76      2.13 49.91        14.38  44.41\n          Pole.vault Javeline X1500m Rank Points Competition\nSEBRLE          5.02    63.19  291.7    1   8217    Decastar\nCLAY            4.92    60.15  301.5    2   8122    Decastar\nBERNARD         5.32    62.77  280.1    4   8067    Decastar\nYURKOV          4.72    63.44  276.4    5   8036    Decastar\nZSIVOCZKY       4.42    55.37  268.0    7   8004    Decastar\nMcMULLEN        4.42    56.37  285.1    8   7995    Decastar\n\n\nOn stocke dans resultats_acp les résultats de l’ACP. Il s’agit d’une liste, dont la structure peut être affichée avec la fonction str(resultats_acp).\n\nresultats_acp &lt;- PCA(decathlon2_quant, scale.unit = TRUE, ncp = 5, graph = FALSE)\n\n\n27.3.2 Etape 2 : Exploiter les résultats de l’ACP avec factoextra\n\nLes fonctions suivantes du package factoextra seront utilisées :\n\n\nget_eigenvalue() : extraire les valeurs propres ;\n\nfviz_eig() : visualiser les valeurs propres ;\n\nget_pca_ind(), get_pca_var() : extraire les résultats pour les individus et les variables, respectivement ;\n\nfviz_pca_ind(), fviz_pca_var() : visualiser les résultats des individus et des variables, respectivement ;\n\nfviz_pca_biplot() : créer un biplot des individus et des variables.\n\n\n27.3.2.1 Analyse des valeurs propres\nLa fonction get_eigenvalue() extrait les valeurs propres de l’ACP, et la fonction fviz_eig() les représente graphiquement. Ces deux fonctions permettent de choisir rapidement le nombre de composantes principales que l’on souhaite retenir.\n\neig.val &lt;- get_eigenvalue(resultats_acp)\neig.val\n\n\n\n       eigenvalue variance.percent cumulative.variance.percent\nDim.1   4.1242133        41.242133                    41.24213\nDim.2   1.8385309        18.385309                    59.62744\nDim.3   1.2391403        12.391403                    72.01885\nDim.4   0.8194402         8.194402                    80.21325\nDim.5   0.7015528         7.015528                    87.22878\nDim.6   0.4228828         4.228828                    91.45760\nDim.7   0.3025817         3.025817                    94.48342\nDim.8   0.2744700         2.744700                    97.22812\nDim.9   0.1552169         1.552169                    98.78029\nDim.10  0.1219710         1.219710                   100.00000\n\n\n\nfviz_eig(resultats_acp, addlabels = TRUE)\n\n\n\n\n\n\n\n\n\n\n27.3.2.2 Analyse de la projection des variables\nLa fonction get_pca_var() permet de récupérer des informations sur les variables :\n\n\nvar$coord : coordonnées des variables pour créer un nuage de points.\n\nvar$cos2 : cosinus carré des coordonnées des variables. Cette variable mesure la qualité de représentation des variables sur le graphique de l’ACP (c’est-à-dire dans quelle mesure la variance de chaque variable se décompose selon les différentes composantes principales) ;\n\nvar$contrib : contient les contributions (en pourcentage) des variables aux composantes principales.\n\n\nvar &lt;- get_pca_var(resultats_acp)\nvar\n\n\n\nPrincipal Component Analysis Results for variables\n ===================================================\n  Name       Description                                    \n1 \"$coord\"   \"Coordinates for the variables\"                \n2 \"$cor\"     \"Correlations between variables and dimensions\"\n3 \"$cos2\"    \"Cos2 for the variables\"                       \n4 \"$contrib\" \"contributions of the variables\"               \n\n\nOn peut représenter facilement les variables sur le cercle de corrélation avec la fonction fviz_pca_var(). Par défaut cette fonction représente les deux premières dimensions, mais on peut choisir d’autres dimensions en précisant axes = c(i, j).\n\nfviz_pca_var(resultats_acp)\n\n\n\n\n\n\n\n\n\n\nfviz_pca_var(resultats_acp, axes = c(1, 3))\n\n\n\n\n\n\n\n\n\nOn peut représenter facilement la qualité de représentation des variables avec le package corrplot (somme en ligne sur toutes les dimensions = 1) :\n\nlibrary(corrplot)\ncorrplot(var$cos2, is.corr = FALSE)\n\n\n\ncorrplot 0.95 loaded\n\n\n\n\n\n\n\n\nOn peut également représenter facilement la contribution des variables aux composantes principales (somme en colonne sur toutes les variables = 100) :\n\ncorrplot(var$contrib, is.corr = FALSE)\n\n\n\n\n\n\n\n\n\nLa fonction fviz_contrib() permet de faire un graphique des contributions à une composante principale :\n\nfviz_contrib(resultats_acp, choice = \"var\", axes = 1)\n\n\n\n\n\n\n\n\n\n\n27.3.2.3 Analyse de la projection des individus\nLa fonction get_pca_ind() permet de récupérer des informations sur les individus : les coordonnées, la qualité de représentation et les contributions.\n\nind &lt;- get_pca_ind(resultats_acp)\nind\n\n\n\nPrincipal Component Analysis Results for individuals\n ===================================================\n  Name       Description                       \n1 \"$coord\"   \"Coordinates for the individuals\" \n2 \"$cos2\"    \"Cos2 for the individuals\"        \n3 \"$contrib\" \"contributions of the individuals\"\n\n\nOn peut représenter facilement les individus sur le graphique des composantes principales avec la fonction fviz_pca_ind(). Par défaut cette fonction représente les deux premières dimensions, mais on peut choisir d’autres dimensions en précisant axes = c(i, j).\n\nfviz_pca_ind(resultats_acp)\n\n\n\n\n\n\n\n\n\n\nfviz_pca_ind(resultats_acp, col.ind = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE # Évite le chevauchement de texte\n             )\n\n\n\n\n\n\n\n\n\nLa fonction fviz_contrib() permet de faire un graphique des contributions des individus à une composante principale :\n\nfviz_contrib(resultats_acp, choice = \"ind\", axes = 1)\n\n\n\n\n\n\n\n\n\nEnfin, la fonction fviz_pca_biplot() permet de réaliser un graphique représentant ensemble les variables et les individus. Dans ce graphique, un individu qui se trouve du même côté d’une variable donnée a une valeur élevée pour cette variable (et inversement).\n\nfviz_pca_biplot(resultats_acp, repel = TRUE,\n                col.var = \"blue\", # Couleur des variables\n                col.ind = \"red\"  # Couleur des individues\n                )",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'analyse de données (ACP, ACM, ACF...)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#RessourcesACP",
    "href": "03_Fiches_thematiques/Fiche_analyse_de_donnees.html#RessourcesACP",
    "title": "27  L’analyse de données (ACP, ACM, ACF…)",
    "section": "\n27.4 Pour en savoir plus",
    "text": "27.4 Pour en savoir plus\nOn trouve sur internet beaucoup de ressources sur le package FactoMineR :\n\nle site officiel du package FactoMineR (entièrement en français) ;\nun cours complet d’analyse de données avec FactoMineR (entièrement en français) ;\nles tutoriels de FactoMineR (entièrement en français) ;\nLes auteurs de FactoMineR ont écrit un livre sur les méthodes d’analyse de données (en anglais), avec des exemples utilisant le package.\n\nUne introduction à l’utilisation de factoextra pour visualiser les résultats (en anglais) ;\nLes auteurs de FactoMineR ont également développé un plugin pour faciliter l’utilisation du package.",
    "crumbs": [
      "Introduction",
      "Manipuler des données avec R",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>L'analyse de données (ACP, ACM, ACF...)</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_graphiques.html",
    "href": "03_Fiches_thematiques/Fiche_graphiques.html",
    "title": "28  Faire des graphiques avec ggplot2",
    "section": "",
    "text": "28.1 Tâches concernées et recommandations\nL’utilisateur souhaite réaliser des graphiques (nuages de points, histogrammes, densité…) et les personnaliser (légendes, titres, échelles…).",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Faire des graphiques avec `ggplot2`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_graphiques.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_graphiques.html#tâches-concernées-et-recommandations",
    "title": "28  Faire des graphiques avec ggplot2",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nIl est recommandé d’utiliser le package ggplot2 qui permet de réaliser et de personnaliser un grand nombre de représentations graphiques ;\nIl est conseillé aux utilisateurs débutants d’utiliser l’add-in esquisse pour se familiariser avec ggplot2.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCertains exemples de cette fiche utilisent les données disponibles dans le package doremifasolData ; vous ne pourrez reproduire ces exemples que si ce package est installé sur la machine sur laquelle vous travaillez. Si vous ne savez pas si ce package est déjà installé, consultez la fiche Comment utiliser la documentation utilitR.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Faire des graphiques avec `ggplot2`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_graphiques.html#découvrir-ggplot2-avec-ladd-in-esquisse",
    "href": "03_Fiches_thematiques/Fiche_graphiques.html#découvrir-ggplot2-avec-ladd-in-esquisse",
    "title": "28  Faire des graphiques avec ggplot2",
    "section": "\n28.2 Découvrir ggplot2 avec l’add-in esquisse\n",
    "text": "28.2 Découvrir ggplot2 avec l’add-in esquisse\n\n\n28.2.1 A quoi sert esquisse ?\nLe package esquisse propose une interface graphique qui facilite la construction de graphiques avec ggplot2. L’objectif de ce package est de vous aider à construire rapidement un code ggplot2 pour faire le graphique que vous voulez. En revanche, l’utilisation d’esquisse ne remplace pas l’écriture d’un code ggplot2, et cet add-in ne doit pas être utilisé pour réaliser et exporter un graphique sans sauvegarder le code qui le produit (sinon vous ne pourrez ni reproduire ni modifier votre graphique par la suite).\nVous pouvez en apprendre davantage sur les add-ins dans le paragraphe de présentation des add-ins.\n\n\n\n\n\n\nTip\n\n\n\nL’interface graphique d’esquisse est assez gourmande en ressources, il est donc déconseillé de l’utiliser avec des données volumineuses (plus de 100 000 observations). Si vous êtes dans cette situation, il est fortement conseillé d’utiliser esquisse sur une petite partie de vos données (les 1 000 ou 10 000 premières lignes par exemple). Une fois que vous avez construit le code ggplot2, vous pouvez le copier-coller dans un script R et réaliser le graphique sur l’ensemble des données.\n\n\n\n\n\n\n\n\nNote\n\n\n\nL’add-in esquisse ne fonctionne qu’avec une version récente de RStudio (version égale ou supérieure à 1.2). Si votre version de RStudio est plus ancienne, vous devrez procéder à une mise à jour avant d’utiliser esquisse.\n\n\n\n28.2.2 Comment utiliser esquisse\n\nL’add-in esquisse est disponible sous la forme d’un package, qu’il faut installer le cas échéant :\n\ninstall.packages(\"esquisse\")\n\nCette section illustre l’utilisation d’esquisse avec la table data_iris_paris_2017 du package doremifasolData, qui contient des données économiques et sociales sur les iris de la ville de Paris en 2017. Il faut donc charger ces données dans R :\n\nlibrary(doremifasolData)\ndata_iris_paris2017 &lt;- doremifasolData::data_iris_paris_2017\n\nUne fois qu’il est installé, vous pouvez accéder à cet add-in en cliquant sur ‘ggplot2’ builder dans le menu Addins de RStudio.\n\n\nVous pouvez également accéder à esquisse en exécutant le code suivant :\n\nlibrary(esquisse)\nesquisser()\n\nDans la première boîte de dialogue, vous devez sélectionner le data.frame qui contient les données que vous voulez représenter sur le graphique. Vous pouvez éventuellement sélectionner certaines variables, et modifier le type de certaines variables.\n\nUne nouvelle boîte de dialogue s’affiche, grâce à laquelle vous pouvez construire un graphique. Les variables présentes dans vos données sont listées en haut.\n\nVous pouvez déplacer les variables dans les différentes catégories (dans le cadre vert) : x pour les abscisses, y pour les ordonnées, color et size pour la couleur et la taille des éléments graphiques… esquisse vous propose automatiquement le type de graphique le plus adapté à vos données, mais vous pouvez changer le type de graphique en cliquant sur la petite icône graphique en haut à gauche (cadre rouge).\n\nEnfin, il est possible de modifier l’apparence du graphique (titres des axes, légende…) avec les menus en bas.\n\nUne fois que le graphique est terminé, vous pouvez récupérer le code ggplot2 qui produit ce graphique en cliquant sur Export & code (cadre rouge).Vous pouvez le copier-coller dans un script R. Il n’est pas recommandé d’exporter directement le graphique, car en ce cas vous ne pourrez ni reproduire ni modifier votre graphique par la suite.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Faire des graphiques avec `ggplot2`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_graphiques.html#les-concepts-clefs-de-ggplot2",
    "href": "03_Fiches_thematiques/Fiche_graphiques.html#les-concepts-clefs-de-ggplot2",
    "title": "28  Faire des graphiques avec ggplot2",
    "section": "\n28.3 Les concepts clefs de ggplot2\n",
    "text": "28.3 Les concepts clefs de ggplot2\n\nL’objectif du package ggplot2 est de fournir une approche unique pour produire quasiment toute représentation graphique de données. Ce package propose un grand nombre de fonctions permettant de personnaliser finement les représentations graphiques. Cette fiche n’est donc qu’une introduction succincte à ggplot2. Pour des formations plus détaillées, se référer à {#ggplot2Ressources}.\nPour commencer, il faut charger le package avec library. Cette fiche illustre l’utilisation de ggplot2 avec la table data_iris_paris_2017 du package doremifasolData, qui contient des données économiques et sociales sur les iris de la ville de Paris en 2017.\n\nlibrary(ggplot2)\nlibrary(doremifasolData)\ndata_iris_paris_2017 &lt;- doremifasolData::data_iris_paris_2017\nggplot2::theme_set(theme_minimal())\n\n\n28.3.1 Introduction\nLa fonction essentielle de ggplot2 est ggplot(). Il faut définir quatre éléments pour construire un graphique avec ggplot() :\n\nla table de données ;\nle mapping : on définit dans l’aesthetic (ou aes) le lien entre les variables des données et ce que l’on veut représenter sur le graphique (quelle variable sur l’axe x, sur l’axe y, quelle variable pour définir une graduation de couleurs…) ;\nla forme géométrique ou geometry  : on définit la représentation graphique qu’on souhaite utiliser. Les géométries ont toutes un nom qui commence par geom_ ; par exemple, il faut utiliser la géométrie geom_point() pour réaliser un nuage de points ;\nles paramètres : on définit les autres paramètres qui dépendent de constantes (par exemple : je veux que toutes mes lignes soient rouges ou de taille 2 pixels).\n\nLa construction d’un graphique repose sur le principe de couches successives. Les différentes couches graphiques se superposent et s’enchaînent grâce à l’opérateur +, comme un pipe. Il est possible d’aller à la ligne dans une instruction ggplot(), il suffit que l’opérateur + figure à la fin de la ligne.\nVoici un exemple de code qui crée un nuage de points (géométrie geom_point()) à partir des données mes_donnees, avec les variables variable1 en abscisse et variable2 en ordonnée :\n\nggplot(data = mes_donnees) + \n  geom_point(mapping = aes(x = variable1, y = variable2), ...)\n\n\n28.3.2 Le mapping et l’utilisation d’aes()\n\nLe mapping désigne dans ggplot2 la relation entre un attribut graphique de la geometry (abscisse, ordonnée, couleur…) et une variable présente dans la table de données. On déclare le mapping grâce à la fonction aes() (pour aesthetic), qui sert donc à identifier les variables que l’on souhaite représenter sur le graphique.\nLes arguments fondamentaux de aes() sont les variables représentées sur l’axe des abscisses et l’axe des ordonnées (x et y). Par exemple, on écrit aes(x = niveau_vie_median, y = taux_chomage) si l’on souhaite représenter pour chaque iris le taux de chômage (sur l’axe y) en fonction du niveau de vie médian (sur l’axe x).\n\nggplot(data_iris_paris_2017) + \n  geom_point(aes(x = niveau_vie_median, y = taux_chomage))\n\n\n\n\n\n\n\nPar ailleurs, la fonction aes() admet d’autres arguments qui permettent de modifier l’apparence des attributs graphiques selon une troisième variable du jeu de données. Voici les arguments supplémentaires les plus courants :\n\n\nAttribut\nDescription\n\n\n\ncolor\nCouleur des lignes ou des points\n\n\nshape\nForme des points\n\n\nsize\nTaille des points\n\n\nalpha\nTransparence des points\n\n\nfill\nCouleur des surfaces\n\n\nlinetype\nType de ligne (continue, pointillée, …)\n\n\n\nDans l’exemple qui suit, on représente pour chaque iris le taux de chômage (sur l’axe y) en fonction du niveau de vie médian (sur l’axe x), en colorant les points en fonction de la part des cadres et professions intellectuelles supérieures parmi les actifs de l’iris (color = part_cadres).\n\nggplot(data_iris_paris_2017) + \n  geom_point(aes(x = niveau_vie_median, y = taux_chomage, color = part_cadres))\n\n\n\n\n\n\n\nIl est également possible d’utiliser ces mêmes arguments pour modifier un attribut graphique sans le lier à une variable. En ce cas, on définit l’attribut à l’extérieur de l’aesthetic (donc à l’extérieur de aes()). Voici l’exemple précédent, modifié pour que tous les points soient (rouge), et non en fonction d’une variable. L’argument color est donc à l’extérieur de aes().\n\nggplot(data_iris_paris_2017) + \n  geom_point(aes(x = niveau_vie_median, y = taux_chomage), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLa notion de mapping et la définition des attributs graphiques à l’intérieur et à l’extérieur d’aes() sont une des principales difficultés de ggplot2. Il est normal de tâtonner lorsqu’on commence à les utiliser. Pour s’y retrouver, il suffit de suivre la règle suivante. Si on établit un lien entre les valeurs d’une variable et un attribut graphique, il s’agit d’un mapping qui doit être défini à l’intérieur de la fonction aes(). Dans le cas contraire, il s’agit d’un simple paramètre du graphique, qui doit être défini à l’extérieur de la fonction aes().\n\n\n\n28.3.3 Les formes géométriques\nLa forme géométrique ou geometry désigne le type de représentation graphique utilisée (nuage de points, histogrammes…). On spécifie le type de représentation que l’on souhaite en utilisant une fonction dont le nom commence par geom_. Le tableau ci-dessous présente quelques représentations graphiques classiques.\n\n\n\n\n\n\n\ngeometry\nDescription\nArguments\n\n\n\ngeom_point()\nNuage de points\n\nx, y, shape, fill, size\n\n\n\ngeom_line()\nLigne\n\nx, y, linetype\n\n\n\ngeom_bar()\nDiagramme en barres\n\nx, fill, linetype, weight\n\n\n\ngeom_histogram()\nHistogramme\n\nx, fill, linetype, weight\n\n\n\ngeom_boxplot()\nBoîte à moustaches\n\nx, y, fill, weight\n\n\n\ngeom_density()\nCourbe de densité\n\nx, y, fill, color, linetype\n\n\n\n\nVoici deux exemples d’utilisation.\n\n\n\n\n\n\nNote\n\n\n\nIl existe un grand nombre de géométries dans ggplot2. Vous pouvez en afficher la liste en exécutant la commande help.search(\"^geom_\", package = \"ggplot2\").\nPar ailleurs, de très nombreux packages proposent encore d’autres géométries pour réaliser des représentations graphiques particulières (cartes avec ggmap, arbres généalogiques avec ggtree et ggenealogy, résultats d’élections avec ggparliament…).\n\n\n\n28.3.4 Combiner plusieurs formes géométriques\nOn peut représenter plusieurs formes géométriques simultanément sur un même graphique (un nuage de points et une droite de régression par exemple). il suffit de les ajouter les unes aux autres avec l’opérateur +. Voici un exemple :\n\nggplot(data_iris_paris_2017) + \n  geom_point(aes(x = niveau_vie_median, y = taux_chomage), alpha = 0.2) + \n  geom_smooth(aes(x = niveau_vie_median, y = taux_chomage), method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl peut arriver qu’on définisse le même mapping dans plusieurs formes géométriques, c’est-à-dire qu’on utilise les mêmes relations entre les variables et les attributs graphiques dans des formes géométriques différents (exemple : les variables x et y sont les mêmes dans un nuage de points et dans la droite de régression). Dans ce cas, il est possible de déclarer le mapping directement dans l’appel à ggplot() plutôt que de le répéter dans chaque forme géométrique. Le mapping sera alors valable pour toutes les formes géométriques du graphique (sauf si celles-ci redéfinissent explicitement le mapping). Le code suivant produit exactement le même graphique que l’exemple précédent :\n\nggplot(data_iris_paris_2017, aes(x = niveau_vie_median, y = taux_chomage)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(method = \"lm\")\n\n\n\n\n28.3.5 Créer un graphique par modalité d’une variable\nLa fonction facet_wrap() permet de représenter des données en facettes, c’est-à-dire décomposées par une variable de croisement. Chaque modalité de la variable catégorielle servira à découper les données pour générer un graphique. Par défaut, les échelles des axes \\(x\\) et \\(y\\) sont identiques mais il est possible de les distinguer avec le paramètre scales (qui prend la valeur \"free_x\" pour libérer l’axe des abscisses, \"free_y\" pour l’axe des ordonnées ou \"free\" pour les deux axes). Les arguments optionnels nrow et ncol permettent de contrôler le nombre lignes et de colonnes. Voici un exemple sur les iris parisiens :\n\nggplot(data_iris_paris_2017) +\n    geom_point(mapping = aes(x = niveau_vie_median, y = taux_chomage, color = part_cadres)) +\n    facet_wrap(~ categorie_arrondissement, scales = 'free', ncol = 1)",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Faire des graphiques avec `ggplot2`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_graphiques.html#comment-personnaliser-un-graphique",
    "href": "03_Fiches_thematiques/Fiche_graphiques.html#comment-personnaliser-un-graphique",
    "title": "28  Faire des graphiques avec ggplot2",
    "section": "\n28.4 Comment personnaliser un graphique",
    "text": "28.4 Comment personnaliser un graphique\nIl est possible de personnaliser un graphique ggplot2 de deux façons :\n\nen modifiant les options une à une (titres du graphique et des axes, graduation des axe…) ;\nen utilisant un theme prédéfini ou en le définissant soi-même.\n\nAvec ggplot, il est possible d’aboutir au même résultat avec plusieurs instructions, en particulier lorsqu’il est question de customisation du thème. Il ne faut ainsi pas hésiter à chercher une solution sur le site StackOverflow.\n\n28.4.1 Les options des graphiques\nLes graphiques réalisés avec ggplot2 peuvent être personnalisés finement. Chaque nouvel élément graphique est à rajouter à l’objet ggplot avec l’opérateur +.\n\n28.4.1.1 Définir les titres du graphique et des axes\nLa fonction labs() permet de définir les titres : le titre général du graphique (title), le sous-titre (subtitle), la note de bas de graphique (caption), les axes (x, y)… Il existe d’autres façons de définir les titres (ggtitle, xlab, ylab).\n\nggplot(data_iris_paris_2017) +\n  geom_point(aes(x = niveau_vie_median, y = taux_chomage, color = part_cadres)) +\n  labs(title=\"Taux de chômage par iris à Paris en fonction du niveau de vie médian\",\n       x=\"Taux de chômage\",\n       y=\"Niveau de vie médian\",\n       color = \"Part de cadres parmi les actifs\",\n       caption=\"Sources : Filosofi 2017, RP 2017\")\n\n\n\n\n\n\n\n\n28.4.1.2 Utilisation des scales\n\nLes fonctions scales dans ggplot2 permettent de modifier la manière dont un attribut graphique est relié aux valeurs d’une variable, et dont la légende correspondante va être affichée. L’utilisation des scales permet de définir facilement un grand nombre d’options. Par exemple, pour les attributs x et y, on peut définir la nature des variables (discrètes ou continues), les graduations, l’unité des étiquettes (pourcentage, euros…), et pour l’attribut color on peut contrôler la palette de couleur utilisée.\nPour modifier une scale existante, on ajoute à l’objet ggplot un élément qui prend la forme scale_&lt;attribut&gt;_&lt;échelle&gt;. Les attributs sont listés dans le tableau de la section @ref(mapping) (size, color, fill…). Les échelles sont listées dans le tableau suivant :\n\n\n\n\n\n\nParamétrage\nDescription\n\n\n\ncontinuous\ngérer les variables continues\n\n\ndiscrete\ngérer les variables discrètes\n\n\ndate\ngérer une variable au format date\n\n\nreverse\ninverser l’axe\n\n\nlog\nconvertir l’échelle d’une variable continue en échelle logarithmique\n\n\nlog10\nconvertir l’échelle d’une variable continue en échelle logarithmique décimale\n\n\nviridis\nutiliser une palette de couleur viridis\n\n\nbrewer\nutiliser une palette de couleur brewer (variable discrète)\n\n\ndistiller\nutiliser une palette de couleur brewer (variable continue)\n\n\ngradient\nutiliser un gradient de 2 couleurs\n\n\ngradient2\nutiliser un gradient divergent de 3 couleurs\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLe package scales est très utile pour mettre en forme les étiquettes des échelles (pourcentage, euro, dollar…) et est développé par les auteurs de ggplot2. Il est vivement conseillé de l’utiliser lorsqu’on veut produire des graphiques pour une publication.\n\n\nDans l’exemple suivant, on utilise deux scales :\n\nla fonction scale_x_log10() pour mettre en forme l’axe des abscisses : les valeurs sont représentées selon une échelle logarithmique, qui va de 10 000 à 70 000 euros, avec une graduation tous les 10 000 euros ;\nla fonction scale_y_continuous() pour mettre en forme l’axe des ordonnées : l’axe va de 0 à 0.3, les étiquettes sont exprimées en pourcentage et on gradue tous les 5 points.\n\n\nlibrary(scales)\nggplot(data_iris_paris_2017) + \n  geom_point(aes(x = niveau_vie_median, y = taux_chomage, color = part_cadres)) +\n  scale_x_log10(\n    limits = c(10000, 70000), breaks = seq(10000, 60000, 10000),\n    labels = dollar_format(prefix = \"\", suffix = \" €\", \n                           big.mark = \" \", accuracy = 1)) + \n  scale_y_continuous(\n    limits = c(0, 0.3), breaks = seq(0, 0.3, 0.05), \n    labels = percent_format(accuracy = 1))\n\n\n\n\n\n\n\n\n28.4.1.3 Modifier les échelles de couleur\nUn cas particulier de la section précédente porte sur les échelles de couleur, utilisés principalement avec les attributs color (pour la couleur des points ou des courbes) et fill (pour le remplissage des barres dans un histogramme). Il existe un grand nombre d’échelles de couleur, utilisables avec les fonctions scale_color_&lt;paramétrage&gt; et scale_fill_&lt;paramétrage&gt; :\n\n\nscale_color_brewer et scale_fill_brewer permettent d’utiliser les nombreuses échelles de couleur du package RColorBrewer (plus d’informations ici) ;\n\nscale_color_grey et scale_fill_grey permettent d’utiliser des échelles de gris ;\n\nscale_color_gradient et scale_fill_gradient permettent d’utiliser des gradients de couleur personnalisés ;\n\nscale_color_viridis et scale_fill_viridis permettent d’utiliser les échelles de couleur du package viridis. Ces échelles de couleur présentent deux grands avantages : elles conviennent aux daltoniens et restent lisibles lorsque le graphique est imprimé en noir et blanc (plus d’informations sur viridis dans cette vignette).\n\nVoici un exemple avec le graphique précédent, dans lequel on indique que la variable représentée sur l’axe des ordonnées est exprimée en pourcentage, de même que la variable indiquant la couleur des points. On choisit par ailleurs une autre échelle de couleur.\n\nlibrary(viridis)\nggplot(data_iris_paris_2017) + \n  geom_point(aes(x = niveau_vie_median, y = taux_chomage, color = part_cadres)) +\n  scale_x_log10(limits = c(10000, 70000), breaks = seq(10000, 60000, 10000)) +\n  scale_y_continuous() +\n  scale_color_viridis()\n\n\n\n\n\n\n\n\n28.4.1.4 Modifier la légende\nLes fonctions guide() et guides() permettent de modifier finement la légende. Les guides peuvent être spécifiés dans chaque scale ou dans une instruction guides(). Voici un exemple :\n\nggplot(data_iris_paris_2017) + \n  geom_point(aes(x = niveau_vie_median, y = taux_chomage, color = part_cadres)) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_color_viridis(labels = scales::percent) +\n  guides(color=guide_colorbar(direction=\"horizontal\",\n                           title.position=\"top\",\n                           label.position=\"bottom\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n28.4.2 Utiliser un thème\n\n28.4.2.1 Utiliser un thème existant\nLorsqu’on veut donner une apparence homogène à un grand nombre de graphiques, il est préférable de définir un thème plutôt que de personnaliser tous les graphiques avec des options identiques. Il existe dans ggplot2 des thèmes prédéfinis que l’on peut utiliser facilement. Par exemple : theme_minimal(), theme_classic(), theme_bw(), theme_dark()… Des packages externes permettent d’enrichir cette collection de thèmes, par exemple ggthemes ou hrbrthemes.\nPour utiliser un thème prédéfini, il suffit d’utiliser la fonction theme_set()comme ceci :\n\ntheme_set(theme_minimal())\n\n\n28.4.2.2 Définir un thème personnalisé\nLa fonction theme() permet de créer des templates, c’est-à-dire de définir tout ce qui n’est pas lié directement aux données sur un graphique, notamment la position, la taille, la couleur et la police des éléments textuels (légende, titres du graphique et des axes), ainsi que la couleur des grilles primaires et secondaires du graphique. La définition d’un thème personnalisé dépasse largement le cadre de la présente fiche, mais doit vous intéresser si vous devez produire des graphiques avec une apparence homogène. Voici quelques références pour les utilisateurs intéressés :\n\nune introduction en français à la définition des thèmes ;\nle chapitre 9 de R Graphics Cookbook (en anglais) aborde la question des thèmes ;\nl’addin RStudio ggThemeAssist est très utile pour se familiariser avec tous les éléments personnalisables des thèmes.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Faire des graphiques avec `ggplot2`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_graphiques.html#exporter-un-graphique",
    "href": "03_Fiches_thematiques/Fiche_graphiques.html#exporter-un-graphique",
    "title": "28  Faire des graphiques avec ggplot2",
    "section": "\n28.5 Exporter un graphique",
    "text": "28.5 Exporter un graphique\nLa fonction ggsave() de ggplot2 permet d’exporter des graphiques dans un fichier externe. Les formats recommandés sont le pdf et le png. Il est néanmoins possible d’utiliser d’autres formats : eps, ps, jpeg, tiff, bmp, svg… Les options width et height contrôlent la taille du graphique. Attention, par défaut, ces paramètres correspondent à la taille de la fenêtre graphique de R, en bas à droite. Il est probable que les valeurs par défaut de ces paramètres ne vous conviennent pas, voire modifient le message de votre graphique ; n’hésitez pas à leur donner des valeurs adaptées.\nVoici un exemple :\n\np &lt;- ggplot(data_iris_paris_2017) +\n  geom_point(aes(x = niveau_vie_median, y = taux_chomage, color = part_cadres)) +\n  labs(title=\"Taux de chômage par iris à Paris en fonction du niveau de vie médian\",\n       x=\"Taux de chômage\",\n       y=\"Niveau de vie médian\",\n       color = \"Part de cadres parmi les actifs\",\n       caption=\"Sources : Filosofi 2017, RP 2017\")\nggsave(\"dossier/export/graphiques/graphique_iris_paris.pdf\", p, width=12, height = 5)",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Faire des graphiques avec `ggplot2`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_graphiques.html#ggplot2Ressources",
    "href": "03_Fiches_thematiques/Fiche_graphiques.html#ggplot2Ressources",
    "title": "28  Faire des graphiques avec ggplot2",
    "section": "\n28.6 Pour en savoir plus",
    "text": "28.6 Pour en savoir plus\n\n\nLa documentation officielle (en anglais) de ggplot2 est très complète et accessible en ligne.\nUne “antisèche” (en français) résumant en deux pages l’ensemble des fonctions et arguments et disponible ici.\nLes parties Data visualisation et Graphics for communication de l’ouvrage en ligne R for data science, de Hadley Wickham, sont une très bonne introduction à ggplot2.\n\nPartie ggplot2 de l’introduction à R et au tidyverse ;\n\nPartie ggplot2 de la formation à R du Ministère de la Transition écologique et solidaire.\nPlusieurs ouvrages, toujours en anglais, abordent en détail l’utilisation de ggplot2 :\n\n\nggplot2 : Elegant Graphics for Data Analysis de Hadley Wickham ;\n\nR Graphics Cookbook de Winston Chang. Le site associé à ce dernier ouvrage comporte aussi pas mal d’exemples et d’informations intéressantes ;\n\n\nEnfin, si ggplot2 présente déjà un très grand nombre de fonctionnalités, il existe aussi un système d’extensions permettant d’ajouter des geometries, des thèmes, etc. Le site ggplot2 extensions est une très bonne ressource pour les parcourir et les découvrir, notamment grâce à sa galerie.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Faire des graphiques avec `ggplot2`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html",
    "title": "29  Produire des documents avec R Markdown",
    "section": "",
    "text": "29.1 Tâches concernées et recommandations\nL’utilisateur souhaite produire avec R des documents contenant à la fois du texte, des extraits de code R et les résultats de l’exécution de programmes R.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#tâches-concernées-et-recommandations",
    "title": "29  Produire des documents avec R Markdown",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nIl est conseillé d’utiliser le package rmarkdown qui permet de produire des documents avec R en utilisant la syntaxe R Markdown ;\nPour aller plus loin et s’exercer sur R Markdown, il est recommandé de lire le chapitre sur R Markdown dans la formation Travail collaboratif avec R.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCertains exemples de cette fiche utilisent les données disponibles dans le package doremifasolData ; vous ne pourrez reproduire ces exemples que si ce package est installé sur la machine sur laquelle vous travaillez. Si vous ne savez pas si ce package est déjà installé, consultez la fiche Comment utiliser la documentation utilitR.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#présentation-générale-de-r-markdown",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#présentation-générale-de-r-markdown",
    "title": "29  Produire des documents avec R Markdown",
    "section": "\n29.2 Présentation générale de R Markdown\n",
    "text": "29.2 Présentation générale de R Markdown\n\nR Markdown est une extension de R qui se présente sous la forme d’un package. R Markdown permet de produire des documents texte en y intégrant nativement des morceaux de code R (pour le rendre public, pour générer des sorties…). R Markdown permet de fluidifier le processus de rédaction d’une publication, en réduisant fortement le nombre de gestes manuels nécessaires pour inclure des graphiques ou du code dans un document : plutôt qu’avoir un code SAS ou Stata ayant généré des sorties Excel/Calc intégrées dans un document Word ou transformées en table LaTeX, on dispose d’un unique document-source qui contient à la fois le texte et les codes qui produisent les sorties du document final. L’utilisation de R Markdown facilite la production de publications reproductibles.\n\n29.2.1 De Markdown à R Markdown\n\nMarkdown est un système d’édition doté d’une syntaxe simplifiée souvent utilisé pour faire de la documentation de projet. Le format est utilisé sur de nombreux sites internet, notamment Gitlab et Stackoverflow. L’extension de ce type de fichier est .md. Markdown présente deux avantages. D’une part, il est facile d’inclure des blocs de code informatique et des équations mathématiques dans un document Markdown. D’autre part, le formatage de blocs de code est simple et très bien fait (et beaucoup plus léger qu’en LaTeX par exemple).\nComme son nom l’indique, R Markdown permet de lier du texte (Markdown) avec du code R. A la différence d’un fichier Markdown classique, un document R Markdown peut à la fois présenter et exécuter du code R, et peut donc intégrer facilement les sorties de ce code (graphiques, tableaux, cartes…). La présente documentation est ainsi entièrement rédigée avec R Markdown, comme on peut le voir en jetant un coup d’oeil au code source de cette fiche.\nIl est toutefois possible de produire un document R Markdown sans exécuter les codes R qu’il contient (comme dans un Markdown classique). En outre, R Markdown propose de plus en plus de langages pour aller au-delà de R, parmi lesquels Python, SQL, C++…\n\n\n\n\n\n\nNote\n\n\n\nR Markdown est une alternative aux notebooks Jupyter qui permettent de lier du code et du texte. Du point de vue de la reproductibilité des résultats, les fichiers R Markdown sont préférables car l’exécution du code dans le document est linéaire alors que les blocs de code des notebook jupyter peuvent être exécutés dans le désordre.\n\n\n\n29.2.2 Que peut produire R Markdown ?\nIl y a une grande liberté dans les formats de sortie disponibles pour un fichier Markdown. Parmi les principaux types de sorties :\n\nDes rapports ou articles en pdf, html, doc, odt, etc. ;\nDes présentations sous forme de slides pdf ou html ;\nDes sites web (comme celui associé à cet ouvrage ou comme la page d’accueil d’utilitR).\n\n\n\n\n\n\n\nNote\n\n\n\nPour la génération d’un document au format pdf, vous devez avoir une installation fonctionnelle de LaTeX sur votre système. Si ça n’est pas le cas, le package tinytex de Yihui Xie vise à faciliter l’installation d’une distribution LaTeX minimale quel que soit le système d’exploitation de votre machine. Pour l’utiliser il vous faut d’abord installer le package tinytex, puis lancer l”installation de LaTeX (prévoir un téléchargement d’environ 200Mo). Voici les instructions à exécuter :\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex() \n\nSous AUS, il n’est pas possible d’installer ce package car il a besoin d’internet. Une distribution MiKTeX (plus riche que tinytex) est disponible et permet de compiler en pdf.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl existe une manière d’obtenir une sortie en pdf qui n’implique pas de passer par LaTeX grâce au package pagedown (présentation plus bas). Par l’intermédiaire de Google Chrome ou Chromium (sa version open-source), pagedown transforme directement le html en pdf. Dans ce cas, il faut ajouter la ligne knit : pagedown::chrome_print dans l’en-tête du document (documentation).\n\n\n\n29.2.3 Pourquoi utiliser R Markdown ?\nR Markdown présente plusieurs avantages pour la production de documents statistiques :\n\nL’utilisation de R Markdown amène l’utilisateur à se préoccuper de la reproductibilité des productions statistiques.\nLa dissociation du fond et de la forme du document permise par R Markdown a de nombreuses vertus, parmi lesquelles :\n\nelle allège les mises à jour ou correctifs du texte ou du code générant des sorties ;\nelle donne de la visibilité au code ayant généré les résultats et illustrations ;\nil est facile de changer le format du document final (par exemple produire un fichier html plutôt que pdf), et la gestion de multiples formats de publication est peu coûteuse (puisque le document source reste le même pour tous les formats).\n\n\nIl est possible de mettre des commentaires partout dans le document, ce qui permet de détailler les résultats et les choix de méthode, et facilite le suivi de l’analyse ou l’appropriation par une personne extérieure.\nLa syntaxe R Markdown a été pensée pour avoir deux grandes qualités :\n\nElle est simple et rapide à prendre en main ; elle est notamment beaucoup plus simple que la syntaxe html ;\nElle est très légère visuellement, ce qui fait qu’un code brut R Markdown reste lisible même pour des personnes qui ne connaissent pas la syntaxe (contrairement à LaTeX…).\n\n\nLorsqu’on associe R Markdown et un outil de versioning tel que Git, les modifications successives apportées au document sont facilement traçables, ce qui permet de :\n\nmaîtriser l’évolution du document, par exemple sur la succession de corrections, apportées tout au long d’un circuit de relecture ;\nne pas multiplier les versions d’un même document ;\ncombiner facilement les modifications apportées par plusieurs utilisateurs.\n\n\n\nR Markdown est bien intégré à RStudio. Les extensions .Rmd sont reconnues automatiquement par RStudio ce qui permet d’avoir de l’autocomplétion également lors de la rédaction de blocs de texte, la possibilité d’exécuter des bouts de code uniquement lors d’une phase d’exploration…\n\n29.2.4 Comment fonctionne R Markdown ?\n\n29.2.4.1 Dissociation du fond et de la forme\nR Markdown dissocie le fond et la forme du document, contrairement aux logiciels de type Office qui fonctionnent selon le principe du WYSIWYG  (What you see is what you get). Le fichier .Rmd contient donc à la fois le texte brut et des commandes de mise en forme. On parle de WYSIWYW (What you see is what you want). Le fichier est composé de texte brut et éventuellement de code informatique, et doit être compilé pour produire le fichier de sortie. Le document final ne peut être visualisé qu’après compilation. A noter que les illustrations (graphiques, tableaux, cartes) peuvent être générées dynamiquement à partir des blocs de code R intégrés dans le document Rmd, ou insérées par des liens.\nLes connaisseurs du format LaTeX reconnaîtront une grande proximité entre LaTeX et Markdown. En effet, dans les deux cas, la mise en forme est appelée par des commandes spéciales. Toutefois, Markdown se distingue de LaTeX par des commandes de mise en forme beaucoup plus légères. Par exemple, il faut utiliser la fonction \\textit{italique} en LaTeX pour mettre du texte en italique, alors qu’en Markdown il suffit de mettre le texte entre deux astériques *italique*.\n\n29.2.4.2 Comment R Markdown produit-il un document ?\nLa compilation d’un document R Markdown se déroule en deux étapes qui sont gérées de manière interne par le package rmarkdown :\n\nTransformation du .Rmd (R Markdown) en .md (Markdown) : le package knitr se charge de transformer le texte et les sorties R en un document markdown standard.\nTransformation du .md en un format de sortie standard (html, pdf, etc.) : tâche effectuée par le logiciel nommé pandoc. Ce dernier est automatiquement installé lorsqu’on a installé le package rmarkdown, il n’y a donc pas à le télécharger.\n\nPour obtenir la sortie désirée d’un document R Markdown on peut utiliser deux méthodes :\n\nPour les usages courants, le bouton Knit de RStudio convient bien :\n\n\nPour des processus de production des fichiers plus complexes, il est possible d’exécuter la commande rmarkdown::render pour générer de manière automatique un fichier R Markdown.\n\n\n\n\n\n\n\nTip\n\n\n\nPour assurer la reproductibilité des résultats, le document doit pouvoir être généré par chacun des membres de l’équipe projet. Voici quelques bonnes pratiques pour y parvenir :\nTout comme pour un script R, les ressources (images, données…) doivent être appelées via des chemins d’accès relatifs. Par défaut, le dossier de travail de travail est celui où se situe le document. Il faut donc utiliser des chemins relatifs, et non absolus :\n- **A ne pas faire :** inclure un lien vers `D:/projets_Gitlab/my_project/donnees/mesdonnees.csv` ;\n- **mais faire plutôt :** inclure un lien vers `./donnees/mesdonnees.csv` ;\nSi un fichier R Markdown se situe dans l’arborescence d’un R Project, il faut faire attention à la question du chemin relatif. Il peut être pratique de fixer comme chemin de départ, l’emplacement du R Project. Les avantages et inconvénients d’un tel choix sont développés ici\nQuelle que soit la configuration, l’emplacement des ressources doit respecter une arborescence connue et utilisée par tous.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLes fichiers .Rmd s’intègrent bien avec git car il s’agit de fichiers texte. En revanche, il faut éviter de mettre sous contrôle de version les fichiers de sortie (pdf, html). Pour ce faire, on peut ajouter au fichier .gitignore les extensions associées, par exemple *.pdf ou *.html",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#comment-construire-un-document-r-markdown",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#comment-construire-un-document-r-markdown",
    "title": "29  Produire des documents avec R Markdown",
    "section": "\n29.3 Comment construire un document R Markdown ?",
    "text": "29.3 Comment construire un document R Markdown ?\n\n29.3.1 Anatomie de la structure d’un document R Markdown\n\nUn document R Markdown comprend deux parties principales :\n\nL’en-tête (YAML header) qui gère les éléments de style ;\nLe contenu qui gère le fond et permet d’alterner librement texte et code.\n\n\n29.3.1.1 L’en-tête\nL’en-tête d’un document R Markdown (parfois appelé YAML header) est délimité par deux lignes de pointillés et contient les métadonnées du document (titre, auteurs, options générales de mise en page…). Il contient au minimum le titre du document et le format de sortie. Il peut être enrichi d’autres champs pour modifier certaines métadonnées (par exemple la date) ou le style du document compilé. Voici un exemple d’en-tête :\n--- \ntitle : \"Produire des documents avec R Markdown\"\nauthor : \"Daffy Duck\"\ndate : \"2025-04-22\"\noutput : \n  html_document :\n    keep_md : true\n    self_contained : true\nbibliography : book.bib\ndescription : \"Un document où je révèle que je ne suis pas un canard\"\n---\nLes éléments obligatoires sont présents (titres et format de sortie, dans le cas un livre) mais de nombreux champs supplémentaires peuvent être ajoutés. Par exemple, les références bibliograhiques sont listées dans un fichier séparé et c’est R Markdown lui-même qui se charge de la mettre en forme et de la lier à des références dans le texte.\n\n29.3.1.2 Corps d’un document R Markdown\n\nLe corps d’un document R Markdown comprend deux types de blocs, qu’on peut alterner librement :\n\nDes blocs de texte brut mis en forme selon la syntaxe markdown ;\nDes blocs de code R (appelés chunks) encadrés par les balises ``` :\n\nces chunks peuvent être nommés (il est même recommandé de le faire) ;\ndes options peuvent être spécifiées. Ces options (détaillées plus bas) permettent par exemple de ne pas faire figurer l’output du code dans le document final ou inversement de ne montrer que l’output du code et non le code l’ayant généré.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl est possible d’ajouter en dessous de l’en-tête un tout premier chunk nommé setup, qui permet de définir des options par défaut pour tous les chunks du document. Par exemple, on peut souhaiter par défaut ne pas afficher le code R de chaque bloc dans le document final.\n\n\n\n29.3.2 Ecrire des blocs de texte en R Markdown\n\nLe corps d’un document contient presque toujours des blocs de texte qui suivent la syntaxe Markdown. Voici un exemple :\nCeci est du texte avec *de l'italique* et **du gras**.\n\nOn peut définir des listes à puces :\n\n- premier élément\n- deuxième élément\nCe code génèrera le texte mis en forme suivant :\n\nCeci est du texte avec de l’italique et du gras.\nOn peut définir des listes à puces :\n\npremier élément\ndeuxième élément\n\n\nOn peut définir des titres de différents niveaux en faisant débuter une ligne par un ou plusieurs # :\nQuand des titres ont été définis, si vous cliquez sur l’icône Show document outline totalement à droite de la barre d’outils associée au fichier R Markdown, une table des matières générée automatiquement à partir des titres s’affiche et vous permet de naviguer facilement dans le document :\nLa syntaxe Markdown permet d’autres mises en forme, comme la possibilité d’insérer des liens ou des images. Par exemple, le code suivant :\n[Exemple de lien](https://example.com)\nPrendra la forme suivante, une fois compilé :\n\nExemple de lien\n\nDans RStudio, le menu Help &gt; Markdown quick reference donne un aperçu plus complet de la syntaxe.\nÀ partir de la version 1.4, RStudio propose de nombreux outils pour faciliter l’édition de fichiers R Markdown. La principale innovation est l’éditeur visuel de Markdown, accessible grâce au bouton\n situé en haut à droite de l’éditeur, qui propose une pré-visualisation du document compilé en live mais aussi des fonctionnalités qui facilitent l’écriture de Markdown (correcteur orthographique amélioré, ajout de citations bibliographiques facilité, plus de raccourcis claviers, etc.).\n\n\nPrésentation de l’éditeur visuel de RStudio\n\n\n\n\n\n\n\nTip\n\n\n\nL’addin Remedy est également pratique et facilite l’écriture de fichiers R Markdown grâce à un menu qui permet de cliquer directement sur le type de balise désiré :\n\n\nPrésentation de l’addin disponible sur github\n\nPour l’installer\n\nremotes::install_github(\"ThinkR-open/remedy\")\n\nPour compter le nombre de mots dans un fichier .Rmd, on peut utiliser le très pratique addin nommé Wordcountaddin :\n\ndevtools::install_github(\"benmarwick/wordcountaddin\")\n\n\n\n\n29.3.3 Insérer un résultat R dans du texte\nPour renforcer la reproductibilité d’un document R Markdown, il est possible de faire appel à R pour insérer des résultats dans du texte. C’est le principe du inline code dans R Markdown.\nIl suffit, pour cela, d’adopter la structure suivante : `r 2+2` . Le résultat sera automatiquement inséré par R en substitution de l’expression.\n\n2 + 2 = `r 2+2`\n\ndeviendra ainsi\n\n2 + 2 = 4\n\n\n29.3.4 Ecrire des blocs de code\n\n29.3.4.1 Insérer un bloc de code\nOutre du texte libre au format Markdown, un document R Markdown peut également contenir du code R. Celui-ci est inclus dans des blocs (chunks) délimités par la syntaxe suivante :\nVous pouvez utiliser le menu Code &gt; Insert Chunk de RStudio ou utiliser le raccourci clavier Ctrl+Alt+I pour automatiquement insérer un bloc R à remplir. Quand votre curseur se trouve dans un bloc, vous pouvez saisir le code R que vous souhaitez, l’exécuter, utiliser l’autocomplétion, etc… exactement comme si vous vous trouviez dans un script R. Vous pouvez également exécuter l’ensemble du code contenu dans un bloc à l’aide du raccourci clavier Ctrl+Maj+Entrée.\n\n29.3.4.2 Exécuter des blocs de code\nDans RStudio les blocs de code R sont en général affichés avec une couleur de fond légèrement différente pour les distinguer du reste du document. En haut à droite du bloc, des boutons sont disponibles si on désire :\n\najouter des options au bloc ;\nexécuter tous les blocs avant celui-ci ;\nexécuter ce bloc exclusivement.\n\nLorsque le document est compilé au format HTML, PDF ou docx, chaque bloc est exécuté tour à tour, et le résultat inclus dans le document final, qu’il s’agisse de texte, d’un tableau ou d’un graphique. Les blocs sont liés entre eux, dans le sens où les données importées ou calculées dans un bloc sont accessibles aux blocs suivants. On peut donc aussi concevoir un document R Markdown comme un script R dans lequel on aurait intercalé des blocs de texte au format Markdown.\n\n\n\n\n\n\nNote\n\n\n\nLe bouton Knit ou le raccourci clavier CTRL+SHIFT+K lancent automatiquement l’exécution du code R dans une nouvelle session. Cela signifie que les objets appartenant à l’environnement global de l’utilisateur sont ignorés, par défaut. C’est une bonne pratique parce qu’elle aboutit à des documents plus reproductibles.\nPar défaut, la commande rmarkdown::render n’utilise pas cette option. Si le fichier R Markdown est compilé en utilisant rmarkdown::render, il est recommandé d’utiliser la syntaxe rmarkdown::render(..., envir = new.env())\n\n\n\n\n\n\n\n\nTip\n\n\n\nPar défaut, dans RStudio, le résultat du bloc de code ne s’affiche pas dans la console mais sous le bloc, dans l’interface visuelle du document. Si vous n’appréciez pas ce fonctionnement, vous pouvez le modifier de la façon suivante : aller dans le panneau Tools &gt; Global Options..., onglet R Markdown, puis décocher l’option Show output inline for all R Markdown documents.\n\n\n\n29.3.4.3 Définir les options des blocs de code\nIl est possible (et recommandé) de nommer un bloc en utilisant un label avant les options. Le nom du bloc se place dans la première ligne du bloc, juste après la lettre r : {r nom_du_bloc}.\nUn point important est que deux blocs ne peuvent pas porter le même nom, il faut donc être rigoureux dans le choix du nom. Nommer les blocs de code présente plusieurs avantages :\n\nen cas d’erreur lors de la compilation du document, R donne le nom du bloc qui provoque l’erreur. Avec des blocs non nommés, on a pour seule information unnamed-chunk-XX ce qui rend difficile d’identifier la source de l’erreur dans un document très long ;\nle bloc peut facilement être retrouvé dans l’arborescence RStudio (voir capture d’écran).\n\nOutre un nom, on peut passer à un bloc une série d’options sous la forme option = valeur. Ces options permettent de contrôler le comportement du bloc lors de la compilation du document. Les blocs de code acceptent de nombreuses options. Les principales options sont les suivantes :\n\n\nOption\nValeurs possibles\nValeur par défaut\n\n\n\necho\nTRUE/FALSE\nTRUE\n\n\neval\nTRUE/FALSE\nTRUE\n\n\ninclude\nTRUE/FALSE\nTRUE\n\n\nresults\n'hide'/'asis'/'markup'/'hold'\n'markup'\n\n\nerror\nTRUE/FALSE\nTRUE\n\n\nwarning\nTRUE/FALSE\nTRUE\n\n\nmessage\nTRUE/FALSE\nTRUE\n\n\n\nVoici un exemple de bloc avec un nom et des options :\nDans ce bloc, l’option echo = FALSE implique que le code R n’est pas inséré dans le document compilé, et que seul le résultat du code est visible ; l’option warning = TRUE implique que les éventuels avertissements générés par l’exécution du code sont insérés dans le document compilé.\nLes options permettent également de contrôler la nature de la sortie R. En particulier, les options fig.height et fig.width permettent de définir la taille de la sortie. Par exemple, le bloc suivant génèrera un graphique plus large que haut :\nalors que le bloc suivant génèrera une figure aux dimensions différentes :",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#paramétrer-un-document-r-markdown",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#paramétrer-un-document-r-markdown",
    "title": "29  Produire des documents avec R Markdown",
    "section": "\n29.4 Paramétrer un document R Markdown\n",
    "text": "29.4 Paramétrer un document R Markdown\n\n\n29.4.1 Personnaliser les options de l’en-tête\nL’en-tête fonctionne selon un système de clé-valeur indenté, format appelé YAML. Le nom de l’option (clé) est suivi de : qui permet de spécifier sa valeur (par exemple true).\nAu premier niveau (sans indentation) se trouvent les options générales, par exemple le titre ou l’auteur du document. S’il est nécessaire d’amender le comportement par défaut de l’option générale, on introduit un niveau d’indentation (une tabulation ou quatre espaces).\nLa personnalisation du document généré se fait en modifiant des options dans le préambule du document. RStudio propose également une petite interface graphique permettant de changer ces options plus facilement. Pour cela, cliquez sur l’icône en forme d’engrenage à droite du bouton Knit et choisissez Output Options :\nLorsque vous changez des options par cette interface, RStudio va en fait modifier le préambule de votre document. Ainsi, si vous choisissez d’afficher une table des matières et de modifier le thème de coloration syntaxique, votre en-tête va devenir quelque chose comme :\n---\ntitle : \"Test R Markdown\"\noutput :\n   html_document : \n     highlight : kate\n     toc : yes\n---\nVous pouvez également modifier les options directement en éditant le préambule. À noter qu’il est possible de spécifier des options différentes selon les formats. Dans ce cas, le format représente le premier niveau d’indentation et les options de celui-ci le second. Par exemple,\n---\ntitle : \"Test R Markdown\"\noutput :\n  html_document : \n    highlight : kate\n    toc : yes\n  pdf_document : \n    fig_caption : yes\n    highlight : kate\n---\nLa liste complète des options possibles est présente sur le site de la documentation officielle (très complet et bien fait) et sur l’antisèche et le guide de référence, accessibles depuis RStudio via le menu Help puis Cheatsheets.\n\n29.4.2 Gestion automatique de la bibliographie\nParmi les tâches les plus pénibles dans un document WYSIWYG (What you see is what you get), la bibliographie tient une place de premier choix. Les utilisateurs de\n connaissent le temps que peut faire gagner un module adapté de gestion des ressources bibliographiques, à savoir bibtex. Un module adapté de bibliographie repose sur des métadonnées avec un système ressemblant au format JSON, c’est-à-dire des champs (par exemple author) associé à des valeurs au format prédéfini (par exemple Dumas, Alexandre).\nLe format bibtex constitue un standard dans le domaine. Il s’agit de définir le type de publication (livre, article, rapport, etc.) qui déterminera la manière dont sera citée le document dans la norme bibliographique adoptée et les champs informatifs (auteur, année, etc.). Le principal avantage d’une gestion bibliographique avec bibtex est que formattage de la référence bibliographique n’est pas fait par l’utilisateur mais est fait automatiquement en fonction d’une norme qui met en forme les champs de la référence bibliographique. Ce sont ainsi des heures pénibles de travail économisées. Une référence bibliographique prend la forme suivante :\n@book{dumascomte,\n  title={Le Comte de Monte-Cristo tome 1},\n  author={Dumas, Alexandre},\n  volume={1},\n  year={2011},\n  publisher={Le Livre de Poche}\n}\nUn fichier séparé, généralement au format .bib, centralise les références. L’ajout du champ bibliography au yaml permet à R Markdown d’automatiquement construire la bibliographie à la fin du document.\n\n\n\n\n\n\nTip\n\n\n\nLa bibliographie se trouve généralement à la fin du document, sauf si un fichier de style [lien vers partie template] en modifie l’emplacement. En général, on ajoute ainsi un titre de section du type # Référencesà la fin du .Rmd pour dissocier la bibliographie de la fin du texte.\n\n\ndonnera l’output suivant :\n\n\n\n\n\n\nTip\n\n\n\nIl est rare qu’il soit nécessaire de devoir écrire soi-même la bibliographie lorsqu’on écrit un article académique. En effet, on trouve souvent des références au format bibtex sur les bases de données scientifiques, notamment sur google scholar ou sur le site ideas.repec (pour les articles d’économie). Sur scholar, pour récupérer une citation, on clique sur le guillemet puis sur bibtex :\nCela ouvre alors un onglet vierge avec la citation en question, en l’occurrence\n@book{dumascomte,\n  title={Le Comte de Monte-Cristo tome 1},\n  author={Dumas, Alexandre},\n  volume={1},\n  year={2011},\n  publisher={Le Livre de Poche}\n}",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#intégrer-des-tableaux-dans-un-document-r-markdown",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#intégrer-des-tableaux-dans-un-document-r-markdown",
    "title": "29  Produire des documents avec R Markdown",
    "section": "\n29.5 Intégrer des tableaux dans un document R Markdown\n",
    "text": "29.5 Intégrer des tableaux dans un document R Markdown\n\nIl existe plusieurs manières de faire des tableaux avec R Markdown. L’écosystème dans ce domaine n’est pas encore stabilisé et évolue rapidement, notamment du fait du développement récent du package gt.\nL’approche la plus appropriée dépend de la structure du besoin :\n\nPour une table de présentation simple, le tableau peut être construit en Markdown\n\nPour une table construite à partir de données, des packages spécialisés sont plus adaptés. L’utilisation de ces derniers permet de ne pas écrire les valeurs numériques et améliore ainsi la reproductibilité des tables construites.\n\n\n\n\n\n\n\nTip\n\n\n\nPour des tables simples, notamment de statistiques agrégées, la fonction kable (du package knitr) et le package d’extension kableExtra peuvent être utilisés.\nPour des tables plus complexes, les packages gt et flextable peuvent être mobilisés.\nPour des tables de régressions économétriques, le package stargazer peut être utilisé.\n\n\n\nPour aller plus loin, il est possible de se référer à ce tutoriel\n\n29.5.1 Construction d’une table Markdown\n\nLe balisage des tables en Markdown est très intuitif. Il suffit d’adopter la structure suivante\n| Titre colonne 1    | Titre colonne 2  |\n| ------------------ | ---------------- |\n| Texte              | Encore du texte  |\n| Texte              | Encore du texte  |\net on obtient ainsi le tableau mis en forme\n\n\nTitre colonne 1\nTitre colonne 2\n\n\n\nTexte\nEncore du texte\n\n\nTexte\nEncore du texte\n\n\n\nL’alignement des colonnes n’est pas obligatoire mais est une bonne pratique pour rendre plus lisible le fichier source.\nDes éléments supplémentaires de stylisation sont disponibles dans le guide de la syntaxe Markdown.\n\n29.5.2 Une table basique en R Markdown\n\nSupposons, par exemple, qu’on désire faire la liste de toutes les communes dont le libellé contient “Montreuil-sur”. On commence par récupérer les données du Code officiel géographique 2019, disponibles dans le package doremifasolData. On utilise ensuite le package stringr pour repérer les communes dont le libellé contient “Montreuil-sur” (l’usage de ce package est détaillé dans la fiche [Manipuler des données textuelles]).\n\nlibrary(doremifasolData)\nlibrary(stringr)\n# Charger les données du COG\ncog_com_2019 &lt;- doremifasolData::cog_com_2019\nmontreuil &lt;- \n  cog_com_2019[stringr::str_detect(cog_com_2019$libelle, \"Montreuil-sur\"), c(\"libelle\",\"com\")]\n\nDans ce cas, on peut représenter les résultats avec la fonction kable du package knitr :\n\nlibrary(knitr)\nkable(montreuil)\n\nLe package kableExtra fournit un ensemble de fonctions de style pour personnaliser le tableau. Une documentation complète peut être trouvée ici.\n\n29.5.3 Des tables plus complexes\n\nLe package gt peut être mobilisé pour construire des tables plus complexes, notamment proposant des éléments à cheval sur plusieurs lignes ou plusieurs colonnes.\nPour constuire des tables de régression, la référence est le package stargazer. Celui-ci propose une grande variété d’options et de nombreux modèles, issus de packages multiples.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#extensionsrmd",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#extensionsrmd",
    "title": "29  Produire des documents avec R Markdown",
    "section": "\n29.6 Extensions de rmarkdown et modèles de documents",
    "text": "29.6 Extensions de rmarkdown et modèles de documents\nA la création d’un document (File -&gt; New File -&gt; R Markdown), un modèle de document minimal est créé. Des modèles plus riches peuvent être utilisés, à partir de packages comme\n\n\nrticles dont la vocation est de proposer des modèles de journaux académiques, au format pdf\n\npagedown qui offre d’autres modèles et plus de marges de liberté dans l’édition du modèle\n\nDes extensions de R Mardown permettent également de construire un ensemble structuré de documents à partir d’un ensemble de fichiers Markdown ou R Markdown\n\n\nbookdown : ce package permet de construire des sites de documentations ou des livres au format pdf. Le présent ouvrage est, par exemple, construit à partir de ce package ;\n\nblogdown : ce package permet de construire des sites web plus généraux que bookdown. Le site web utilitR est, par exemple, construit à partir de ce package ;\n\npkgdown : ce package permet de construire de manière automatique des sites de documentation de packages.\n\nIl existe également de nombreuses initiatives, par exemple gouvdown, pour proposer des modèles de documents dans l’administration française, harmonisés notamment avec les chartes graphiques de certaines institutions.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#intégrer-des-figures-dans-r-markdown",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#intégrer-des-figures-dans-r-markdown",
    "title": "29  Produire des documents avec R Markdown",
    "section": "\n29.7 Intégrer des figures dans R Markdown\n",
    "text": "29.7 Intégrer des figures dans R Markdown\n\n\n29.7.1 Depuis un fichier image\nIl existe deux manières canoniques d’introduire des graphiques dans un document R Markdown :\n\n\n![Titre de la Figure](monfichier.png) est une syntaxe héritée de Markdown. Elle offre des possibilités de customisation limitée mais elle a l’avantage de ne pas dépendre de R et ainsi, de fonctionner quelque soit le constructeur de document utilisé. Les options supplémentaires s’intègrent dans des accolades ![Titre de la Figure](monfichier.png){width=50%} ;\n\nknitr::include_graphics permet, en conjonction d’options de blocs, d’aller plus loin.\n\n29.7.2 Sans passer par un fichier image\nR Markdown présente l’avantage de pouvoir créer un document sans générer de sorties intermédiaires (tableaux, graphiques…). Si un bloc de code propose d’inséer un graphique dans le document final, le graphique sera automatiquement et directement intégré au document. Par exemple, le bloc suivant permet de représenter l’histogramme des populations communales (avec une échelle logarithmique) à partir des données Filosofi agrégées, disponibles dans le package doremifasolData.\n\nlibrary(doremifasolData)\nlibrary(ggplot2)\nfilosofi_com_2016 &lt;- doremifasolData::filosofi_com_2016\nggplot(data = filosofi_com_2016) +\n  geom_histogram(aes(x = NBPERSMENFISC16, y = ..density..)) +\n  xlab(\"Nombre de personnes\") + ylab(\"Densité\") +\n  scale_x_log10()",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown.html#RessourcesRMarkdown",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown.html#RessourcesRMarkdown",
    "title": "29  Produire des documents avec R Markdown",
    "section": "\n29.8 Pour en savoir plus",
    "text": "29.8 Pour en savoir plus\n\nLes ouvrages Cookbook R Markdown et R Markdown Definitive Guide ;\nLe manuel de référence bookdown ;\nLa partie R Markdown de la formation Travail collaboratif avec R ;\n\nGuide de la syntaxe R Markdown ;\nR Markdown Cheatsheet\n\nCours de Julien Barnier sur le sujet ;\n\nChapitre sur le sujet dans le livre R For Data Science ;\nTutoriel de ThinkR sur les tableaux",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Produire des documents avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html",
    "title": "30  Produire des rapports automatisés avec R Markdown",
    "section": "",
    "text": "30.1 Tâches concernées et recommandations\nL’utilisateur souhaite réaliser l’une des tâches suivantes :",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Produire des rapports automatisés avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#tâches-concernées-et-recommandations",
    "title": "30  Produire des rapports automatisés avec R Markdown",
    "section": "",
    "text": "produire un rapport automatisé, reproductible et facile à actualiser en cas de modification des données ;\nproduire de nombreux rapports automatisés sur un même modèle en faisant varier certains paramètres.\n\n\n\n\n\n\n\nTâche concernée et recommandation\n\n\n\nIl est recommandé d’utiliser R Markdown pour produire ce type de rapports. Si vous ne connaissez pas R Markdown, il est indispensable de lire au préalable la fiche R Markdown.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Produire des rapports automatisés avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#les-rapports-automatisés",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#les-rapports-automatisés",
    "title": "30  Produire des rapports automatisés avec R Markdown",
    "section": "\n30.2 Les rapports automatisés",
    "text": "30.2 Les rapports automatisés\n\n30.2.1 Introduction\nUn rapport automatisé est un document qui contient du texte et des informations (graphiques, tableaux…) produites en exécutant un code, appelé code source. Un rapport automatisé peut prendre plusieurs formes :\n\nune page internet (un fichier html) ;\nun document texte (Word, writer, LaTeX) ;\nune présentation (slides) ;\nune carte.\n\nLes rapports automatisés présentent deux grands avantages. Premièrement, ils sont reproductibles, car le code source contient toutes les instructions nécessaires à la production des informations contenues dans le rapport. Deuxièmement, ils sont faciles à actualiser en cas de modification des données.\nDans le cas de R Markdown, le code source est un fichier texte portant l’extension .Rmd. Il rassemble en un même endroit les instructions de traitement des données, mais aussi les commentaires associés sous forme de texte, des images, des cartes… Le document final (ou output) est produit après une étape de compilation, par exemple en cliquant sur le bouton  Knit  de RStudio.\n\n\n\n\n\n\nNote\n\n\n\nLes instructions de traitement de données sont généralement rédigées en R, mais il est possible d’utiliser R Markdown avec d’autres langages (par exemple python).\n\n\n\n30.2.2 Quelques bonnes pratiques\nCette section détaille les bonnes pratiques à adopter pour réaliser des rapports automatisés. Le principal conseil tient en une phrase : tous les réglages du rapport paramétré doivent être regroupés au début du code source.\n\n30.2.3 Remplir l’en-tête\nLe code source d’un rapport automatisé commence toujours par un en-tête YAML qui doit contenir au minimum un titre et un format de sortie. Voici un exemple d’en-tête :\n---\ntitle: \"Titre du rapport\"\ndate: '22 April 2025'\noutput: pdf_document\nauthor: \"Anne Onyme\"\ndescription: \"Une description vraiment utile\"\n---\nL’en-tête permet de paramétrer finement le document de sortie : format du document, présence d’une table des matières, d’une bibliographie…\nIl existe un nombre considérable d’options, dont certaines sont spécifiques à un format de sortie (pdf, html, …). La liste des options est détaillée sur le site de la documentation officielle de R Markdown et sur l’antisèche et le guide de référence de R Markdown, accessibles depuis RStudio via le menu Help puis Cheatsheets.\n\n\n\n\n\n\nTip\n\n\n\nSi vous souhaitez construire un rapport automatisé que vous utiliserez régulièrement, il est vivement conseillé de prendre le temps de définir un en-tête qui corresponde précisément à ce que vous voulez produire.\n\n\n\n30.2.4 Configurer le fonctionnement de knitr\n\nDans un deuxième temps, il est souhaitable de configurer le comportement par défaut de knitr. La fiche R Markdown détaille les principales options. Dans le cas des rapports automatisés, la configuration de knitr doit porter au minimum sur les deux points suivants :\n\nla position et taille des figures ;\nl’affichage ou non des instructions R dans le fichier de sortie.\n\nLa méthode la plus simple pour configurer knitr consiste à inclure un chunk de configuration au début du fichier R Markdown (juste après l’en-tête). Ce chunk de configuration utilise la fonction knitr::opts_chunk$set() pour définir les options par défaut applicables à tous les chunks du code source. La fonction opts_chunk$set permet de définir de nombreux paramètres, dont la liste complète est disponible sur https://yihui.org/knitr/options. Voici un exemple de chunk de configuration :\n```{r configuration, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE,\n                      warning = FALSE,\n                      message = FALSE,\n                      error = TRUE,\n                      fig.align = \"center\",\n                      out.width = \"75%\")\n```\nDans cet exemple, il est demandé à R Markdown de ne pas inclure les instructions R dans le document de sortie (echo = FALSE). Les warnings et les messages d’informations n’apparaîtront pas non plus dans le fichier de sortie. En revanche, les erreurs apparaîtront (error = TRUE). Les figures sont centrées (fig.align = \"center\") et ont une largeur de 75% de la largeur du texte (out.width = \"75%\").\n\n\n\n\n\n\nTip\n\n\n\nUne attention particulière doit être portée à l’utilisation de l’option cache. L’option cache = TRUE permet de signaler à R Markdown de ne pas ré-exécuter du code n’ayant pas été modifié depuis la dernière compilation. Ceci peut faire gagner beaucoup de temps, notamment pour les chunks d’importation de données volumineuses, mais peut conduire à des comportements non souhaités, notamment lorsque les données que vous utilisez dans votre rapport ont été actualisées. Si vous utilisez l’option cache = TRUE, et si aucun élément du code source n’a été modifié, alors les données ne seront pas réimportées lorsque vous recompilerez le rapport.\n\n\n\n30.2.5 Charger les packages et les données au début du code source\nIl est recommandé d’intégrer au début de votre code source (par exemple juste après la configuration des options de knitr) un chunk qui charge les packages utilisés dans le rapport, puis un chunk qui importe l’ensemble des données manipulées dans le rapport. L’intérêt de cette approche est qu’elle permet de voir facilement quels packages et quelles données sont utilisées par votre rapport.\nDans la mesure du possible, il est préférable d’utiliser des chemins relatifs pour les fichiers (exemple : ./donnees/mesdonnees.csv), plutôt que des chemins absolus (exemple : D:/chemin/vers/les/donnees/mesdonnees.csv).\nA titre d’exemple, les deux chunks qui chargent les packages et les données pourraient ressembler à ceci :\n```{r packages, include = FALSE}\nlibrary(doremifasolData)\nlibrary(data.table)\nlibrary(ggplot2)\n```\n```{r donnees, include = FALSE}\ndonnees &lt;- fread(\"./donnees/mesdonnees.csv\")\n```\n\n\n\n\n\n\nNote\n\n\n\nIl peut arriver qu’il soit difficile d’accéder à des fichiers en utilisant uniquement des chemins relatifs. Une solution de repli consiste à définir le chemin absolu du dossier à un seul endroit dans le code source du rapport, puis à l’utiliser de façon relative dans les fonctions d’importation. Voici un exemple :\n```{r donnees, include = FALSE}\n# Définir UNE SEULE FOIS le répertoire des données\ndossier_donnees &lt;- \"D:/chemin/vers/les/donnees/\"\n\n# Charger les données avec un chemin composé\ndonnees1 &lt;- fread(paste0(dossier_donnees, \"mesdonnees1.csv\"))\ndonnees2 &lt;- fread(paste0(dossier_donnees, \"mesdonnees2.csv\"))\n```\n\n\n\n30.2.6 Nommer les chunks\n\nIl est important de donner un nom à tous les chunks, car cela facilite grandement la résolution des problèmes, en particulier si votre rapport est long. En effet, si la compilation du code source génère une erreur, R Markdown vous indiquera le nom du chunk où se trouve l’erreur. Le nom du chunk se positionne immédiatement avant les éventuelles options, par exemple :\n```{r nom_du_chunk, include = FALSE}\nresultat &lt;- 1 + 1\n```",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Produire des rapports automatisés avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#les-rapports-paramétrés",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#les-rapports-paramétrés",
    "title": "30  Produire des rapports automatisés avec R Markdown",
    "section": "\n30.3 Les rapports paramétrés",
    "text": "30.3 Les rapports paramétrés\n\n30.3.1 Définition\nUn rapport paramétré est une forme particulière de rapport automatisé. Il a donc les mêmes avantages que ce dernier (reproductible et simple à actualiser). Il permet en outre de générer différents outputs à partir du même code source, mais en distinguant les traitements selon un ou plusieurs paramètres définis dans l’en-tête YAML.\nPar exemple, on peut créer un rapport paramétré qui produit des statistiques descriptives sur un département, dont le numéro est un paramètre de l’en-tête YAML du code source. Il suffit alors de changer le numéro du département dans l’en-tête pour produire un rapport sur un autre département, sans modifier le code source.\nPar conséquent, un même modèle de rapport “générique” (appelé template) peut être utilisé pour produire de multiples rapports sur différents territoires, différentes périodes, différents secteurs…\n\n30.3.2 Paramétrer un rapport\nVoici un exemple de situation dans laquelle les rapports paramétrés sont utiles. Un utilisateur souhaite produire pour chaque département français un rapport donnant le nombre de communes de ce département.\nUne première approche consiste à créer un rapport R Markdown par département (fichier_Ain.Rmd, fichier_Aisne.Rmd…). Si elle peut convenir lorsque le nombre de documents à produire est réduit, cette approche n’est clairement pas adaptée lorsqu’il est question de réaliser plusieurs dizaines de rapports.\nUne solution est d’introduire le département comme paramètre du rapport :\n---\ntitle: \"Titre du rapport\"\noutput: html_document\nparams:\n  codeDpt: \"01\"\n---\nIl faut ensuite remplacer dans les différents chunks le code du département par params$codeDpt. Une fois ces 2 modifications (en-tête et chunks) effectués, il n’est plus besoin de changer le département qu’à un seul endroit.\n\n30.3.3 Utiliser un modèle de rapport paramétré\nUne fois que le modèle de rapport paramétré est défini, il est possible de l’utiliser pour publier facilement des rapports. La compilation du code source avec la commande rmarkdown::render(\"rapportParametre.Rmd\") générera le document suivant :\nVous pouvez remarquer que ce rapport porte sur le département “01”, car le paramètre codeDpt prend par défaut la valeur “01” d’après l’en-tête du modèle de rapport. Si l’utilisateur souhaite produire le même type de document pour un autre département, il devra passer explicitement le paramètre params = list(codeDpt = \"XX\") à la fonction render. Par exemple, le code suivant produit un rapport sur le département “02” (l’Aisne).\n\nrmarkdown::render(\n  input = \"rapportParametre.Rmd\", \n  params = list(codeDpt = \"02\")\n)\n\nDans cet exemple, l’output ressemblera à ceci :\n\n30.3.4 Automatiser la génération de rapports paramétrés\nLa dernière étape de l’utilisation des rapports paramétrés consiste à en automatiser la production avec une fonction. Voici comment définir une fonction qui génère automatiquement un rapport à partir du modèle de rapport présenté ci-dessus :\n\nGenererRapport &lt;- function(codeDpt) {\n    rmarkdown::render(\n      input = \"rapportParametre.Rmd\",\n      params = list(codeDpt = codeDpt),\n      envir = new.env(),\n      output_file = paste0(\"Rapport_\", codeDpt, \".html\")\n    )\n}\n\nUne fois la fonction définie, il est possible de générer automatiquement un rapport paramétré pour le département “31” (Haute-Garonne) en exécutant la commande GenererRapport(codeDpt = \"31\"). Par défaut, le fichier de sortie s’appellera Rapport_31.html, mais il est possible de choisir un autre nom avec l’argument output_file.\nEnfin, cette fonction peut être utilisée dans une boucle for ou un appel à la fonction lapply pour produire un grand nombre de rapports.\n\n30.3.5 Un exemple complet de rapport\nLe modèle de rapport paramétré suivant répond au besoin de l’utilisateur. Vous pouvez noter que cet exemple minimal contient tous les éléments d’un rapport paramétré : un en-tête renseigné qui contient la liste des paramètres (il n’y en a qu’un seul : codeDpt), un chunk de configuration, des chunks qui importent les packages et les données, du texte, et des chunks manipulant des données.\n\n\n\n\n\n\nNote\n\n\n\nVous pouvez télécharger le fichier Rmd de cet exemple sur cette page.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Produire des rapports automatisés avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#RessourcesRapportsParam",
    "href": "03_Fiches_thematiques/Fiche_rmarkdown_param_report.html#RessourcesRapportsParam",
    "title": "30  Produire des rapports automatisés avec R Markdown",
    "section": "\n30.4 Pour en savoir plus",
    "text": "30.4 Pour en savoir plus\n\nLa documentation officielle sur les rapports paramétrés avec R Markdown.",
    "crumbs": [
      "Introduction",
      "Produire des sorties avec R",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Produire des rapports automatisés avec `R Markdown`</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html",
    "href": "02_Bonnes_pratiques/01-qualite-code.html",
    "title": "31  Qualité du code",
    "section": "",
    "text": "31.1 Enjeux\nCette partie détaille de manière plus étendue les éléments enseignés dans le cadre d’une formation aux bonnes pratiques construite par l’Insee et dont les supports ont été ouverts à cette adresse.\nLors de l’apprentissage d’un langage, il est assez naturel de voir le code d’une manière très fonctionnelle : on désire réaliser une tâche donnée — par exemple nettoyer des champs textuels — et on va donc assembler dans un script des bouts de code, souvent trouvés sur internet, jusqu’à obtenir un projet qui réalise la tâche voulue. La structure du projet importe assez peu, tant qu’elle permet d’importer et traiter les données nécessaires à la tâche en question.\nSi cette approche flexible et minimaliste fonctionne très bien lors de la phase d’apprentissage, il est malgré tout indispensable de s’en détacher progressivement à mesure qu’on progresse et que l’on peut être amené à réaliser des projets collaboratifs ou amenés à durer dans le temps.\nLorsqu’on travaille avec R, il est important de considérer le code non seulement comme un outil pour effectuer des tâches, mais aussi comme un moyen de communiquer nos méthodes et résultats à d’autres personnes. En adoptant des bonnes pratiques, on améliore la lisibilité et la compréhension d’un code, ce qui facilite la collaboration avec les réutilisateurs du code mais aussi auprès de publics extérieurs, comme les chercheurs qui souhaitent comprendre les traitements mis en oeuvre.\nLa lisibilité et la maintenabilité du code sont des aspects clés pour assurer la qualité d’un projet statistique. Les bonnes pratiques aident à écrire du code clair et structuré, ce qui fait gagner du temps pour s’approprier un code (lisibilité), corriger des erreurs ou apporter des modifications à un code (maintenabilité). Un code étant plus souvent lu qu’écrit1, c’est en effet la phase de maintenance d’un code qui s’avère la plus coûteuse, et non sa rédaction initiale.\nLa réutilisation d’un code ou de productions associées à du code, comme des bases de données, peut être grandement facilitée en adoptant des bonnes pratiques.\nGrâce aux bonnes pratiques, nous pouvons nous assurer que notre travail est transparent et facilement vérifiable. Cette exigence de reproductibilité, notion centrale dans le domaine de la recherche scientifique, s’applique également dans d’autres domaines où la transparence méthodologique est cruciale pour la validité et la fiabilité des résultats. Un code de qualité facilite ainsi la vérification et la reproduction de nos résultats par d’autres personnes. A l’image du processus de revue par les pairs (peer review) dans le domaine scientifique, se développent des revues de code (code review) qui favorisent la production d’un code de qualité.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#adopter-les-standards-communautaires",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#adopter-les-standards-communautaires",
    "title": "31  Qualité du code",
    "section": "\n31.2 Adopter les standards communautaires",
    "text": "31.2 Adopter les standards communautaires\n\n31.2.1 Enjeux\n\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread”\nTidyverse Style Guide\n\nTout comme la correction de la ponctuation peut rendre un texte plus facile à lire, une bonne pratique de codage peut rendre notre code plus facile à comprendre, à maintenir et à réutiliser.\nIl est notamment important de respecter les conventions du langage dans lequel le code est rédigé. Cela peut inclure des normes de formatage telles que l’indentation et la mise en forme, ainsi que des conventions de nommage telles que les noms de variables et de fonctions. En utilisant les conventions standardisées du langage, nous pouvons rendre notre code plus cohérent et plus facile à comprendre pour les autres personnes travaillant dans ce langage.\nIl existe deux guides de référence qui exposent les conventions de la communauté R concernant la qualité du code : le Tidyverse style guide et le Google style guide. Ces guides proposent des conseils sur la façon d’écrire du code clair et structuré en utilisant les bonnes pratiques recommandées pour le langage R. Il est utile de lire les introductions et de se référer ponctuellement à ceux-ci pour s’assurer d’adopter des bonnes pratiques en matière de codage en R.\n\n\n\n\n\n\nNote\n\n\n\nCes deux guides diffèrent sur certaines règles syntaxiques.\nPar exemple, le Tidyverse style guide recommande de ne pas introduire de return en fin de fonction alors que le Google style guide préconise de le faire. Les deux conventions peuvent se défendre et le choix entre les deux revêt une forme d’arbitraire. Par exemple, si on privilégie la lisibilité, il est conseillé d’inclure systématiquement un return dans les fonctions, alors qu’un développeur cherchant la concision n’utilisera pas de return. De même, le choix entre camel case (objets dont les mots sont délimités avec des majuscules comme addValues) dans le Google style guide et snake case (séparation avec des _ comme add_values) proposé par Tidyverse style guide est arbitraire.\nComme il est difficile de donner des arguments objectifs pour privilégier une règle plutôt qu’une autre, il n’est pas impossible de parfois suivre celles du Tidyverse style guide et dans d’autres occasions celles du Google style guide. L’important est plutôt d’être cohérent dans le cadre d’un projet en suivant les mêmes conventions dans l’ensemble des scripts qui le constituent.\n\n\n\n31.2.2 Outils\nPour implémenter de manière automatisée certaines des règles syntaxiques présentes dans les guides, il existe plusieurs types d’outils.\n\nUn linter est un programme qui vérifie que le code est formellement conforme à un certain style guide, et signale les erreurs. En revanche, un linter ne modifie pas directement le code et ne repère pas les erreurs de fond.\nUn formatter est un programme qui reformate un code source pour le rendre conforme à un certain style guide. Par définition, un formatter modifie directement le code.\n\nUn linter se comporte un peu comme un correcteur orthographique d’un traitement de texte dont on aurait désactivé la fonction de remplacement automatique. Le formatter correspond plutôt au correcteur automatique d’un téléphone portable qui corrige automatiquement ce qu’il considère comme des erreurs.\n\n\n\n\n\n\nNote\n\n\n\n\n\nExemples d’erreurs repérées par un linter :\n\nlignes de code trop longues ou mal indentées, parenthèses non équilibrées, noms de fonctions mal construits…\n\n\n\nExemples d’erreurs non repérées par un linter :\n\nfonctions mal utilisées, arguments mal spécifiés, structure du code incohérente, code insuffisamment documenté…\n\n\n\n\n\nDans le cas de R :\n\nle linter à utiliser est le package lintr;\nle formatter à utiliser est le package styler.\n\n\n\n\n\n\n\nTip\n\n\n\nPour que lintr utilise le guide de style tidyverse, il suffit\n\nlintr::use_lintr(type = \"tidyverse\")\n\n\n\nPour utiliser un linter sur l’ensemble des scripts d’un projet R, la commande consacrée est :\n\nlintr::lint_dir()\n\n::warning file=01-qualite-code.qmd,line=272,col=27::file=01-qualite-code.qmd,line=272,col=27,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=02-nommage.qmd,line=30,col=7::file=02-nommage.qmd,line=30,col=7,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=02-nommage.qmd,line=56,col=10::file=02-nommage.qmd,line=56,col=10,[T_and_F_symbol_linter] Use TRUE instead of the symbol T.\n::warning file=02-nommage.qmd,line=60,col=1::file=02-nommage.qmd,line=60,col=1,[object_name_linter] Variable and function name style should match snake_case or symbols.\n::warning file=02-nommage.qmd,line=60,col=2::file=02-nommage.qmd,line=60,col=2,[T_and_F_symbol_linter] Don't use T as a variable name, as it can break code relying on T being TRUE.\n::warning file=02-nommage.qmd,line=63,col=10::file=02-nommage.qmd,line=63,col=10,[T_and_F_symbol_linter] Use TRUE instead of the symbol T.\n::warning file=02-nommage.qmd,line=64,col=7::file=02-nommage.qmd,line=64,col=7,[T_and_F_symbol_linter] Use TRUE instead of the symbol T.\n::warning file=02-nommage.qmd,line=77,col=19::file=02-nommage.qmd,line=77,col=19,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=02-nommage.qmd,line=78,col=69::file=02-nommage.qmd,line=78,col=69,[brace_linter] There should be a space before an opening curly brace.\n::warning file=02-nommage.qmd,line=78,col=69::file=02-nommage.qmd,line=78,col=69,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=02-nommage.qmd,line=83,col=20::file=02-nommage.qmd,line=83,col=20,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=02-nommage.qmd,line=84,col=69::file=02-nommage.qmd,line=84,col=69,[brace_linter] There should be a space before an opening curly brace.\n::warning file=02-nommage.qmd,line=84,col=69::file=02-nommage.qmd,line=84,col=69,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=03-syntaxe.qmd,line=30,col=6::file=03-syntaxe.qmd,line=30,col=6,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=03-syntaxe.qmd,line=30,col=9::file=03-syntaxe.qmd,line=30,col=9,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=30,col=9::file=03-syntaxe.qmd,line=30,col=9,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=03-syntaxe.qmd,line=31,col=0::file=03-syntaxe.qmd,line=31,col=0,[indentation_linter] Indentation should be 2 spaces but is 0 spaces.\n::warning file=03-syntaxe.qmd,line=32,col=6::file=03-syntaxe.qmd,line=32,col=6,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=03-syntaxe.qmd,line=32,col=9::file=03-syntaxe.qmd,line=32,col=9,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=32,col=9::file=03-syntaxe.qmd,line=32,col=9,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=03-syntaxe.qmd,line=33,col=0::file=03-syntaxe.qmd,line=33,col=0,[indentation_linter] Indentation should be 4 spaces but is 0 spaces.\n::warning file=03-syntaxe.qmd,line=34,col=0::file=03-syntaxe.qmd,line=34,col=0,[indentation_linter] Indentation should be 2 spaces but is 0 spaces.\n::warning file=03-syntaxe.qmd,line=34,col=6::file=03-syntaxe.qmd,line=34,col=6,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=35,col=0::file=03-syntaxe.qmd,line=35,col=0,[indentation_linter] Indentation should be 4 spaces but is 0 spaces.\n::warning file=03-syntaxe.qmd,line=36,col=0::file=03-syntaxe.qmd,line=36,col=0,[indentation_linter] Indentation should be 2 spaces but is 0 spaces.\n::warning file=03-syntaxe.qmd,line=37,col=6::file=03-syntaxe.qmd,line=37,col=6,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=38,col=0::file=03-syntaxe.qmd,line=38,col=0,[indentation_linter] Indentation should be 2 spaces but is 0 spaces.\n::warning file=03-syntaxe.qmd,line=43,col=6::file=03-syntaxe.qmd,line=43,col=6,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=03-syntaxe.qmd,line=43,col=9::file=03-syntaxe.qmd,line=43,col=9,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=43,col=9::file=03-syntaxe.qmd,line=43,col=9,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=03-syntaxe.qmd,line=45,col=8::file=03-syntaxe.qmd,line=45,col=8,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=03-syntaxe.qmd,line=45,col=11::file=03-syntaxe.qmd,line=45,col=11,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=45,col=11::file=03-syntaxe.qmd,line=45,col=11,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=03-syntaxe.qmd,line=47,col=8::file=03-syntaxe.qmd,line=47,col=8,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=50,col=6::file=03-syntaxe.qmd,line=50,col=6,[brace_linter] There should be a space before an opening curly brace.\n::warning file=03-syntaxe.qmd,line=67,col=4::file=03-syntaxe.qmd,line=67,col=4,[commas_linter] Commas should always have a space after.\n::warning file=03-syntaxe.qmd,line=68,col=3::file=03-syntaxe.qmd,line=68,col=3,[commas_linter] Commas should never have a space before.\n::warning file=03-syntaxe.qmd,line=68,col=3::file=03-syntaxe.qmd,line=68,col=3,[spaces_inside_linter] Do not place spaces after square brackets.\n::warning file=03-syntaxe.qmd,line=68,col=5::file=03-syntaxe.qmd,line=68,col=5,[commas_linter] Commas should always have a space after.\n::warning file=03-syntaxe.qmd,line=69,col=3::file=03-syntaxe.qmd,line=69,col=3,[commas_linter] Commas should never have a space before.\n::warning file=03-syntaxe.qmd,line=69,col=3::file=03-syntaxe.qmd,line=69,col=3,[spaces_inside_linter] Do not place spaces after square brackets.\n::warning file=03-syntaxe.qmd,line=79,col=5::file=03-syntaxe.qmd,line=79,col=5,[function_left_parentheses_linter] Remove spaces before the left parenthesis in a function call.\n::warning file=03-syntaxe.qmd,line=80,col=6::file=03-syntaxe.qmd,line=80,col=6,[spaces_inside_linter] Do not place spaces after parentheses.\n::warning file=03-syntaxe.qmd,line=80,col=22::file=03-syntaxe.qmd,line=80,col=22,[spaces_inside_linter] Do not place spaces before parentheses.\n::warning file=03-syntaxe.qmd,line=92,col=3::file=03-syntaxe.qmd,line=92,col=3,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=03-syntaxe.qmd,line=120,col=81::file=03-syntaxe.qmd,line=120,col=81,[line_length_linter] Lines should not be more than 80 characters. This line is 83 characters.\n::warning file=04-organisation.qmd,line=11,col=74::file=04-organisation.qmd,line=11,col=74,[commas_linter] Commas should always have a space after.\n::warning file=04-organisation.qmd,line=11,col=81::file=04-organisation.qmd,line=11,col=81,[line_length_linter] Lines should not be more than 80 characters. This line is 85 characters.\n::warning file=04-organisation.qmd,line=12,col=3::file=04-organisation.qmd,line=12,col=3,[spaces_left_parentheses_linter] Place a space before left parenthesis, except in a function call.\n::warning file=04-organisation.qmd,line=59,col=10::file=04-organisation.qmd,line=59,col=10,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=04-organisation.qmd,line=79,col=3::file=04-organisation.qmd,line=79,col=3,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=04-organisation.qmd,line=123,col=1::file=04-organisation.qmd,line=123,col=1,[trailing_blank_lines_linter] Trailing blank lines are superfluous.\n::warning file=05-regles.qmd,line=8,col=34::file=05-regles.qmd,line=8,col=34,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=05-regles.qmd,line=9,col=35::file=05-regles.qmd,line=9,col=35,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=05-regles.qmd,line=31,col=34::file=05-regles.qmd,line=31,col=34,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=05-regles.qmd,line=32,col=35::file=05-regles.qmd,line=32,col=35,[infix_spaces_linter] Put spaces around all infix operators.\n::warning file=05-regles.qmd,line=69,col=8::file=05-regles.qmd,line=69,col=8,[error] unexpected symbol\n::warning file=06-outils.qmd,line=14,col=3::file=06-outils.qmd,line=14,col=3,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=16,col=3::file=06-outils.qmd,line=16,col=3,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=33,col=3::file=06-outils.qmd,line=33,col=3,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=37,col=3::file=06-outils.qmd,line=37,col=3,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=49,col=3::file=06-outils.qmd,line=49,col=3,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=56,col=3::file=06-outils.qmd,line=56,col=3,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=69,col=20::file=06-outils.qmd,line=69,col=20,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=71,col=23::file=06-outils.qmd,line=71,col=23,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=112,col=37::file=06-outils.qmd,line=112,col=37,[brace_linter] There should be a space before an opening curly brace.\n::warning file=06-outils.qmd,line=112,col=37::file=06-outils.qmd,line=112,col=37,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=06-outils.qmd,line=113,col=1::file=06-outils.qmd,line=113,col=1,[trailing_whitespace_linter] Trailing whitespace is superfluous.\n::warning file=06-outils.qmd,line=117,col=42::file=06-outils.qmd,line=117,col=42,[commas_linter] Commas should always have a space after.\n::warning file=06-outils.qmd,line=118,col=9::file=06-outils.qmd,line=118,col=9,[brace_linter] There should be a space before an opening curly brace.\n\n\nLe linter renvoie une suite, plus ou moins longue selon la qualité du projet, de dérogations aux bonnes pratiques.\n Le linter ne faisant pas les corrections automatiquement, il est donc nécessaire d’ouvrir le fichier, se rendre à la ligne correspondante, et corriger. Les lignes indiquées ne sont pas mises à jour automatiquement, elles peuvent donc ne plus correspondre à celles du fichier lors de la phase de modifications. Il est donc pratique de faire tourner régulièrement le linter lors d’une phase de nettoyage.\nIl est également possible de n’évaluer qu’un fichier avec lintr::lint:\n\nlintr::lint(\"mesfonctions_pour_faire_ceci.R\")\n\n\n\n::warning file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=1,col=59::file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=1,col=59,[brace_linter] There should be a space before an opening curly brace.\n::warning file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=1,col=59::file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=1,col=59,[paren_body_linter] There should be a space between a right parenthesis and a body expression.\n::warning file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=3,col=14::file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=3,col=14,[object_usage_linter] no visible global function definition for '%&gt;%'\n::warning file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=3,col=25::file=/home/runner/work/utilitR/utilitR/mesfonctions_pour_faire_ceci.R,line=3,col=25,[object_usage_linter] no visible binding for global variable 'NBPERSMENFISC16'\n\n\nLe package styler propose le même type de fonctions qui vont quant à elles modifier le code:\n\nPour modifier un seul script, la fonction à utiliser est styler::style_file ;\nPour modifier l’ensemble des scripts d’un dossier, la fonction à utiliser est styler::style_dir",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#utiliser-des-fonctions",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#utiliser-des-fonctions",
    "title": "31  Qualité du code",
    "section": "\n31.3 Utiliser des fonctions",
    "text": "31.3 Utiliser des fonctions\n\n31.3.1 Pourquoi utiliser des fonctions?\nL’utilisation de fonctions est l’une des bonnes pratiques en matière de programmation qui s’applique à tous les langages de programmation, y compris R.\nLa règle DRY pour do not repeat yourself (ne pas se répéter) indique qu’il faut éviter de copier-coller du code lorsqu’il est utilisé plus de deux fois. Au lieu de cela, on devrait encapsuler ce code dans une fonction et utiliser cette fonction aux endroits où cela est nécessaire.\nUtiliser des fonctions présente plusieurs avantages:\n\nUtiliser des fonctions réduit les risques d’erreurs liées au copier-coller de code. Si une modification est nécessaire, elle peut être apportée dans la fonction, à un seul endroit du code, ce qui garantit que toutes les utilisations de la fonction seront automatiquement mises à jour. Cette pratique minimise les erreurs et peut représenter une économie de temps substantielle dans un gros projet.\nUtiliser des fonctions rend également le code plus lisible et plus compact en encapsulant un traitement spécifique dans une section distincte du code.\nUtiliser des fonctions facilite la réutilisation et la documentation du code. D’une part parce qu’en encapsulant un traitement dans une fonction, on peut facilement le réutiliser dans d’autres parties du code. D’autre part, parce que décrire clairement ce que fait chaque fonction contribue à documenter le code dans son ensemble.\n\nEnfin, un nom bien choisi pour une fonction donne déjà une bonne idée de ce à quoi elle sert, facilitant en cela la compréhension d’une chaîne de traitements.\n\n31.3.2 Comment bien utiliser les fonctions?\nUn biais à éviter est le code spaghetti. Il s’agit d’un code qui est difficile à comprendre et à maintenir en raison de sa complexité, de sa longueur et de sa structure désorganisée. Pour éviter le code spaghetti, il est important de suivre certaines règles pour écrire des fonctions pertinentes. Voici les trois principales règles à retenir :\n\nUne tâche = une fonction : chaque fonction devrait effectuer une seule tâche spécifique. Cela permet de rendre le code plus clair et plus facile à comprendre.\nUne tâche complexe = un enchaînement de fonctions réalisant chacune une tâche simple : si une tâche est complexe, elle peut être divisée en plusieurs tâches plus simples et encapsulées dans des fonctions distinctes. Cela permet de rendre le code plus facile à comprendre et à maintenir.\nLimiter l’utilisation de variables globales : les variables globales sont accessibles depuis n’importe quel endroit du code, ce qui peut rendre le code difficile à comprendre et à maintenir. Il est donc recommandé de limiter l’utilisation de variables globales et d’utiliser des variables locales au lieu de cela. Cela permet de rendre le code plus clair et plus facile à comprendre.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#auto-documenter-son-code",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#auto-documenter-son-code",
    "title": "31  Qualité du code",
    "section": "\n31.4 Auto-documenter son code",
    "text": "31.4 Auto-documenter son code\nLes grands principes de la documentation de code consistent à :\n\nDocumenter le pourquoi plutôt que le comment : il est plus important de comprendre pourquoi le code a été écrit de la manière dont il l’a été, plutôt que de connaître les détails techniques de son fonctionnement. En documentant le pourquoi, on peut mieux comprendre le but du code et comment il s’intègre dans le projet plus global.\nPrivilégier l’auto-documentation via des nommages pertinents : le code peut être plus clair et plus facile à comprendre si les variables, les fonctions et les autres éléments ont des noms pertinents et explicites. Cela permet de documenter le code de manière implicite et de rendre la lecture du code plus intuitive.\n\nEn gardant ces grands principes à l’esprit, on peut écrire du code qui est plus facile à comprendre et à maintenir, ce qui peut économiser du temps et des ressources dans le long terme.\n. . .\n\n\n\n\n\n\nTip\n\n\n\nComment bien documenter un script ?\n\n\nMinimum 🚦 : commentaire au début du script pour décrire ce qu’il fait ;\n\nBien 👍 : commenter les parties “délicates” du code ;\n\nIdéal 💪 : documenter ses fonctions avec la syntaxe roxygen2.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#pas-dambiguïté-sur-les-packages-utilisés",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#pas-dambiguïté-sur-les-packages-utilisés",
    "title": "31  Qualité du code",
    "section": "\n31.5 Pas d’ambiguïté sur les packages utilisés",
    "text": "31.5 Pas d’ambiguïté sur les packages utilisés\nDeux fonctions peuvent avoir le même nom dans des packages différents. Par exemple, la fonction select existe dans les packages dplyr et MASS. Par défaut, R utilise la fonction du package chargé le plus récemment (avec library()). Ce comportement peut causer des erreurs difficiles à repérer, car il est nécessaire d’exécuter le code pour les détecter. Par exemple, le code suivant renvoie une erreur difficile à comprendre si on ne l’a pas déjà rencontrée.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nbpe_ens_2018 &lt;- doremifasolData::bpe_ens_2018\n\nnombre &lt;- bpe_ens_2018 %&gt;%\n  as_tibble() %&gt;%\n  select(TYPEQU, NB_EQUIP) \n\nError in select(., TYPEQU, NB_EQUIP): unused arguments (TYPEQU, NB_EQUIP)\n\n\nCela provient du fait que MASS étant le dernier package chargé, R utilise sa fonction select plutôt que celle de dplyr.\nAfin d’éviter ces erreurs, il est recommandé de réserver library(pkg) aux packages dont on utilise des fonctions à de nombreuses reprises dans un code. Inversement, pour les packages utilisés de façon ponctuelle il est recommandé d’indiquer explicitement le package en utilisant la notation package::fonction(). De même, si une fonction présente le même nom dans deux packages, il est recommandé d’utiliser cette notation. Cela permet de garantir que la bonne fonction est appelée et d’éviter les erreurs potentielles.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#le-package-conflicted",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#le-package-conflicted",
    "title": "31  Qualité du code",
    "section": "\n31.6 Le package conflicted\n",
    "text": "31.6 Le package conflicted\n\nLe package conflicted aide à gérer les conflits de packages de manière fluide.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#ressources-supplémentaires",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#ressources-supplémentaires",
    "title": "31  Qualité du code",
    "section": "\n31.7 Ressources supplémentaires",
    "text": "31.7 Ressources supplémentaires\n\n\nR Packages par Hadley Wickham and Jenny Bryan\nUne présentation très bien faite\n\n\nUn cours complet sur la reproductibilité avec R\n\nL’équivalent Python en 3A d’ENSAE",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#exercices",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#exercices",
    "title": "31  Qualité du code",
    "section": "\n31.8 Exercices",
    "text": "31.8 Exercices\nto be completed",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/01-qualite-code.html#footnotes",
    "href": "02_Bonnes_pratiques/01-qualite-code.html#footnotes",
    "title": "31  Qualité du code",
    "section": "",
    "text": "Cette phrase très connue est une citation de Guido Van Rossum, le créateur de Python. R comme Python sont des langages conçus pour être plus transparents et faciles à lire que des langages bas niveaux comme C.↩︎",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Qualité du code</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html",
    "href": "02_Bonnes_pratiques/02-structure-code.html",
    "title": "32  Structure des projets",
    "section": "",
    "text": "32.1 Tâches concernées et recommandations\nL’utilisateur souhaite améliorer la structuration de ses projets R afin de favoriser leur maintenabilité et leur réutilisation.\nCette partie détaille de manière plus étendue les éléments enseignés dans le cadre d’une formation aux bonnes pratiques construite par l’Insee et dont les supports ont été ouverts à cette adresse.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#tâches-concernées-et-recommandations",
    "href": "02_Bonnes_pratiques/02-structure-code.html#tâches-concernées-et-recommandations",
    "title": "32  Structure des projets",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nIl est recommandé d’utiliser systématiquement les projets RStudio dans des projets impliquant des programmes R.\nIl est recommandé de structurer les projets en dossiers thématiques (données, code, sorties, documentation) et d’organiser ces dossiers afin de séparer les entrées d’une chaîne de traitement, les objets intermédiaires et les sorties.\nIl est recommandé d’adopter des noms signifiants pour les fichiers et de ne jamais utiliser des espaces dans les noms de dossiers et fichiers.\nIl est recommandé d’accorder une attention particulière au fichier README.md.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#enjeux",
    "href": "02_Bonnes_pratiques/02-structure-code.html#enjeux",
    "title": "32  Structure des projets",
    "section": "32.2 Enjeux",
    "text": "32.2 Enjeux\nOn peut comparer la structure d’un projet à l’organisation d’un bureau. Si ce dernier est désordonné, il est très difficile de dissocier les dossiers et de ne pas prendre beaucoup de temps, voire abandonner toute recherche, lorsqu’il est nécessaire d’en retrouver un. A l’inverse, un bureau bien organisé, et au sein de ce bureau des dossiers bien rangés, faciliteront la recherche d’information. Le bon fonctionnement d’un projet informatique est identique. Un projet bien structuré, avec une organisation sensée, améliorera la lisibilité du projet ainsi que sa maintenabilité.\nPrenons l’organisation suivante, à ne pas reproduire:\n├── report.Rmd\n├── correlation.png\n├── data.csv\n├── data2.csv\n├── fig1.png\n├── figure 2 (copy).png\n├── report.pdf\n├── partial data.csv\n├── script.R\n└── script_final.R\nSource : eliocamp.github.io\nSans une documentation claire sur l’organisation du projet, il est très difficile de comprendre la hiérarchie et l’ordre d’exécution des scripts, de séparer les bases qui sont en entrée du traitement de celles produites par le traitement, d’être certain que toutes les productions sont issues du traitement et non de copier-coller manuels et d’être certain que toutes les productions ont été faites à partir de la dernière version du code.\nComme pour le rangement d’un bureau, la méthode la plus efficace n’est pas d’attendre que la situation devienne ingérable mais d’organiser en continu le projet. Git est un outil qui favorise cette bonne pratique, puisqu’il évite la duplication des fichiers, mais il ne s’agit pas d’un outil miraculeux. Il est donc recommandé d’adopter des conventions, assez similaires à celles proposées dans la fiche Qualité du code. par exemple l’autodocumentation par des conventions de nommage cohérentes et l’organisation du projet dans des noms de dossiers logiques pour permettre à d’autres de comprendre l’objectif d’un projet, sa structure et ses productions.\nLes principes généraux sont les suivants, détaillés dans les parties qui suivent:\n\nUtiliser les projets RStudio ;\nOrganiser son projet en sous-dossiers numérotés ;\nDonner des noms pertinents aux fichiers et dossiers ;\nDocumenter son projet.\n\nIl est important d’être vigilant sur la bonne structuration d’un projet car un code mal structuré limite la lisibilité du projet et est très coûteux à maintenir (concept de la dette technique).",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#utiliser-les-projets-rstudio",
    "href": "02_Bonnes_pratiques/02-structure-code.html#utiliser-les-projets-rstudio",
    "title": "32  Structure des projets",
    "section": "32.3 Utiliser les projets RStudio",
    "text": "32.3 Utiliser les projets RStudio\nUne fiche détaillée est consacrée aux projets RStudio. Les principaux avantages des projets RStudio sont les suivants:\n\nTous les fichiers nécessaires au projet sont dans un même dossier ;\nLe dossier contenant le projet RStudio est automatiquement utilisé comme working directory ;\nIls utilisent des chemins relatifs plutôt qu’absolus.\n\nCela peut aider à éviter les problèmes de chemin lors de la reprise du projet sur un autre ordinateur ou avec une autre personne.\nSi vous suivez le mode opératoire pour l’utilisation de Git dans RStudio détaillé dans la fiche dédiée, vous êtes assuré de travailler dans un projet RStudio.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#organiser-son-projet-en-sous-dossiers",
    "href": "02_Bonnes_pratiques/02-structure-code.html#organiser-son-projet-en-sous-dossiers",
    "title": "32  Structure des projets",
    "section": "32.4 Organiser son projet en sous-dossiers",
    "text": "32.4 Organiser son projet en sous-dossiers\nComme toute convention, toute structure de dossier comporte une certaine forme d’arbitraire et il n’apparaît pas toujours évident de considérer qu’une règle d’organisation est plus légitime qu’une autre. Il est plus important de chercher des structures lisibles et cohérentes, admises par une communauté large de praticiens d’un langage.\nPour reprendre l’exemple précédent, une structure déjà plus lisible est la suivante:\n├── data\n│   ├── raw\n│   │   ├── data.csv\n│   │   └── data2.csv\n│   └── derived\n│       └── partial data.csv\n├── scripts\n│   └── script.R\n├── analysis\n│   ├── script_final.R\n│   └── report.Rmd\n└── output\n    ├── fig1.png\n    ├── figure 2 (copy).png\n    ├── figure10.png\n    ├── correlation.png\n    └── report.pdf\nDans cette organisation :\n\nle dossier data vise à stocker les données locales utilisées par le projet. Le dossier raw contient les données de base, qui doivent rester immuables. Le dossier derived peut contenir des tables intermédiaires produites à partir des données de base ;\nla dossier scripts contient les scripts qui réalisent des traitements de données : téléchargement, import, nettoyage, etc. ;\nle dossier analysis contient les scripts qui réalisent une analyse statistique et/ou une mise en forme sous forme de rapport ;\nle dossier output contient les fichiers générés par le projet : rapports, figures, etc. Ces fichiers doivent toujours pouvoir être re-générés par le code présent dans le projet, par souci de reproductibilité.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#donner-des-noms-pertinents-aux-fichiers",
    "href": "02_Bonnes_pratiques/02-structure-code.html#donner-des-noms-pertinents-aux-fichiers",
    "title": "32  Structure des projets",
    "section": "32.5 Donner des noms pertinents aux fichiers",
    "text": "32.5 Donner des noms pertinents aux fichiers\nLa structure précédente est déjà plus claire. Néanmoins, au sein de chaque dossier, l’organisation est encore perfectible. En donnant des noms clairs aux fichiers et en séparant au sein de chaque dossier les différents niveaux, on comprend beaucoup mieux l’organisation du projet et ainsi la chaine de production.\n├── data\n│   ├── raw\n│   │   ├── dpe_logement_202103.csv\n│   │   └── dpe_logement_202003.csv\n│   └── derived\n│       └── dpe_logement_merged_preprocessed.csv\n├── scripts\n│   └── preprocessing.R\n├── analysis\n│   ├── generate_plots.R\n│   └── report.Rmd\n└── output\n    ├── histogram_energy_diagnostic.png\n    ├── barplot_consumption_pcs.png\n    ├── correlation_matrix.png\n    └── report.pdf\n\n\n\n\n\n\nNote\n\n\n\nLes noms de dossiers sont ici en Anglais, par souci de cohérence avec les noms de fichiers. Cette règle n’est pas obligatoire, il est tout à fait possible d’adopter la convention de nommer les dossiers et fichiers en Français.\nDans ce cas, il est néanmoins important de ne pas utiliser d’accent dans les noms, car ceux-ci peuvent provoquer des erreurs, au même titre que les espaces ou autres caractères spéciaux.\n\n\nLes données initiales sont isolées des données retraitées dans le cadre de l’analyse. Le code est décomposé entre la principale chaine de traitement - sans doute à l’origine du 📁 derived/dpe_logement_merged_preprocessed.csv et celle qui génère des valorisations ultérieures. En observant exclusivement l’aborescence, grâce aux noms signifiants, on comprend que le code traite des données de diagnostics énergétiques (DPE) pour une problématique d’analyse de consommation énergétique, sans doute en lien avec une problématique d’inégale répartition selon les PCS.\n\n\n\n\n\n\nTip\n\n\n\nIl est recommandé de ne jamais utiliser d’espaces dans les noms de fichiers ou de dossiers. Ceci peut amener à des erreurs difficiles à détecter lors de l’exécution de scripts R.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#documenter-son-projet",
    "href": "02_Bonnes_pratiques/02-structure-code.html#documenter-son-projet",
    "title": "32  Structure des projets",
    "section": "32.6 Documenter son projet",
    "text": "32.6 Documenter son projet\nLe fichier README.md, situé à la racine du projet, est à la fois la carte d’identité et la vitrine du projet. En effet, comme il s’agit du fichier qui fait office de point d’entrée d’un projet sur Github ou Gitlab, il s’agit de la première source d’information pour comprendre l’objet du projet.\nIdéalement, ce fichier contient :\n\nUne présentation du contexte et des objectifs du projet;\nUne description du fonctionnement du projet (modalités d’installation d’un package, scripts principaux…).\n\nDans le cadre d’un projet collaboratif, il peut être utile d’intégrer une petite section expliquant la démarche de contribution ou renvoyer vers un guide de contribution plus complet si le projet est conséquent. Pour favoriser des contributions multiples, la documentation utilitR, par exemple, propose un guide des contributeurs assez détaillé.\nVoici quelques modèles de README.md complets :\n\nutilitR;\nDoReMIFaSol.",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#ressources-supplémentaires",
    "href": "02_Bonnes_pratiques/02-structure-code.html#ressources-supplémentaires",
    "title": "32  Structure des projets",
    "section": "32.7 Ressources supplémentaires",
    "text": "32.7 Ressources supplémentaires\n\neliocamp.github.io",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "02_Bonnes_pratiques/02-structure-code.html#exercices",
    "href": "02_Bonnes_pratiques/02-structure-code.html#exercices",
    "title": "32  Structure des projets",
    "section": "32.8 Exercices",
    "text": "32.8 Exercices\nTO BE COMPLETED",
    "crumbs": [
      "Introduction",
      "Bonnes pratiques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Structure des projets</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html",
    "href": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html",
    "title": "33  Utiliser RStudio sur l’environnement AUSv3",
    "section": "",
    "text": "33.1 Présentation succincte d’AUSv3\nLes serveurs AUSv3 sont des environnements informatiques de travail sur lesquels les agents de l’Insee peuvent traiter des données confidentielles de façon sécurisée. Les serveurs AUSv3 sont regroupés en cinq groupes de serveurs appelés collections : la collection SAS, la collection WPS, la collection RPython, la collection Formation et la collection Lab.\nUne collection AUSv3 est un regroupement de serveurs tous strictement identiques entre eux : les ressources informatiques, la configuration et les logiciels disponibles sont exactement les mêmes sur les différents serveurs d’une collection. En revanche, les collections sont différentes entre elles. Par exemple, tous les serveurs de la collection RPython disposent R, RStudio et Python (mais pas de SAS), et tous les serveurs de la collection SAS disposent de SAS (mais pas de RStudio ni de Python).",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Utiliser RStudio sur l'environnement AUSv3</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#présentation-succincte-dausv3",
    "href": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#présentation-succincte-dausv3",
    "title": "33  Utiliser RStudio sur l’environnement AUSv3",
    "section": "",
    "text": "Spécificité Insee\n\n\n\nLes serveurs AUSv3 font l’objet d’une documentation fournie. Cette documentation se trouve principalement sur l’intranet Insee. Vous pouvez y accéder par l’intranet, puis en cliquant sur Services &gt; DSI &gt; Libre Service statistique &gt; AUS V3.\nDes fiches et des tutoriels sont également disponibles dans AUSv3, dans le lecteur Commun.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLe document de référence sur l’utilisation d’AUSv3 est le guide de l’utilisateur d’AUSv3 disponible depuis l’intranet.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Utiliser RStudio sur l'environnement AUSv3</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#comment-se-connecter-à-ausv3",
    "href": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#comment-se-connecter-à-ausv3",
    "title": "33  Utiliser RStudio sur l’environnement AUSv3",
    "section": "33.2 Comment se connecter à AUSv3",
    "text": "33.2 Comment se connecter à AUSv3\n\n33.2.1 Obtenir un accès à AUSv3\nPour obtenir un accès à AUSv3, vous devez déposer une demande sur l’outil Siamoi. Voici la procédure :\n\nDouble-cliquer sur l’icône Siamoi disponible sur le bureau de votre poste local ;\nCliquer sur “Faire une demande” ;\nSous la bannière “AUS : Création / suppression / modification de mon espace personnel”, cliquer sur “Demander”.\n\nVous devriez recevoir des emails vous indiquant que votre accès à AUS a été créé. Suite à la création de votre accès à AUS, vous devriez voir apparaître l’icône suivante sur votre bureau :\n\n\n\n\n\n\n\nTip\n\n\n\nSi votre accès à AUS n’est pas créé rapidement, vous pouvez ouvrir un ticket Siamoi.\n\n\n\n\n33.2.2 Ouvrir une session sur AUSv3\nPour ouvrir une session sur la collection RPython, il faut cliquer sur l’icône d’AUSv3\n\npuis sur le raccourci",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Utiliser RStudio sur l'environnement AUSv3</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#utiliser-rstudio-dans-ausv3",
    "href": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#utiliser-rstudio-dans-ausv3",
    "title": "33  Utiliser RStudio sur l’environnement AUSv3",
    "section": "33.3 Utiliser RStudio dans AUSv3",
    "text": "33.3 Utiliser RStudio dans AUSv3\n\n33.3.1 Ouvrir RStudio\nPour ouvrir RStudio dans AUSv3, il suffit de double-cliquer sur l’icône RStudio présente sur le bureau d’AUS (encadrée en rouge sur l’image ci-dessous).\n\nD’une manière générale, R et RStudio s’utilisent sur AUSv3 de la même manière que sur un ordinateur personnel. Il existe toutefois certaines particularités liées aux droits d’accès des utilisateurs à certains dossiers :\n\ndans AUSv3, les utilisateurs ont accès au CRAN via un proxy ;\nl’absence d’accès à Internet rend impossible l’installation directe de packages hébergés sur des forges logicielles extérieures à l’Insee (par exemple, la fonction remotes::install_github() ne fonctionne pas) ;\nles packages installés par un utilisateur sont stockés par défaut sur son espace personnel, dans le dossier (ESPERT)/R/win-library ; il est toutefois possible de changer ce répertoire par défaut avec la fonction .libPaths() ;\nles fichiers de configuration .Rprofile et .Renviron sont recherchés par défaut au démarrage dans le dossier (ESPERT)/R.\n\nPour plus de détails concernant ces points, consulter les fiches Utiliser des packages R et Personnaliser la configuration de R.\n\n\n\n\n\n\nNote\n\n\n\nIl est possible de demander la mise à disposition dans AUSv3 de packages non disponibles sur le CRAN. Pour ce faire, il faut déposer une demande métier sur Siamoi.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLes ressources informatiques des serveurs AUSv3 (mémoire vive et processeurs) sont partagées entre les applications et les utilisateurs. Il est donc essentiel de veiller à faire un bon usage de ces ressources, de façon à ne pas gêner le travail des autres applications ou utilisateurs. Il est conseillé de lire la fiche Superviser sa session R pour apprendre à superviser votre usage de R.\n\n\n\n\n33.3.2 Travailler avec Git sur AUSv3\n\n33.3.2.1 Créer une clé SSH ou un token https\nPour mettre en place une clé SSH ou un token https sur AUSv3, vous pouvez vous reporter aux ressources suivantes :\n\nla fiche Configurer Git sur son poste de travail ;\nla fiche consacrée à ce sujet dans la documentation d’AUSv3 ;\nla partie sur les bonnes pratiques dans le guide de l’utilisateur d’AUSv3.\n\n\n\n33.3.2.2 Où stocker les dépôts locaux\nLorsque vous créez un projet RStudio en clonant un dépôt distant, il est nécessaire de définir l’emplacement de votre dépôt local (voir la fiche Travailler avec Git). Dans AUSv3, il est conseillé de stocker vos dépôts locaux dans votre espace personnel (lecteur ESPERT). Par exemple, vous pouvez stocker tous vos dépôts dans un dossier DepotsGit du lecteur ESPERT. En revanche, il est déconseillé de stocker un dépôt local dans un coffre ou dans un espace partagé, car d’autres agents peuvent avoir accès à votre dépôt local.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Utiliser RStudio sur l'environnement AUSv3</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#pour-en-savoir-plus",
    "href": "01_R_Insee/Fiche_utiliser_Rstudio_AUSv3.html#pour-en-savoir-plus",
    "title": "33  Utiliser RStudio sur l’environnement AUSv3",
    "section": "33.4 Pour en savoir plus",
    "text": "33.4 Pour en savoir plus\n\nLa foire aux questions d’AUSv3 est disponible sur Agora : [agora.insee.fr], puis cliquer sur Services &gt; DSI &gt; Libre Service statistique &gt; AUS V3 ;\nLa documentation d’AUSv3 est disponible dans le dossier Y:/Documentation/AUSv3 ;\nle guide de l’utilisateur d’AUSv3 disponible sur l’intranet.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Utiliser RStudio sur l'environnement AUSv3</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "",
    "text": "34.1 Tâches concernées et recommandations\nL’utilisateur souhaite versionner son projet depuis son poste de travail en ayant recours pour cela à un dépôt distant, disponible sur un espace type GitLab ou GitHub.\nL’utilisation du logiciel Git en local, qui relève plutôt de l’usage quotidien du logiciel, est reportée dans une fiche dédiée (Utiliser Git avec RStudio).",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html#tâches-concernées-et-recommandations",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html#tâches-concernées-et-recommandations",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "",
    "text": "Tâche concernée et recommandation\n\n\n\n\nPour interagir avec un dépôt Git distant, l’utilisateur a le choix entre deux protocoles d’authentification : SSH ou HTTPS.\nIl est conseillé d’utiliser le protocole HTTPS qui est simple à configurer. Vous pouvez utiliser le protocole SSH si vous le souhaitez, mais il peut s’avérer difficile à configurer.\nSi vous utilisez l’authentification HTTPS, il est recommandé de s’authentifier avec des jetons personnels d’accès (personal access tokens) plutôt qu’avec un login et un mot de passe.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html#les-différentes-forges-utilisables",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html#les-différentes-forges-utilisables",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "\n34.2 Les différentes forges utilisables",
    "text": "34.2 Les différentes forges utilisables\nUne forge est un espace informatique en ligne qui accueille des dépôts distants. Un dépôt distant (remote repository en anglais) est un dossier que vous pouvez utiliser pour stocker vos codes (et leur historique), et pour partager des codes avec vos collègues ou stocker vos codes personnels.\nUne forge prend généralement forme d’un site internet. Il existe deux forges principales :\n\nGitHub (accessible sur le Web, à l’adresse https://github.com), qui appartient à Microsoft ;\nGitLab qui se décline sous différentes formes :\n\nune version publique, disponible sur https://gitlab.com, qui propose les fonctionnalités principales de versionnage du code mais également des fonctionnalités supplémentaires (discussions, mise à disposition de machines (runners) pour tester des scripts…) ;\ndes versions internes, qui reprennent la majorité des fonctionnalités de la version publique mais proposent des machines (appelées runners) internes. Celles-ci ne sont pas forcément ouvertes sur internet.\n\n\n\nLes forges proposent des fonctionnalités précieuses pour gérer un projet impliquant du code ou de la documentation. La formation Travail collaboratif avec R décrit ces fonctionnalités de manière plus détaillée.\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nUne forge interne GitLab est accessible depuis AUS. Son adresse, ainsi que des éléments complémentaires à cette fiche, sont disponibles dans la documentation AUS (Y:/Documentation/AUSV3/).\n\n\n\n\n\n\n\n\nNote\n\n\n\nUne instance GitLab est disponible sur le SSP Cloud [https://git.lab.sspcloud.fr/] avec laquelle peut interagir un service RStudio [lien vers fiche quand elle sera là], et vous pouvez l’utiliser si vous le souhaitez. Toutefois, le SSP Cloud est une plateforme accessible depuis Internet et a donc accès aux dépôts hébergés sur https://gitlab.com ou https://github.com. Il est donc conseillé d’utiliser une de ces deux forges plutôt que https://git.lab.sspcloud.fr/.\n\n\nLa suite de cette fiche décrit l’utilisation d’un dépôt distant sur une forge de type GitLab. Néanmoins, la procédure pour configurer l’interaction avec un dépôt distant situé sur GitHub est très similaire, ce qui est illustré dans la fiche Utiliser Git avec RStudio.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html#git-install",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html#git-install",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "\n34.3 Installer Git sur son poste de travail",
    "text": "34.3 Installer Git sur son poste de travail\nPour interagir avec un dépôt distant sur GitLab ou GitHub, il est nécessaire d’utiliser l’outil de contrôle de version Git.\nAvant d’essayer d’installer Git, le premier réflexe est de vérifier qu’il n’est pas déjà disponible. En effet, sur un serveur partagé comme AUS, ou sur une architecture comme le SSP Cloud, il n’est pas nécessaire d’installer Git puisque celui-ci est déjà disponible et configuré.\nSi Git n’est pas disponible, il faut l’installer. Pour ce faire, il suffit de télécharger le logiciel site web officiel, puis de l’installer. Une fois le logiciel installé, il est possible de le retrouver dans le menu Démarrer, rubrique Git. L’utilisateur y trouvera plusieurs outils, dont un outil de ligne de commande (Git Bash) et une interface (Git GUI).",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html#interaction-avec-un-dépôt-distant-principe",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html#interaction-avec-un-dépôt-distant-principe",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "\n34.4 Interaction avec un dépôt distant : principe",
    "text": "34.4 Interaction avec un dépôt distant : principe\nGit est un système décentralisé de contrôle de version : les codes sont modifiés par chaque agent sur son poste de travail, puis sont mis en conformité avec la version collective disponible sur le dépôt distant au moment où l’agent le décide.\nIl est donc nécessaire que la forge connaisse l’identité de chacun des contributeurs, afin de déterminer qui est l’auteur d’une modification apportée aux codes stockés dans le dépôt distant. Pour que GitLab reconnaisse un utilisateur proposant des modifications, il est nécessaire de s’authentifier (un dépôt distant, même public, ne peut pas être modifié par n’importe qui). L’authentification consiste ainsi à fournir un élément que seul vous et la forge sont censés connaître : un mot de passe, une clé compliquée, un jeton d’accès…\nPlus précisément, il existe deux modalités pour faire connaître son identité à GitLab :\n\nune authentification HTTPS (décrite ici) : l’authentification se fait avec un login et un mot de passe (qu’il faut renseigner à chaque interaction avec le dépôt), ou avec un token (méthode à privilégier).\n\nune authentification SSH : l’authentification se fait par une clé cryptée disponible sur le poste de travail et que GitHub ou GitLab connaît. Une fois configurée, cette méthode ne nécessite plus de faire connaître son identité : l’empreinte digitale que constitue la clé suffit à reconnaître un utilisateur.\n\n\n\n\n\n\n\nTip\n\n\n\nSi vous utilisez l’authentification HTTPS, il est conseillé d’utiliser un jeton d’accès. Celui-ci permet de s’authentifier, est facilement révocable et ne bénéficie pas des super-pouvoirs qu’octroie un mot de passe comme changer le nom d’un dépôt voire le supprimer. Plus d’éléments sur les jetons d’accès sont disponibles ici.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl est important de ne jamais stocker un token, et encore moins son mot de passe, dans un projet. Il est possible de stocker un mot de passe ou token de manière sécurisée et durable avec le credential helper de Git. Celui-ci est présenté par la suite.\nS’il n’est pas possible d’utiliser le credential helper de Git, un mot de passe ou token peut être stocké de manière sécurisé dans un système de gestion de mot de passe comme Keepass.\n\n\nLa méthode SSH peut être laborieuse à mettre en place, mais elle est commode dans le travail quotidien car elle permet de ne pas avoir à renseigner son login et son mot de passe à chaque interaction avec le dépôt distant. Cependant, la méthode HTTPS est préférable car :\n\nelle est beaucoup plus légère à configurer. L’utilisation d’un système de mémoire du mot de passe, à savoir le credentiel helper, est beaucoup plus simple à mettre en place que la génération de clés SSH ;\nelle n’est pas bloquée par des pare-feux, contrairement à la méthode SSH. Par exemple, il est impossible sur un poste Insee d’interagir avec un dépôt sur https://gitlab.com ou https://github.com en SSH ;\nil est possible de conserver en mémoire ses identifiants (grâce au git credentials helper) pour ne pas avoir à les renseigner à chaque fois.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html#git-connexion-https",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html#git-connexion-https",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "\n34.5 Configurer l’accès à dépôt distant en HTTPS\n",
    "text": "34.5 Configurer l’accès à dépôt distant en HTTPS\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nPour pouvoir utiliser l’authentification HTTPS avec le Gitlab interne de l’Insee, il est nécessaire de modifier un paramètre relatif aux certificats SSL. Pour cela, ouvrir la ligne de commande Git (démarche expliquée dans la fiche Utiliser Git avec RStudio) et taper la commande suivante :\ngit config --global http.sslVerify false\nIl est également possible d’utiliser l’authentification SSH\n\n\n\n34.5.1 Démarche générale\nPour interagir avec un dépôt distant en utilisant le protocole HTTPS, il suffit de configurer le mot de passe dans GitLab, en allant dans Preferences &gt; Password, comme indiqué dans la capture d’écran ci-dessous :\nUne fois le mot de passe configuré, il ne reste plus qu’à cloner le dépôt distant sur lequel on veut travailler, par exemple depuis RStudio, suivant le protocole décrit dans la fiche Utiliser Git avec RStudio, en ayant pris soin de sélectionner l’URL correspondant au protocole HTTPS, comme ci-dessous :\nAu moment du clonage, l’utilisateur est invité à entrer son identifiant (celui de son profil Gitlab avec lequel il accède au dépôt distant), ainsi que son mot de passe (celui qu’il aura donc spécifié dans son compte GitLab) auquel on substituera un token.\n\n\n\n\n\n\nTip\n\n\n\nSelon le niveau de visibilité fixé par le propriétaire du dépôt distant, il est possible qu’il ne soit pas nécessaire de s’identifier au moment du clonage (c’est le cas pour les dépôts publics). En revanche, si le niveau de visibilité du dépôt est plus élevé (niveau privé), il se peut qu’il soit impossible à l’utilisateur de cloner le dépôt ; dans ce cas, il ne lui sera pas non plus possible de visualiser ce dernier dans GitLab. Il faut alors contacter le propriétaire du dépôt pour lui en demander l’accès.\n\n\n\n34.5.2 Définir un jeton d’accès sur Gitlab\nDans GitLab, en haut à droite de la page, cliquer sur Edit profile :\nDans le bandeau à gauche, cliquer sur Access Tokens :\nChoisir un nom (par exemple PAT_GITLAB, mais ce n’est pas un nom obligatoire) et des droits associés. Les droits read_repository et write_repository permettent d’effectuer les opérations standard (pull et push) décrites dans la fiche Utilisation Git avec RStudio.\nAprès avoir validé, le jeton s’affiche afin de la récupérer et le stocker dans un endroit sécurisé (par exemple un gestionnaire de mot de passes). Attention, ce sera la seule fois qu’il sera visible.\nPlus bas sur la page, il est possible de voir l’ensemble des jetons utilisés, la date de dernière utilisation. C’est aussi ici que peut être supprimé le jeton en cas de doute sur sa confidentialité.\n\n\n\n\n\n\nNote\n\n\n\nLa marche à suivre pour créer un jeton d’accès sur GitHub est identique. La documentation officielle présente des captures d’écran qui illustrent la démarche. Il faut choisir le niveau repo pour avoir les droits mentionnés précédemment.\n\n\n\n34.5.3 Garder en mémoire ses identifiants\nL’authentification HTTPS implique en principe de renseigner ses identifiants (login et password, ou token) à chaque interaction avec le dépôt distant. Toutefois, Git propose un outil pour conserver temporairement en mémoire (cache) des informations d’authentification. Cette mise en mémoire est complètement optionnelle, et vous pouvez utiliser Git quotidiennement sans vous servir de cette possibilité.\nLa mise en mémoire se fait en deux temps:\n\nIl faut d’abord ouvrir une invite de commande Git Bash (explications ici) ;\n\nIl faut ensuite exécuter la ligne de code suivante pour stocker pendant quinze minutes les informations de connexion :\n# Sous windows\ngit config --global credential.helper manager\n\n# Sous mac et linux\ngit config --global credential.helper \n\n\n\n\n\n\n\n\nNote\n\n\n\nSi vous avez fait une faute de frappe dans le mot de passe ou dans le jeton, il est possible de vider la mémoire de la manière suivante, sous Mac ou Linux :\ngit config --global --unset credential.helper\nSous Windows, si vous avez utilisé l’option manager-core évoquée ci-dessus, vous pouvez utiliser une interface graphique pour effacer le mot de passe ou jeton erroné. Pour cela, dans le menu démarrer, taper Gestionnaire d'identification (ou Credential Manager si Windows ne trouve pas). Dans l’interface graphique qui s’ouvre, il est possible de supprimer le mot de passe ou jeton en question. Après cela, vous devriez à nouveau avoir l’opportunité de taper un mot de passe ou jeton lors d’une authentification HTTPS.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html#git-connexion-ssh",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html#git-connexion-ssh",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "\n34.6 Configurer l’accès à dépôt distant en SSH\n",
    "text": "34.6 Configurer l’accès à dépôt distant en SSH\n\nPour interagir avec un dépôt distant en utilisant le protocole SSH, il faut créer une clé SSH, plus exactement une paire de chaînes de caractères, qui va permettre une authentification automatique auprès du dépôt distant. La version publique de la paire de clés est connue du dépôt distant ; la version privée reste quant à elle la propriété seule et unique de l’utilisateur. C’est l’association par un logiciel de cryptographie de ces deux versions qui permet l’authentification de l’utilisateur.\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nL’authentification SSH ne fonctionne pas sur l’instance GitLab du SSP Cloud. Seule l’authentification en HTTPS est possible.\n\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nLa documentation d’AUS propose un tutoriel détaillé sur la configuration de la clé SSH. Vous pouvez le trouver ici : Y:/Documentation/AUSV3/Git_Utiliser Git sous AUSv3.pdf.\n\n\n\n34.6.1 Générer une clé SSH\nUn utilisateur peut générer autant de clés SSH qu’il le souhaite. Par exemple, vous pouvez générer une clé SSH par projet sur lequel vous travaillez (même si l’utilité de cette méthode n’est pas évidente).\nVoici comment créer une clé SSH avec Git Bash :\n\nIl faut d’abord ouvrir une invite de commande Git Bash (explications ici) ;\nIl faut ensuite exécuter la ligne de code suivante :\n\nLa clé est générée sur le chemin spécifié et sera constituée de deux fichiers texte :\n\nun fichier dont le nom se termine par .pub (comme Public Key) qui fournit la partie publique de la clé, et qui a vocation à être diffusée (donc c’est ce fichier qu’on chargera sur Gitlab pour permettre l’authentification) ;\nun fichier dont le nom se termine par .ppk (comme Private Key) qui est la partie privée de la clé, et qui ne doit être diffusée en aucun cas.\n\nLa version privée de la clé doit être localisée dans un dossier caché que Git va automatiquement utiliser (en général un dossier .ssh localisé sur un dossier personnel).\n\n34.6.2 Ajout de la clé SSH sous GitLab\nPour copier la partie publique de la clé sous GitLab, il faut aller dans la partie Settings de l’espace personnel sous GitLab, puis sélectionner “SSH Keys” sur le bandeau latéral gauche. On se trouve face à une fenêtre de texte comme ceci :\ndans laquelle on doit copier-coller le texte que l’on trouve dans le fichier .pub de la clé SSH (en prenant soin d’éliminer l’éventuel texte en fin du type “imported-openssh-key”).\nGitLab est désormais configuré, et il est alors possible de faire dialoguer Git et GitLab en suivant, par exemple, les routines proposées dans la fiche Utiliser Git avec RStudio.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "03_Fiches_thematiques/Fiche_configurer_git.html#GitRessources",
    "href": "03_Fiches_thematiques/Fiche_configurer_git.html#GitRessources",
    "title": "34  Configurer Git sur son poste de travail",
    "section": "\n34.7 Pour en savoir plus",
    "text": "34.7 Pour en savoir plus\n\n\nformation Travail collaboratif avec R ;\n\nHappy Git with R.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Configurer `Git` sur son poste de travail</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche-personnaliser-R.html",
    "href": "01_R_Insee/Fiche-personnaliser-R.html",
    "title": "35  Personnaliser la configuration de R",
    "section": "",
    "text": "35.1 Comprendre le lancement de R\nL’agent souhaite personnaliser sa configuration de R. Cette personnalisation consiste à définir certaines actions ou valeurs qui seront effectuées automatiquement à l’ouverture d’une session R.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Personnaliser la configuration de `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche-personnaliser-R.html#comprendre-le-lancement-de-r",
    "href": "01_R_Insee/Fiche-personnaliser-R.html#comprendre-le-lancement-de-r",
    "title": "35  Personnaliser la configuration de R",
    "section": "",
    "text": "35.1.1 Vue d’ensemble de l’initialisation de R\n\nDès ses origines, R a été conçu pour offrir une grande souplesse d’utilisation. Cette souplesse engendre toutefois une complexité dans les procédures d’initialisation de R. La documentation sur le lancement de R montre d’ailleurs toute la complexité de la chose :\n\nhelp(\"Startup\")\n\nDe façon simplifiée, la procédure d’initialisation de R se déroule en trois temps :\n\nR cherche des fichiers de type environment. Ces fichiers permettent notamment d’indiquer des chemins spécifiques où se trouvent d’autres fichiers, ou certaines caractéristiques comme le proxy.\nR cherche des fichiers de type profile. Ces fichiers contiennent du code R qui se lance automatiquement au démarrage de R. Ils permettent par exemple d’indiquer des packages supplémentaires à charger, de personnaliser le message d’accueil de R, ou de sauvegarder automatiquement les informations de la session lorsqu’on la quitte.\nR prépare l’ouverture de la session avec R, avec les informations récupérées précédemment dans les fichiers environment et profile, et d’autres informations, comme par exemple les données qui ont pu être conservées dans un .Rdata. R charge les packages de base (installés avec R), avant de charger l’historique des commandes (s’il en existe un). C’est lors de ce dernier temps que R peut charger les données et l’historique après s’être interrompu inopinément.\n\nEnfin, il existe d’autres fichiers comme le .Rhistory qui contient l’historique des commandes, notamment celles réalisées à la console. Ce fichier peut également contenir des informations sensibles, il est donc rare de le partager, mais il est possible de le conserver.\n\n35.1.2 Configuration générale et configuration par projet\nIl est possible de personnaliser votre configuration de R en définissant des fichiers environment et profile. Ces fichiers peuvent être définis à deux niveaux qu’il faut bien distinguer :\n\nla configuration générale de R, qui s’applique à toutes les sessions R que vous ouvrez. Dans ce cas, les fichiers de configuration doivent être placés dans le répertoire Home. Vous pouvez récupérer le chemin de ce répertoire en exécutant la commande Sys.getenv(\"HOME\") (sous Windows, typiquement Mes documents).\nla configuration spécifique à un Rproject (projet RStudio), qui s’applique uniquement lorsque vous travaillez sur ce projet. Dans ce cas, les fichiers de configuration doivent être placés dans le répertoire du Rproject auquel ils s’appliquent.\n\nDéfinir des fichiers environment et profile spécifiques à un Rproject s’avère très utile lorsque les projets sur lesquels vous travaillez requièrent des configurations de R différentes.\nUn point essentiel est qu’une session R ne charge qu’un seul fichier environment et un seul fichier profile. Les fichiers de configurations sont chargés par ordre de spécificité décroissant :\n\nles fichiers présents dans le répertoire d’un Rproject sont prioritaires ;\nà défaut, les fichiers présents dans le répertoire Home ;\nà défaut, les fichiers présents dans le répertoire d’installation de R.\n\nAjouter un fichier environment dans le répertoire d’un Rproject aura donc pour conséquence que l’éventuel fichier environment présent dans le répertoire Home ne sera pas chargé par la session R lorsque vous travaillerez sur ce projet.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Personnaliser la configuration de `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche-personnaliser-R.html#personnaliser-sa-configuration",
    "href": "01_R_Insee/Fiche-personnaliser-R.html#personnaliser-sa-configuration",
    "title": "35  Personnaliser la configuration de R",
    "section": "\n35.2 Personnaliser sa configuration",
    "text": "35.2 Personnaliser sa configuration\n\n35.2.1 Le fichier .Renviron\n\nLe fichier d’environnement .Renviron sert à définir des variables d’environnement. Ces variables sont utiles pour R lui-même ou pour les interactions entre R et d’autres logiciels. Ces variables d’environnement peuvent également modifier la manière dont R réalise certaines opérations.\nPar exemple, un .Renviron fictif pourrait ressembler à ceci :\nMON_API_INSEE=MaCleSecreteVraimentTresSecrete\nhttp_proxy=le_proxy\nhttps_proxy=le_proxy\n_R_CHECK_LENGTH_1_LOGIC2_=verbose\n_R_CHECK_LENGTH_1_CONDITION_=true\n\n\nUn tel fichier n’est pas constitué de code R, mais de paires nom=valeur. Chaque paire définit une variable d’environnement qui sera exploitée par R pour interagir avec d’autres logiciels, ou directement pour lui. Ce fichier fictif réalise les opérations suivantes :\n\nil définit la clé d’API dans une variable MON_API_INSEE. Ultérieurement, un script R pourra faire appel à cette variable via la commande Sys.getenv(\"MON_API_INSEE\"). De cette façon, la clé confidentielle n’est pas directement visible dans le script R, et ce script peut être partagé en toute sécurité ;\nil modifie les paramètres du proxy pour permettre à R de se connecter à Internet ;\nil modifie des variables internes à R (telles que _R_CHECK_LENGTH_1_CONDITION_ et _R_CHECK_LENGTH_1_LOGIC2_) ;\nil contient une ligne vide finale : cette ligne vide est indispensable, ne l’oubliez jamais !\n\n\n\n\n\n\n\nTip\n\n\n\nVoici deux conseils importants sur l’usage du fichier d’environnement .Renviron :\n\nil est recommandé d’être très prudent dans la personnalisation du fichier d’environnement ;\nle fichier .Renviron peut contenir des informations personnelles et/ou sensibles comme les clés d’API ou l’adresse du proxy. Ce fichier est donc strictement personnel, et ne doit pas être partagé (notamment via git).\n\n\n\nLa méthode la plus simple pour modifier le fichier .Renviron est la suivante :\n\nOuvrir le fichier .Renviron. La commande usethis::edit_r_environ() permet d’ouvrir le fichier .Renviron de votre profil utilisateur (c’est le fichier d’environnement utilisé lorsqu’il n’y a pas de fichier d’environnement à votre projet). Si vous travaillez dans un RProject, la commande usethis::edit_r_environ(scope = \"project\") permet d’ouvrir le le fichier .Renviron spécifique à votre projet.\nModifier le fichier, sans oublier la ligne vide en fin de document.\nSauvegarder le fichier.\nRelancer R, via Session &gt; Restart R ou Ctrl + ⇧ Shift + F10 dans RStudio.\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nComme dans de nombreuses institutions, la navigation sur Internet depuis un poste de l’Insee est contrôlée par un proxy (intermédiaire entre le web et un ordinateur). Il est indispensable de paramétrer l’adresse du proxy pour que R puisse accéder à Internet (par exemple pour télécharger un package). Pour ce faire, vous pouvez récupérer l’adresse du proxy de l’Insee en exécutant la commande suivante :\n\ncurl::ie_get_proxy_for_url()\n\nPuis vous pouvez ajouter l’adresse du proxy dans votre fichier .Renviron. Il faut ajouter deux lignes dans ce fichier :\nhttp_proxy=adresse_du_proxy\nhttps_proxy=adresse_du_proxy\nIl est également possible d’ajouter ces variables d’environnement au niveau du profil utilisateur Windows sur votre poste de travail Insee. Cela permet à tout processus lancé d’hériter automatiquement de ces variables d’environnement, évitant ainsi de réserver cette configuration uniquement pour R, comme dans la première option proposée. De nombreux autres logiciels utilisent par convention ces noms de variables d’environnement (http_proxy, https_proxy ou no_proxy) pour la configuration du proxy lorsqu’ils effectuent des requêtes HTTP ; il est donc possible de mutualiser ces paramètres. C’est notamment le cas d’outils en ligne de commande tels que le client pour des requêtes réseau curl ou des gestionnaires de paquets comme pip pour Python.\nPour ce faire, ouvrez l’interface de modification des variables d’environnement pour votre compte utilisateur en utilisant le raccourci clavier ⊞ + R pour exécuter la commande rundll32 sysdm.cpl,EditEnvironmentVariables, puis ajoutez les nouvelles variables. Redémarrez ensuite R pour que cette modification prenne effet .\n\n\nIl est possible de modifier les paramètres de son environnement ou d’en générer d’autres après l’ouverture de la session R, avec la commande Sys.setenv(). Vous pouvez par exemple exécuter la commande Sys.setenv(\"MON_API_INSEE\"=\"jamais\"). De même, la fonction Sys.getenv() permet de lister certaines valeurs présentes dans l’environnement actuel. Toutefois, toutes les procédures d’initialisation de R se seront déjà déroulées alors avec les anciennes valeurs, définies par défaut. Cette subtilité peut être importante !\n\n35.2.2 Le fichier .Rprofile\n\nLe fichier .RProfile contient du code R qui sera exécuté par R après le chargement des fichiers .Renviron. Voici trois utilisations standards du fichier .Rprofile :\n\nmodifier le miroir par défaut du CRAN (pour télécharger des packages) ;\nmodifier le message d’accueil de la session ;\npersonnaliser l’affichage de R.\n\nUn fichier .Rprofile peut par exemple ressembler à ceci :\noptions(repos = c(CRAN = \"https://cran.rstudio.org\"))\noptions(prompt=\"R&gt; \", digits=4)\noptions(continue= \"+++ \")\nif (interactive()) {\noptions(width = 120)\n}\n\n.First &lt;- function(){\ncat(\"\\n Allez, hop hop hop, au travail !\\n\\n\")\n}\n.Last &lt;- function(){\ncat(\"\\n Je m'en vais comme un prince...\\n\\n\")\n}\n\n\nCe fichier .Rprofile permet de :\n\ndéfinir un miroir par défaut pour le CRAN ;\nmodifier l’affichage de l’invite de commande (le “&gt;” de début de ligne devient “R&gt;”) ;\nmodifier l’affichage par défaut des nombres décimaux ;\nmodifier l’affichage de la console lorsqu’une partie du code reste à écrire (le “+” en début de ligne devient “+++”) ;\ndans le cadre d’un appel à la console, la largeur des impressions est fixée à 120 ;\nles fonctions .First et .Last permettent d’effectuer des opérations spécifiques au lancement et à la fermeture de la session R (imprimer du texte dans le cas présent) ;\nAttention, comme précédemment, un fichier .Rprofile doit impérativement se terminer par une ligne vierge.\n\nLa fonction options() affiche les valeurs actuellement chargées pour les options. Il est possible de les modifier à la volée avec la commande options(nom_de_l_option = valeur_de_l_option). Attention toutefois, modifier une option avec la fonction options() n’est valable que pour la session de R actuellement active ! Si R redémarre, toutes les options modifiées à la volée ne sont plus prises en compte. Si vous voulez modifier durablement la valeur d’une option, il est préférable de modifier le fichier .Rprofile.\nUn exemple plus réaliste de .Rprofile peut être trouvé ici : c’est le fichier qui permet notamment de réaliser la documentation utilitR. Notez notamment le message à l’ouverture du projet. Avoir des fonctions n’est pas forcément interdit, tant que ces fonctions sont également partagées, comme c’est le cas dans ce projet !\n\n\n\n\n\n\nTip\n\n\n\nIl n’est nullement obligatoire de définir des réglages spécifiques avec un fichier .Rprofile. Toutefois, si vous le faites, il est fortement conseillé de :\n\ntravailler dans le .Rprofile d’un projet, et non pas le .Rprofile général de votre profil d’utilisateur ;\npartager le fichier .Rprofile du projet avec les autres contributeurs du projet via une forge logicielle, après avoir vérifié qu’il ne contenait aucune information sensible.\n\nIl est fortement déconseillé de réaliser les actions suivantes dans le fichier .Rprofile :\n\ncharger des packages ;\ndéfinir des alias de fonctions ;\nmodifier le comportement par défaut de R, comme par exemple options(stringsAsFactors = FALSE).\n\nEn effet, toute personne qui ne dispose pas de ce fichier de configuration ne sera pas en mesure de reproduire vos travaux.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Personnaliser la configuration de `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche-personnaliser-R.html#quelques-bonnes-pratiques",
    "href": "01_R_Insee/Fiche-personnaliser-R.html#quelques-bonnes-pratiques",
    "title": "35  Personnaliser la configuration de R",
    "section": "\n35.3 Quelques bonnes pratiques",
    "text": "35.3 Quelques bonnes pratiques\nLa personnalisation de R est une opération sensible qui ne doit pas interférer avec l’exigence de reproductibilité des travaux statistiques. Cette personnalisation doit donc porter uniquement sur des modifications sans impact sur les calculs et sur des informations ayant vocation à ne pas être diffusées. Ces informations à ne pas diffuser peuvent être de deux types :\n\n\ninformations sensibles (comme les identifiants d’API ou les paramètres de proxy). Il est important que ces informations ne figurent pas dans les programmes de façon à pouvoir partager librement les codes ;\n\ninformations d’infrastructure (comme les chemins vers des exécutables). Ces informations n’ont pas vocation à être partagées, car elles varient d’un poste d’un agent à l’autre.\n\nLes autres opérations modifiant le comportement de base (comme options(stringsAsFactors = FALSE)) doivent être partagées et accessibles, via le fichier .Rprofile. Pour information, depuis R version 4.0, la valeur de stringsAsFactors est FALSE par défaut, alors qu’elle était TRUE pour les versions antérieures.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Personnaliser la configuration de `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche-personnaliser-R.html#pour-en-savoir-plus",
    "href": "01_R_Insee/Fiche-personnaliser-R.html#pour-en-savoir-plus",
    "title": "35  Personnaliser la configuration de R",
    "section": "\n35.4 Pour en savoir plus",
    "text": "35.4 Pour en savoir plus\nL’immense majorité de la documentation sur l’utilisation des fichiers de configuration de R est en langue anglaise :\n\n\nUsing .Rprofile and .Renviron par Nicholas Tierney ;\n\nR Startup, chapitre 7 de Rstats - What they forgot to teach you, par Jennifer Bryan er Jim Hester ;\n\nManaging R with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf par Alex Gold ;\n\nEfficient R Programing - chapter 2.5 par Colin Gillespie & Robin Lovelace ;\n\nR for Enterprise: Understanding R’s Startup par Sean Lopp (RStudio).",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Personnaliser la configuration de `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_ressources.html",
    "href": "01_R_Insee/Fiche_utiliser_ressources.html",
    "title": "36  Superviser sa session R",
    "section": "",
    "text": "36.1 Pourquoi faut-il suivre son usage des ressources informatiques ?\nUn usage inadapté des logiciels statistiques peut aboutir à deux problèmes : la saturation de la mémoire vive, et la saturation des processeurs. Le premier problème est plus fréquent avec R que le second.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Superviser sa session `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_ressources.html#pourquoi-faut-il-suivre-son-usage-des-ressources-informatiques",
    "href": "01_R_Insee/Fiche_utiliser_ressources.html#pourquoi-faut-il-suivre-son-usage-des-ressources-informatiques",
    "title": "36  Superviser sa session R",
    "section": "",
    "text": "36.1.1 R et la mémoire vive\nContrairement à d’autres logiciels tels que SAS, R est un langage conçu pour traiter des données chargées dans la mémoire vive (RAM) de l’ordinateur. Cette caractéristique permet à R d’être relativement rapide, mais induit des difficultés lorsque les données sont volumineuses. Le principal risque d’une mauvaise gestion de la mémoire est d’aboutir à une saturation de la mémoire vive : la session R occupe l’intégralité de la mémoire vive, ce qui ralentit les traitements, voire paralyse l’ordinateur. Les problèmes de saturation de la RAM se traduisent généralement par un message du type cannot allocate a vector of size ** Mb (quand la session ne plante pas…).\nLe risque de saturation de la mémoire vive est particulièrement fréquent lorsqu’on travaille sur une infrastructure informatique où la mémoire vive est partagée avec d’autres utilisateurs (comme c’est le cas dans les environnements partagés, comme l’est l’architecture AUS) Si plusieurs utilisateurs occupent chacun une grande quantité de mémoire vive sur la même machine virtuelle, il est facile d’arriver à une situation de saturation qui paralyse tous les agents connectés (voire fait planter leur session R).\nLe risque de saturation de la mémoire vive est aggravé par la façon dont R gère la mémoire. En effet, une session R augmente automatiquement la quantité de mémoire vive qu’elle occupe lorsque les traitements réclament davantage de mémoire, mais elle ne la libère pas toujours lorsque les besoins en mémoire diminuent. Il est donc possible qu’une session R occupe beaucoup plus de mémoire que ce dont elle a besoin. Par exemple, si vous lancez une session R, que vous importez une table de données de 10 Go, puis que vous la supprimez immédiatement de votre session avec la fonction rm(), il est possible que votre session R continue à occuper inutilement 10 Go de mémoire vive, et ce quand bien même votre environnement RStudio vous indique qu’il n’y a aucun objet en mémoire.\nLa conclusion est simple : suivre attentivement l’usage que votre session R fait de la mémoire vive est essentiel pour votre efficacité comme pour celles des autres agents.\n\n\n36.1.2 R et les processeurs\nLe second risque est celui d’une saturation des processeurs : une session R consomme l’intégralité de la puissance de calcul du serveur, ce qui paralyse les sessions des autres utilisateurs. Ce risque est relativement peu fréquent avec R, pour deux raisons. D’une part, le langage R est conçu pour réaliser des traitements en utilisant un seul processeur et non tous les coeurs disponibles. D’autre part, les serveurs partagés ont la possibilité de reporter des traitements d’utilisateurs différents sur des coeurs différents\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nToutes les machines virtuelles d’AUS disposent d’au moins une dizaine de processeurs ce qui permet, quand un coeur est saturé par un calcul intensif, d’avoir un nouveau traitement statistique exécuté dans un autre coeur.\n\n\nToutefois, il peut arriver qu’un traitement réalisé avec R sature l’ensemble des processeurs. En effet, certains packages R sont conçus pour utiliser un grand nombre de processeurs en parallèle, de façon à accélérer les calculs. Il est donc important de suivre l’usage que votre session R fait des processeurs.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Superviser sa session `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_ressources.html#superviser-sa-session-r-avec-le-gestionnaire-des-tâches",
    "href": "01_R_Insee/Fiche_utiliser_ressources.html#superviser-sa-session-r-avec-le-gestionnaire-des-tâches",
    "title": "36  Superviser sa session R",
    "section": "36.2 Superviser sa session R avec le gestionnaire des tâches",
    "text": "36.2 Superviser sa session R avec le gestionnaire des tâches\nVous pouvez facilement superviser votre session R avec le gestionnaire des tâches. Ce programme vous permet de suivre votre consommation de mémoire vive et votre usage des processeurs. Il offre également la possibilité de forcer l’arrêt de votre session R si elle est bloquée, mais cette manoeuvre ne doit être réalisée qu’en dernier recours.\n\n36.2.1 Ouvrir le gestionnaire des tâches\nLe gestionnaire des tâches est une application installée sur toutes les machines Windows. Son équivalent Linux est htop. Le gestionnaire des tâches est généralement accessible avec le raccourci bien connu Ctrl + Alt + Suppr.\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nPour ouvrir le gestionnaire des tâches dans AUS, il suffit d’utiliser le raccourci présent sur le bureau, ou d’utiliser le raccourci clavier suivant : Ctrl + ⇧ Shift + Echap.\n\n\nLorsque la fenêtre suivante s’affiche, il faut cliquer sur Plus de détails.\n\n\n\n36.2.2 Utiliser le gestionnaire des tâches\nL’onglet qui vous sera le plus utile est l’onglet Utilisateurs. Il affiche la liste de tous les agents connectés sur la machine virtuelle. Voici comment vous pouvez utiliser cet onglet pour superviser votre session R :\n\nles deux pourcentages dans le cadre rouge indiquent le taux d’utilisation des processeurs et le taux d’occupation de la mémoire vive, au niveau de l’ensemble des utilisateurs connectés sur cette machine virtuelle. Vous pouvez constater sur l’image que le serveur n’est pas saturé : le taux d’utilisation est faible (moins de 15%) et le taux d’occupation de la mémoire vive est proche de 40%.\nles lignes du tableau indiquent le taux d’utilisation des processeurs et le taux d’occupation de la mémoire vive, au niveau de chaque utilisateur. Vous devez normalement retrouver votre IDEP dans la première colonne (les IDEP sont masqués sur les captures d’écran pour des raisons de sécurité). Le cadre bleu correspond par exemple à l’auteur de cette fiche. Vous pouvez constater qu’il utilise 7,5% des processeurs et qu’il occupe environ 10 Go de mémoire vive.\nEn cliquant sur la petite flèche grise (indiquée par la flèche noire), vous pouvez afficher la consommation de ressources de chacun des programmes que vous utilisez.\n\n\nVoici trois références à garder en tête pour superviser votre session R :\n\nLe serveur est proche de la saturation lorsque l’un des deux taux du cadre rouge dépasse 80%.\nVotre utilisation de la mémoire vive est excessive lorsqu’elle dépasse durablement 50 Go alors que vous ne réalisez pas de traitement. Vous pouvez évidemment utiliser beaucoup plus de mémoire pendant un traitement particulièrement lourd.\nVotre utilisation des processeurs est excessive lorsqu’elle dépasse durablement 25%.\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nEn cas de saturation, il faut ouvrir un ticket Si@moi pour signaler un problème de saturation à l’assistance informatique dans les deux cas suivants :\n\nl’ensemble du serveur est durablement saturé (plus de 15 minutes) ;\nvotre session R fait un usage excessif des ressources informatiques et vous ne parvenez pas à arrêter votre traitement.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Superviser sa session `R`</span>"
    ]
  },
  {
    "objectID": "01_R_Insee/Fiche_utiliser_ressources.html#comment-limiter-la-consommation-de-mémoire-vive-avec-r",
    "href": "01_R_Insee/Fiche_utiliser_ressources.html#comment-limiter-la-consommation-de-mémoire-vive-avec-r",
    "title": "36  Superviser sa session R",
    "section": "36.3 Comment limiter la consommation de mémoire vive avec R ?",
    "text": "36.3 Comment limiter la consommation de mémoire vive avec R ?\nLes paragraphes suivants donnent quelques conseils pour limiter votre consommation de mémoire vive.\n\n36.3.1 Importer uniquement les données nécessaires\nLa saturation de la mémoire vive est souvent provoquée par un utilisateur qui essaie d’importer des données très volumineuses. Voici deux bonnes pratiques à adopter, en particulier lorsque l’ensemble des données que vous souhaitez importer avec R est volumineux (par exemple d’une taille supérieure à 2 Go) :\n\nIl est conseillé d’importer uniquement les colonnes dont vous avez besoin. Toutes les fonctions d’importation de données présentées dans cette documentation comprennent une option permettant de choisir les colonnes. Le tableau qui suit vous indique le nom de l’option en fonction du format des données. Vous pouvez vous reporter à la fiche correspondante pour les détails sur son utilisation.\n\n\n\n\n\n\n\n\n\nFormat des données\nFonction\nPackage\nOption\n\n\n\n\nFichier plat (.csv, .tsv, .txt)\nfread()\ndata.table\nselect\n\n\nTable SAS\nread_sas()\nhaven\ncol_select\n\n\nFichier .xlsx\nread.xlsx()\nopenxlsx\ncols\n\n\nFichier .xls\nread_excel()\nreadxl\nrange\n\n\nFichier .ods\nread_ods()\nreadODS\nrange\n\n\n\nDans le cas où vous ne savez pas quelles sont les colonnes dont vous avez besoin (parce que vous découvrez les données par exemple), il est conseillé de commencer par importer un petit nombre de lignes (1 000 ou 10 000) afin d’étudier les données et de choisir ensuite les colonnes à importer. Toutes les fonctions d’importation de données présentées dans cette documentation comprennent une option permettant de définir le nombre de lignes à importer. Le tableau qui suit vous indique le nom de l’option en fonction du format des données. Vous pouvez vous reporter à la fiche correspondante pour les détails sur son utilisation.\n\n\n\n\n\n\n\n\n\nFormat des données\nFonction\nPackage\nOption\n\n\n\n\nFichier plat (.csv, .tsv, .txt)\nfread()\ndata.table\nnrows\n\n\nFichier plat (.csv, .tsv, .txt)\nread_csv()\nreadr\nn_max\n\n\nTable SAS\nread_sas()\nhaven\nn_max\n\n\nFichier .xlsx\nread.xlsx()\nopenxlsx\nrows\n\n\nFichier .xls\nread_excel()\nreadxl\nn_max\n\n\nFichier .ods\nread_ods()\nreadODS\nrange\n\n\n\n\n\n\n36.3.2 Faire preuve de prudence en faisant des jointures\nUn autre cas standard de saturation de la mémoire vive provient d’une erreur dans la réalisation d’une jointure entre deux tables. En effet, une jointure mal réalisée peut aboutir à une table d’une taille largement supérieure à celle de la mémoire vive disponible sur l’ordinateur ou le serveur. De façon générale, une jointure entre des tables comprenant un grand nombre d’observations (par exemple plus de 500 000 observations) doit être menée avec prudence. Vous trouverez davantage de conseils dans la fiche [Joindre des tables de données], en particulier dans la dernière section.\n\n\n36.3.3 Nettoyer régulièrement la mémoire vive\nComme mentionné précédemment, une session R augmente automatiquement la quantité de mémoire vive qu’elle occupe lorsque les traitements réclament davantage de mémoire, mais ne la libère pas toujours lorsque les besoins en mémoire diminuent. Il est donc important de vérifier régulièrement que vous n’occupez pas plus de mémoire vive que nécessaire.\nPour ce faire, la solution la plus simple consiste à utiliser la fonction gc() (garbage collection, qui signifie littéralement “enlèvement des ordures”). Cette fonction analyse la mémoire vive occupée par la session R, et libère la mémoire qui est occupée inutilement. Il est vivement recommandé d’exécuter la fonction gc() après un traitement portant sur des données volumineuses.\nVoici une liste indicative d’opérations après lesquelles vous pouvez exécuter gc() :\n\nune jointure entre deux tables volumineuses ;\nla suppression d’objets volumineux avec la fonction rm() ;\nune estimation économétrique complexe ;\ntoute opération statistique qui prend plus de quelques minutes.\n\nIl y a deux façons de nettoyer la mémoire vive:\n\nMéthode n°1: exécuter gc();\nMéthode n°2: utiliser l’interface proposée par RStudio. Il faut cliquer sur la petite flèche rouge, puis sur Free unused R Memory.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVoici deux remarques sur la fonction gc() :\n\nPlus la session R occupe de mémoire vive, plus la fonction gc() met de temps à nettoyer la mémoire. Ce nettoyage peut prendre jusqu’à plusieurs minutes si vous manipulez des données volumineuses.\nCertaines ressources documentaires sur R affirment qu’il est inutile de se servir de gc() car R nettoie automatiquement la mémoire vive lorsqu’elle est presque saturée. Ceci est vrai lorsque vous utilisez R sur votre poste local (et où il n’y a qu’une seule session R). En revanche, ce n’est pas vrai dans le cas où plusieurs sessions R partagent la mémoire vive (car votre session R ne peut déclencher le nettoyage de la mémoire vive dans la session R d’un autre utilisateur).\n\n\n\n\n\n\n\n\n\nSpécificité Insee\n\n\n\nLes conseils et les bonnes pratiques présentés dans cette fiche devraient suffire à résoudre la plupart des problèmes de saturation que vous pourriez rencontrer. Toutefois, il est possible que cela ne suffise pas, parce que vos traitements requièrent des ressources informatiques particulièrement importantes. En ce cas, il faut déposer une demande métier dans Si@moi.",
    "crumbs": [
      "Introduction",
      "Éléments de configuration",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Superviser sa session `R`</span>"
    ]
  }
]